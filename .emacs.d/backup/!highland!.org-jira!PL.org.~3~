* DONE Creating new network throws InvalidParameterError 	    :PL_7157:
  :PROPERTIES:
  :assignee: arobinson
  :reporter: arobinson
  :type:     Bug
  :priority: Blocker
  :status:   Resolved
  :components: cm
  :created:  2013-10-17 20:58:41
  :updated:  2013-10-21 19:04:09
  :ID:       PL-7157
  :resolution: Fixed
  :END:
** description: PL-7157
  Trying to add a new network to a configuration fails.
  
  REPRO:
  1. Create config from template that has existing network.
  2. Attempt to add new network, making sure subnet does not overlap existing network.
  3. Click on Create Network
  
  EXPECTED:
  New network created.
  
  ACTUAL:
  Error message:
  
  {noformat}
  The operation cannot be performed at this time. Please try again or contact support. (InvalidParameterError, 40E0D375)
  {noformat}
  
  Network is not created.
  
  Error watch:
  {noformat}---------------------------------------------------------
  
  2013-10-17 20:56:12 tuk6m1wfe4 [err] [/wfe.ab0026401974013181b1005056a30032.151]: [ERROR] logging.rb:47 - API EXCEPTION! The operation cannot be performed at this time. Please try again or contact support. (InvalidParameterError, 40E0D375) [Platform::Client::MQClient::Error]
   (in: create_network(["configuration-613246", "automatic", {"nat_cidr"=>nil, "domain_name"=>"test.net", "cidr_block"=>"10.0.0.0/24", "primary_nameserver"=>nil, "secondary_nameserver"=>nil}])(caller_options: {"samba_fileserver"=>true, "prefer_kvm"=>true})
  ----------------------------------------------------------------------{noformat}
** Comment: andrus
   :PROPERTIES:
   :ID:       114793
   :created:  2013-10-18 16:12:51
   :END:
  Giving to you as this seems like it might be rate-limiting related. You'll need to poke around in the logs a bit... see me for pointers.
** Comment: bxiao
   :PROPERTIES:
   :ID:       114918
   :created:  2013-10-18 21:00:13
   :END:
  Investigating...
** Comment: bxiao
   :PROPERTIES:
   :ID:       114919
   :created:  2013-10-18 21:01:54
   :END:
  Hi Ross, 
  where can I get the full log? 
** Comment: bxiao
   :PROPERTIES:
   :ID:       114920
   :created:  2013-10-18 21:07:38
   :END:
  Looks like tuk6m1logger1 has the right logs.
** Comment: bxiao
   :PROPERTIES:
   :ID:       114927
   :created:  2013-10-18 21:46:07
   :END:
  Looks like there is None parameter which should be a string in create_network
  
  2013-10-17 20:56:12 tuk6m1gb2 [debug] [wfe.ab0026401974013181b1005056a30032.151.1@/tuk6m1gb2.mgt.test.skytap.com-0.22078.request.create_network.586.greenbox]: [DEBUG] errors.py:209 - Error processing message: MQRequestMessage(action:create_network, unique_id:tuk6m1wfe4:req:7a8642d0199c013181b1005056a30032:wfe.ab0026401974013181b1005056a30032.151.1:316): {"incident_id": "40E0D375", "name": "request.create_network", "context": "wfe.ab0026401974013181b1005056a30032.151.1@/tuk6m1gb2.mgt.test.skytap.com-0.22078.request.create_network.586", "__severity": "error", "msg": "\"None is not of type 'string'\"", "timestamp": "2013-10-17 20:56:12", "type": "InvalidParameterError", "resources": []}  
  
** Comment: bxiao
   :PROPERTIES:
   :ID:       114947
   :created:  2013-10-18 22:41:39
   :END:
  Here is the payload that causes the error:
  
  
  2013-10-17 20:56:12 tuk6m1gb2 [debug] [/greenbox-tuk6m1gb2.mgt.test.skytap.com-6.22078.greenbox]: [DEBUG] MQFramework.py:806 - MQFramework:gr
  eenbox, read, request,  time_in_queue:35 {"_mq_protocol_version": 1, "_timestamp": "2013-10-17T20:56:12Z", "_reply_routing_key": "web-rpc.ab0
  026401974013181b1005056a30032.test.default", "caller_context": {"context_id": "wfe.ab0026401974013181b1005056a30032.151.1@", "caller_options"
  : {"samba_fileserver": true, "prefer_kvm": true}}, "destination_service": "greenbox", "_unique_id": "tuk6m1wfe4:req:7a8642d0199c013181b100505
  6a30032:wfe.ab0026401974013181b1005056a30032.151.1:316", "_message_type": "request", "action": "create_network", "payload": {"network_attribu
  tes": {"secondary_nameserver": null, "cidr_block": "10.0.0.0/24", "primary_nameserver": null, "domain_name": "test.net", "nat_cidr": null}, "
  configuration_key": "configuration-613246", "network_type": "automatic"}}
  
  And the problem is "primary_nameserver" is null:
  
  Failed validating 'type' in schema['properties']['network_attributes']['properties']['primary_nameserver']:
      {'type': 'string'}
  
  

** Comment: bxiao
   :PROPERTIES:
   :ID:       114950
   :created:  2013-10-18 23:08:17
   :END:
  The "None" primary_nameserver come from configuration specification:
  
  
  2013-10-17 20:56:11 tuk6m1cm3 [debug] [wfe.ab0026401974013181b1005056a30032.151.1@/tuk6m1gb1.mgt.test.skytap.com-0.13850.request.get_configur
  ation_specification.602/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-1.32118.get_configuration_specification.8.api]: [DEBUG] Configura
  tionManager.py:46 - get_configuration_specification RETURN 0.09 {'configuration-613246': ({'disable_internet': False, 'routable': False, 'run
  nable': True, 'region': 'test/tuk6r1', 'vms': [{'hardware': {'disk_policy': {'key': 'disk_policy-100', 'iops_limit': '275', 'size_limit': '20
  97152', 'disk_limit': '15', 'disk_size_limit': '2096128', 'offset': '165'}, 'uuid': None, 'floppy': 'fd0', 'nics': [{'nic_type': 'e1000', 'pr
  omiscuous_mode_enabled': False, 'nat_addresses': {'networks': {}, 'vpns': {}}, 'vm_key': 'vm-613246-4136714', 'virtual_dev': 'e1000', 'hostna
  me': 'auto-host-1', 'public_ips': [], 'mac': '00:50:56:19:c9:fc', 'published_services': [{'external_address': '199.204.217.3', 'internal_port
  ': '22', 'external_port': '24778'}], 'key': 'nic-613246-4136714-0', 'address': '10.0.2.1', 'configuration_key': 'configuration-613246', 'netw
  ork_key': 'network-613246-452584'}], 'hardware_policy': {'max_ram': '32768', 'min_ram': '256', 'max_cpus': '8', 'min_cpus': '1'}, 'ram': '102
  4', 'hardware_version': '7', 'cpus': '1', 'guest_cpu_type': '4', 'vnc_keymap': None, 'disk_controllers': [{'bus_type': 'ide', 'controller': '
  0', 'disks': [{'size': '0', 'device_type': 'cdrom', 'key': 'disk-613246-4136714-ide-0-0', 'lun': '0'}], 'virtual_dev': None, 'key': 'disk_con
  troller-613246-4136714-ide-0'}, {'bus_type': 'scsi', 'controller': '0', 'disks': [{'size': '20480', 'device_type': 'disk', 'key': 'disk-61324
  6-4136714-scsi-0-0', 'lun': '0'}], 'virtual_dev': 'lsilogic', 'key': 'disk_controller-613246-4136714-scsi-0'}], 'host_cpu_type': None, 'guest
  OS': 'ubuntu-64'}, 'vm_extra_options_groups': [], 'key': 'vm-613246-4136714'}], 'account_keys': ['account-2'], 'key': 'configuration-613246',
   'networks': [{'nat_cidr': None, 'domain_name': 'auto-test-1.net', 'network_type': 'automatic', 'nat_pool': None, 'primary_nameserver': None,
   'vpn_attachments': [], 'key': 'network-613246-452584', 'secondary_nameserver': None, 'cidr_block': '10.0.2.0/24', 'configuration_key': 'conf
  iguration-613246', 'gateway': '10.0.2.254', 'tunnels': []}]}, 0)}
  
  Now I'm not sure who should look at this. It seems to be reasonable to not specify nameservers when creating configuration, but it will be needed when creating network.
  
  - should WFE be responsible to always pass a valid primary nameserver when sending create network request?
  - should GB wrap this in client error so 
** Comment: bxiao
   :PROPERTIES:
   :ID:       114951
   :created:  2013-10-18 23:10:55
   :END:
  
  I talked to Ross, it seems that schema should allow None primary_nameserver. Since I created unit test for this I'll go ahead and fix the schema.
** Comment: bxiao
   :PROPERTIES:
   :ID:       115148
   :created:  2013-10-21 19:03:33
   :END:
  This is fixed by MattP @ changeset 12621. Resolving.
* TODO cm deadlock in delete configuration			    :PL_6968:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: lonnieh
  :type:     Bug
  :priority: Critical
  :status:   Open
  :components: cm
  :created:  2013-09-20 13:42:05
  :updated:  2013-09-25 19:07:31
  :ID:       PL-6968
  :END:
** description: PL-6968
  {noformat}
  2013-09-20 13:33:42 tuk6m1cm2 [err] [wfe.77f43650039001316c8e005056a3002c.756.11@/tuk6m1gb2.mgt.test.skytap.com-0.26087.request.delete_configuration.1070513/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.29382.delete_configuration.49.configuration_manager]: [ERROR] operations.py:135 - operation completed with exclusions: configurations: [configuration-593492] vms: [] networks: [network-593492-437262] account_depots: [] vpns: [] tunnels: [] tunnel_consumers: []
  ----------------------------------------------------------------------
  2013-09-20 13:33:42 tuk6m1cm2 [err] [wfe.77f43650039001316c8e005056a3002c.756.11@/tuk6m1gb2.mgt.test.skytap.com-0.26087.request.delete_configuration.1070513/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.29382.delete_configuration.49.configuration_manager]: [ERROR] threadutils.py:95 - Error executing asynchronous operation asynch-op <function execute at 0x5a5c230>(, {}) : (OperationalError) (1213, 'Deadlock found when trying to get lock; try restarting transaction') 'DELETE FROM configurations WHERE configurations.configuration_id = %s' (593492L,) at Traceback (most recent call last):
    File "/highland/hosting_platform/hosting_platform/common/threadutils.py", line 87, in run
      self.return_value = self.operation(*self.args, **self.kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 62, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 80, in execute
      self.provision()  # pylint: disable=E1101
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 596, in provision
      self.unprovisioned_hook()
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/delete.py", line 209, in unprovisioned_hook
      tx.query(Configuration).filter(Configuration.configuration_id == self.configuration.configuration_id).delete()
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/query.py", line 2283, in delete
      result = session.execute(delete_stmt, params=self._params)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/session.py", line 763, in execute
      clause, params or {})
    File "/usr/lib/pymodules/python2.6/sqlalchemy/engine/base.py", line 1399, in execute
      params)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/engine/base.py", line 1532, in _execute_clauseelement
      compiled_sql, distilled_params
    File "/usr/lib/pymodules/python2.6/sqlalchemy/engine/base.py", line 1640, in _execute_context
      context)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/engine/base.py", line 1633, in _execute_context
      context)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/engine/default.py", line 325, in do_execute
      cursor.execute(statement, parameters)
    File "/usr/lib/pymodules/python2.6/MySQLdb/cursors.py", line 166, in execute
      self.errorhandler(self, exc, value)
    File "/usr/lib/pymodules/python2.6/MySQLdb/connections.py", line 35, in defaulterrorhandler
      raise errorclass, errorvalue
  OperationalError: (OperationalError) (1213, 'Deadlock found when trying to get lock; try restarting transaction') 'DELETE FROM configurations WHERE configurations.configuration_id = %s' (593492L,)
  ----------------------------------------------------------------------{noformat}
  
  deadlock from mysql:
  {noformat}
  ------------------------
  LATEST DETECTED DEADLOCK
  ------------------------
  130913 13:27:06
  *** (1) TRANSACTION:
  TRANSACTION 0 1329308440, ACTIVE 0 sec, process no 1726, OS thread id 140041075902208 fetching rows
  mysql tables in use 6, locked 4
  LOCK WAIT 4 lock struct(s), heap size 1216, 9 row lock(s)
  MySQL thread id 4158825, query id 2579798706 tuk1m1cm5.mgt.prod.skytap.com 10.8.16.39 root Sending data
  SELECT tunnel_consumers.tunnel_key AS tunnel_consumers_tunnel_key, tunnel_consumers.left_network_id AS tunnel_consumers_left_network_id, tunnel_consumers.right_network_id AS tunnel_consumers_right_network_id, tunnel_consumers.region AS tunnel_consumers_region, tunnel_consumers.left_endpoint_active AS tunnel_consumers_left_endpoint_active, tunnel_consumers.right_endpoint_active AS tunnel_consumers_right_endpoint_active, operations_1.operation_id AS operations_1_operation_id, operations_1.timestamp AS operations_1_timestamp, operations_1.service_instance_id AS operations_1_service_instance_id, operations_1.operation_type AS operations_1_operation_type, operations_1.context_id AS operations_1_context_id, operations_1.fleeting AS operations_1_fleeting, operations_1._type AS operations_1__type 
  FROM tunnel_consumer_operation_exclusion AS tunnel_consumer_operation_exclusion_1, tunnel_consumers LEFT OUTER JOIN tu
  *** (1) WAITING FOR THIS LOCK TO BE GRANTED:
  RECORD LOCKS space id 0 page no 91754 n bits 208 index `fk_tunnel_consumer_operation_exclusion_operations_operation_id` of table `configuration`.`tunnel_consumer_operation_exclusion` trx id 0 1329308440 lock_mode X locks rec but not gap waiting
  Record lock, heap no 137 PHYSICAL RECORD: n_fields 2; compact format; info bits 32
   0: len 4; hex 8114b8b2; asc     ;; 1: len 19; hex 74756e6e656c2d39303432302d313131363734; asc tunnel-90420-111674;;
  
  *** (2) TRANSACTION:
  TRANSACTION 0 1329308430, ACTIVE 0 sec, process no 1726, OS thread id 140041115305728 starting index read, thread declared inside InnoDB 500
  mysql tables in use 1, locked 1
  8 lock struct(s), heap size 1216, 6 row lock(s), undo log entries 4
  MySQL thread id 4158790, query id 2579798759 tuk1m1cm1.mgt.prod.skytap.com 10.8.16.35 root updating
  DELETE FROM tunnel_consumer_operation_exclusion WHERE tunnel_consumer_operation_exclusion.tunnel_key = 'tunnel-111656-111674' AND tunnel_consumer_operation_exclusion.operation_id = 18135218
  *** (2) HOLDS THE LOCK(S):
  RECORD LOCKS space id 0 page no 91754 n bits 208 index `fk_tunnel_consumer_operation_exclusion_operations_operation_id` of table `configuration`.`tunnel_consumer_operation_exclusion` trx id 0 1329308430 lock_mode X locks rec but not gap
  Record lock, heap no 137 PHYSICAL RECORD: n_fields 2; compact format; info bits 32
   0: len 4; hex 8114b8b2; asc     ;; 1: len 19; hex 74756e6e656c2d39303432302d313131363734; asc tunnel-90420-111674;;
  
  *** (2) WAITING FOR THIS LOCK TO BE GRANTED:
  RECORD LOCKS space id 0 page no 91753 n bits 104 index `PRIMARY` of table `configuration`.`tunnel_consumer_operation_exclusion` trx id 0 1329308430 lock_mode X locks rec but not gap waiting
  Record lock, heap no 32 PHYSICAL RECORD: n_fields 4; compact format; info bits 0
   0: len 20; hex 74756e6e656c2d3131313635362d313131363734; asc tunnel-111656-111674;; 1: len 6; hex 00004f3b851a; asc   O;  ;; 2: len 7; hex 8000018012012d; asc       -;; 3: len 4; hex 8114b8b2; asc     ;;
  
  *** WE ROLL BACK TRANSACTION (1)
  {noformat}
* DONE CM: Rate Limiting handles region incorrectly		    :PL_7162:
  :PROPERTIES:
  :assignee: evanc
  :reporter: ybranch
  :type:     Bug
  :priority: Critical
  :status:   Resolved
  :created:  2013-10-17 22:10:25
  :updated:  2013-10-24 17:34:17
  :ID:       PL-7162
  :resolution: Fixed
  :END:
** description: PL-7162
  I have no clue what to make of these errors....
  
  {code}
  2013-10-17 21:54:11 tuk6m1gb2 [err] [wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.test.skytap.com-0.22173.request.run_vms.673.greenbox]: [ERROR] errors.py:209 - message handler failed: {"incident_id": "503A01AA", "cause": {"incident_id": "503A01AA", "name": "run", "context": "wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.test.skytap.com-0.22173.request.run_vms.673/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.31540.run.8", "msg": "675.56525735294076", "timestamp": "2013-10-17 21:54:11", "type": "GlobalIsScaredError"}, "name": "request.run_vms", "context": "wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.test.skytap.com-0.22173.request.run_vms.673", "__severity": "error", "msg": "'{\"incident_id\": \"503A01AA\", \"name\": \"run\", \"context\": \"wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.test.skytap.com-0.22173.request.run_vms.673/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.31540.run.8\", \"__severity\": \"error\", \"msg\": \"675.56525735294076\", \"timestamp\": \"2013-10-17 21:54:11\", \"type\": \"GlobalIsScaredError\"}'", "timestamp": "2013-10-17 21:54:11", "type": "ConfigurationManagerInternalError"}
  {code}
** Comment: ybranch
   :PROPERTIES:
   :ID:       114708
   :created:  2013-10-17 22:13:22
   :END:
  Here's something with the same context
  {code}
  2013-10-17 21:54:11 tuk6m1cm1 [warning] [wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.tes
  t.skytap.com-0.22173.request.run_vms.673/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.31540.run.8.ratelimiter]: [WARNING] l
  imits.py:172 - Cannot load regional configuration: ConfigurationError: ("'No such environment configuration directory: /highland/configs/test
  /test/tuk6r1'", 'No such environment configuration directory: /highland/configs/test/test/tuk6r1'). 
  {code}
  
  
  The path it's trying is incorrect.
** Comment: ybranch
   :PROPERTIES:
   :ID:       114715
   :created:  2013-10-17 22:22:10
   :END:
  Bing, starting with you as the config path it tries is not correct.
** Comment: lonnieh
   :PROPERTIES:
   :ID:       114776
   :created:  2013-10-18 14:46:35
   :END:
  Bing, if it helps to play around in a multi-region environment, you can use lonnieh-cloud for that...
** Comment: bxiao
   :PROPERTIES:
   :ID:       114805
   :created:  2013-10-18 16:26:26
   :END:
  Investigating...
** Comment: bxiao
   :PROPERTIES:
   :ID:       114870
   :created:  2013-10-18 18:17:24
   :END:
  Is it possible for me to get full logs?
  
  Looks like the "region" of the vas are "test/tuk6r1". I think it's supposed to be "tuk6r1". Need to look at the full logs to get more context.
  
** Comment: bxiao
   :PROPERTIES:
   :ID:       114875
   :created:  2013-10-18 18:35:51
   :updated:  2013-10-18 18:36:14
   :END:
  Got full log. Looks like the "region" being passed around include the env name ('test/tuk6r1' vs 'tuk6r1'):
  
  
  2013-10-17 00:00:06 tuk6m1cm2 [debug] [/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.30764.configuration_manager]: [DEBUG] MQClientPika.py:251 - write_message accounting.test.default {"_mq_protocol_version": 1, "_timestamp": "2013-10-17T00:00:06.290144Z", "_reply_routing_key": "ConfigurationManager_synchronous_configuration_manager-tuk6m1cm2_mgt_test_skytap_com-1.test.default", "caller_context": null, "_unique_id": "req:tuk6m1cm2:ConfigurationManagerService:30764:2013-10-17_00-00-06:0", "_message_type": "request", "action": "manage_resources", "payload": {"reservations": {"resources": {"network-49804-66040": {"region": "test/tuk6r1", "resource_types": {"networks": 1}, "details": {}}, "vm-49804-80746": {"region": "test/tuk6r1", "resource_types": {"svms": 1}, "details": {"details": "{\"hosting_node\": \"c8b12.mgt.test.skytap.com\", \"ram\": 1024, \"cpus\": 1, \"cpu_type\": \"Type C\"}"}}}}, "charge_tag": "ctag-00bf99b018ed0131f323005056a30030"}}
  ----------------------------------------------------------------------
  

** Comment: andrus
   :PROPERTIES:
   :ID:       114876
   :created:  2013-10-18 18:38:31
   :updated:  2013-10-18 18:38:53
   :END:
  Sorry, never explained this to you.
  
  management regions are "plain" - like "test" or "prod"
  
  resource regions look like sub-directories - e.g. "test/tuk6r1" or "prod/slg1r1"
  
  So, your code here:
  {code}
  region = vms[0].region
  assert all(vm.region == region for vm in vms), "not all vms in the same region"
  env = os.path.join(config.HIGHLAND_ENV, region)
  {code}
  
  is working too hard...
  

** Comment: bxiao
   :PROPERTIES:
   :ID:       114892
   :created:  2013-10-18 19:21:24
   :END:
  Right.
  
  Talk to Lonnie, this is not Greenbox/messaging issue. So renamed the title.
  
  In the case where the file is not found (due to the wrong path issue), we fall back to default - /highland/configs/test/hosting_platform_conf.yaml", which has the following:
  
  
  rate_limiting:
     big_operation_cost:   30
     big_operation_mem:  4.0
     big_operation_disk: 6.8
     big_operation_host: 4.4
     big_operation_net:   9.6
     system_big_operation_count: 11
     max_operation_multiplier: 7.6
  
  
  
  2013-10-17 21:54:11 tuk6m1cm1 [debug] [wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.test.
  skytap.com-0.22173.request.run_vms.673/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.31540.run.8.ratelimiter]: [DEBUG] limit
  s.py:302 - ratelimiter costs: op run, storage 675.57, host 409.09, net 3.12
  
  
  2013-10-17 21:54:11 tuk6m1cm1 [info] [wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.test.skytap.com-0.22173.request.run_vms.673/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.31540.run.8.ratelimiter]: [INFO] limits.py:370 - ratelimiter GlobalIsScaredError: op run, max_cost 675.565257353, single_op_threshold 228.0, accounts [2L]
  
  
  
  mysql> select * from limits where account_id=2;
  +------------+---------------------+---------------------+---------------------+-------------+
  | account_id | storage_time        | host_time           | network_time        | region      |
  +------------+---------------------+---------------------+---------------------+-------------+
  |          2 | 2013-05-14 20:15:15 | 2013-05-14 20:15:11 | 2013-05-14 20:15:09 |             |
  |          2 | 2013-10-18 19:00:47 | 2013-10-18 19:00:44 | 2013-10-18 19:00:39 | test/tuk6r1 |
  |          2 | 2013-10-18 17:22:08 | 2013-10-18 17:22:08 | 2013-10-18 17:22:08 | test/tuk6r2 |
  +------------+---------------------+---------------------+---------------------+-------------+
  
  
  Because the max_cost(675.56) > single_op_threshold(228) we raise GlobalScaredError. So even if we fix the path this is still expected behavior.
  
** Comment: bxiao
   :PROPERTIES:
   :ID:       115030
   :created:  2013-10-21 00:15:09
   :END:
  Fix is in integ
** Comment: evanc
   :PROPERTIES:
   :ID:       115703
   :created:  2013-10-23 23:40:07
   :END:
  In Test:r49:hosting_platform:0f7c5fbfee20, we are seeing the following recorded in the log file:
  {noformat}
  2013-10-23 19:24:40 tuk6m1cm2 [warning] [wfe.01d3da201e45013181f4005056a30032.11/trn.7b00d0201e460131f372005056a30030.5.1@/greenbox-tuk6m1gb2.mgt.test.skytap.com-9-0.23872.request.run_vms.550/qm-run-0/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.18682.run.26.ratelimiter]: [WARNING] limits.py:172 - Cannot load regional configuration: ConfigurationError: ("'Configuration file not found for hosting_platform'", 'Configuration file not found for hosting_platform'). 
  {noformat}
  
  However, we have 'hosting_platform_conf.yaml' in each of our regional configuration directories, and these config files contain rate limiting entries.
** Comment: bxiao
   :PROPERTIES:
   :ID:       115740
   :created:  2013-10-24 04:15:46
   :END:
  Investigating
** Comment: bxiao
   :PROPERTIES:
   :ID:       115745
   :created:  2013-10-24 05:06:57
   :END:
  Pushed fix to integ
** Comment: bxiao
   :PROPERTIES:
   :ID:       115802
   :created:  2013-10-24 16:38:52
   :END:
  Looks like I pushed the fix to R50 instead of R49. Reactive until I move the change over
** Comment: bxiao
   :PROPERTIES:
   :ID:       115811
   :created:  2013-10-24 17:34:17
   :END:
  Now the fix is pushed to R49
* DONE Parameter validation errors in mq reply does not reveal useful information :PL_7198:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: bxiao
  :type:     Bug
  :priority: Critical
  :resolution: Unfortunate
  :status:   Resolved
  :components: common
  :created:  2013-10-18 22:50:02
  :updated:  2013-10-20 20:17:58
  :ID:       PL-7198
  :END:
** description: PL-7198
  Here is an example error:
  
  2013-10-17 20:56:12 tuk6m1gb2 [debug] [wfe.ab0026401974013181b1005056a30032.151.1@/tuk6m1gb2.mgt.test.skytap.com-0.22078.request.create_network.586.greenbox]: [DEBUG] errors.py:209 - Error processing message: MQRequestMessage(action:create_network, unique_id:tuk6m1wfe4:req:7a8642d0199c013181b1005056a30032:wfe.ab0026401974013181b1005056a30032.151.1:316): {"incident_id": "40E0D375", "name": "request.create_network", "context": "wfe.ab0026401974013181b1005056a30032.151.1@/tuk6m1gb2.mgt.test.skytap.com-0.22078.request.create_network.586", "__severity": "error", "msg": "\"None is not of type 'string'\"", "timestamp": "2013-10-17 20:56:12", "type": "InvalidParameterError", "resources": []}
  
  This error gives a message "None is not of type 'string'" but does not tell which part in the request that causes this.
  
  The message should include details from json schema validator, as is below:
  
  
  ValidationError: None is not of type 'string'
  
  Failed validating 'type' in schema['properties']['network_attributes']['properties']['primary_nameserver']:
      {'type': 'string'}
  
  On instance['network_attributes']['primary_nameserver']:
      None
** Comment: mpietrek
   :PROPERTIES:
   :ID:       115016
   :created:  2013-10-20 20:17:46
   :END:
  The problem here is the use of "oneOf". Because replies can be either "result" or "error", we have to have schema for both of the possible results. If there was only one schema, jsonschema validation could point to the exact problem.
  
  I have a tool checked in (.../api/tools/jsonschema_errorhelper.py) which is trivially modified to include the payload data and the target schema. It then makes a best effort to give you more useful information.
* TODO add ppa for emacs-24 to appdev nodetype			    :PL_6701:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: andrus
  :type:     Improvement
  :priority: Major
  :status:   In Progress
  :components: skytap-support
  :created:  2013-08-12 16:46:47
  :updated:  2013-09-03 19:44:57
  :ID:       PL-6701
  :END:
** description: PL-6701
** Comment: bxiao
   :PROPERTIES:
   :ID:       108906
   :created:  2013-08-12 17:09:01
   :END:
  The emacs 24 for Ubuntu I found is not the official release, but a stable snapshot. It can be installed side by side with emacs 23.
  
  With emacs 24 you can install Jedi extension (http://tkf.github.io/emacs-jedi/) for Python. It has much improved support over emacs basic Python mode.
  
  To install emacs 24 on Ubuntu 10.04:
  
  
  sudo add-apt-repository ppa:cassou/emacs
  sudo apt-get update
  sudo apt-get install emacs-snapshot-el emacs-snapshot-gtk emacs-snapshot
  
** Comment: andrus
   :PROPERTIES:
   :ID:       108923
   :created:  2013-08-12 20:22:03
   :END:
  Jeremy - can you show bing how to do this in puppet, then assign it to him to actually do? 
** Comment: jlingmann
   :PROPERTIES:
   :ID:       108929
   :created:  2013-08-12 20:36:32
   :END:
  Sure, no problem.  Bing, do you have some time this afternoon to chat?
* TODO rate limiting should differentiate customer from user	    :PL_6744:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: andrus
  :type:     Improvement
  :priority: Major
  :status:   Open
  :components: cm
  :created:  2013-08-19 17:50:48
  :updated:  2013-08-21 20:01:32
  :ID:       PL-6744
  :END:
** description: PL-6744
  So one user can't slam the entire customer.
** Comment: andrus
   :PROPERTIES:
   :ID:       109814
   :created:  2013-08-21 20:01:13
   :END:
  Presenting case:
  
  * A single customer with multiple users, including one primarily using the API. The goal is to be able to have the API usage *not* clobber the "real" users. The suggested approach is to have customer-scope rate limiting at some multiple of user-scope rate limiting (like 3 or 5).
  
  

* TODO Finish two-phase CM migration (first part in R49)	    :PL_6901:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: bxiao
  :type:     Task
  :priority: Major
  :status:   Open
  :components: cm
  :created:  2013-09-10 16:17:35
  :updated:  2013-10-18 19:28:55
  :ID:       PL-6901
  :END:
** description: PL-6901
  In Configuration database
  Make the new 'name' column of accounts table non-nullable
  
  The new region column of limits table has default empty string. Remove this default.
** Comment: bxiao
   :PROPERTIES:
   :ID:       114898
   :created:  2013-10-18 19:28:55
   :END:
  Non-nullable and default to empty is done in R49.
  
  In R50 we will cleanup old record with empty region.
* TODO nhn failure leaves nic disconnected, requires manual reconnect :PL_6931:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: andrus
  :type:     Improvement
  :priority: Major
  :status:   Open
  :components: cm, hosting
  :created:  2013-09-12 19:54:25
  :updated:  2013-10-02 20:53:56
  :ID:       PL-6931
  :END:
** description: PL-6931
  Encountered during outage-115.
  
  Here's a fragment of the error log (this is vm-746838-1893656):
  
  {code}
  2013-09-11 15:24:25 tuk1m1cm4 [err] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.service]: [ERROR] errors.py:209 - service invocation failure for network_server.metadatahost_create(nic-746838-1893656-0-metadatahost,network-746838-728466-metadataserver,192.168.0.5,vm-746838-1893656)
  : {"incident_id": "4276904F", "cause": {"incident_id": "162C2DC8", "type": "UnknownNetworkError"}, "name": "run", "context": "wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294", "__severity": "error", "msg": "\"ResourceParentStateInvalid(u'resource=(metadatahost:6473560:6441556:(nic-746838-1893656-0-metadatahost:network-746838-728466-metadataserver):[configuration-746838]:[UNDEPLOYED:(vgr:UNDEPLOYED)](locked)), parent_resource=(metadataserver:6441556:6441546:(network-746838-728466-metadataserver:network-746838-728466):[configuration-746838]:[DEPLOYED:(vgr:UNDEPLOYED)](unlocked)): ',)\"", "timestamp": "2013-09-11 15:24:25", "type": "InternalNetworkingError"}
  2013-09-11 15:24:25 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [DEBUG] unprovision.py:741 - unprovisioning nic-746838-1893656-0-metadatahost for nic-746838-1893656-0
  2013-09-11 15:24:25 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.call_node_trace]: [DEBUG] CallNode.py:96 - 110174672: calling tuk1r1nsvip1:9002.metadatahost_destroy(('nic-746838-1893656-0-metadatahost',)) caller_options: {'samba_fileserver': False, 'prefer_kvm': False}
  2013-09-11 15:24:25 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.call_node_trace]: [DEBUG] CallNode.py:152 - 110174672: returning None
  2013-09-11 15:24:25 tuk1m1cm4 [err] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [ERROR] errors.py:209 - failure attaching nic-746838-1893656-0, it will not have network services or connectivity: {"incident_id": "4276904F", "cause": {"incident_id": "162C2DC8", "type": "UnknownNetworkError"}, "name": "run", "context": "wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294", "__severity": "error", "msg": "\"ResourceParentStateInvalid(u'resource=(metadatahost:6473560:6441556:(nic-746838-1893656-0-metadatahost:network-746838-728466-metadataserver):[configuration-746838]:[UNDEPLOYED:(vgr:UNDEPLOYED)](locked)), parent_resource=(metadataserver:6441556:6441546:(network-746838-728466-metadataserver:network-746838-728466):[configuration-746838]:[DEPLOYED:(vgr:UNDEPLOYED)](unlocked)): ',)\"", "timestamp": "2013-09-11 15:24:25", "type": "InternalNetworkingError"}
  2013-09-11 15:24:25 tuk1m1cm4 [info] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [INFO] unprovision.py:198 - cleaning up orphaned vpns: vpn-129390,vpn-195962,vpn-542770,vpn-661130,vpn-661182,vpn-670476,vpn-711360
  2013-09-11 15:24:25 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [DEBUG] unprovision.py:204 - UnprovisionNetworksOperation: configuration-746838 {} ['vpn-542770', 'vpn-129390', 'vpn-661182', 'vpn-670476', 'vpn-195962', 'vpn-661130', 'vpn-711360']
  2013-09-11 15:24:29 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [DEBUG] vpns.py:697 - UnprovisionVPNsOperation: set([vpn-195962, vpn-670476, vpn-661182, vpn-542770, vpn-129390, vpn-661130])
  2013-09-11 15:24:29 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [DEBUG] network_provisioning.py:303 - unprovision_stale_nat_entries: []
  2013-09-11 15:24:29 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [DEBUG] network_provisioning.py:303 - unprovision_stale_nat_entries: []
  2013-09-11 15:24:29 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.api]: [DEBUG] ConfigurationManager.py:64 - run ASYNC_END 155.67
  {code}
  
  That is, cm couldn't correctly provision the nic, so it was left disconnected, and it entailed a manual stop/start of the VM to resolve it.
  
  NOTE: a disconnect/reconnect of the NIC would have done as well, but I didn't think of it at the time.
  
  I also didn't act quickly enough to catch it then, but I *presume* the NIC would have had an error attached to it, which could be automatically detected and, possibly, resolved.
** Comment: andrus
   :PROPERTIES:
   :ID:       111935
   :created:  2013-09-16 17:04:05
   :END:
  I expect this will entail changes to vmrunstateservice to scan for "busted" NICs and Do Something Responsible. Might also require a change to CM APIs.
* TODO Remove vend plumbing from CM for shared drives that use samba service :PL_6939:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: ybranch
  :type:     Task
  :priority: Major
  :status:   Open
  :components: cm
  :created:  2013-09-13 17:14:41
  :updated:  2013-09-16 15:27:07
  :ID:       PL-6939
  :END:
** description: PL-6939
  *If* we go full samba service in R49, these are not needed. 
** Comment: andrus
   :PROPERTIES:
   :ID:       111825
   :created:  2013-09-13 17:25:04
   :END:
  Apologies, but IDK the plans for phased deployment of samba. Are we running samba at all in production? 
  
  If not, I'd prefer to leave the vends in CM until we've got some experience with samba, and have run with it full-throttle-up for at least a release.
  
  So... r50? 
  
  
  
  

* TODO cmcmd fails ugly in resource region			    :PL_7158:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: andrus
  :type:     Bug
  :priority: Major
  :status:   In Progress
  :components: cm
  :created:  2013-10-17 21:07:27
  :updated:  2013-10-21 21:14:05
  :ID:       PL-7158
  :END:
** description: PL-7158
  {code}
  ---------------------------------------------------------------------------
  AttributeError                            Traceback (most recent call last)
  
  /highland/hosting_platform/hosting_platform/services/configuration_manager/tools/cmcmd.py in <module>()
      188     cmcmd = InterpreterProxy(ConfigurationManagerCommand())
      189     ConfigurationManagerCommand().execute_cmdline()  # in interpreter, this will print command help
  --> 190     debug_context("cmcmd by %s" % getuser())
      191
      192     def register_service(region, service_type, host, port, service_instance_id=''):
  
  /highland/hosting_platform/hosting_platform/services/configuration_manager/Context.py in debug_context(context_name)
       74         current_context.leave()
       75     else:
  ---> 76         db = ConfigurationDB.getInstance()
       77     return Context(context_name, "debug_instance", db=db,
       78                    mq=MQMessageProcessor(ConfigurationManagerAPI.SERVICE_NAME,
  
  /highland/hosting_platform/hosting_platform/common/singletonmixin.py in getInstance(cls, *lstArgs, **dctKwArgs)
      204                 raise SingletonException, 'Singleton already instantiated, but getInstance() called with args.'
      205         else:
  --> 206             _createSingletonInstance(cls, lstArgs, dctKwArgs)
      207
      208         return cls.cInstance
  
  /highland/hosting_platform/hosting_platform/common/singletonmixin.py in _createSingletonInstance(cls, lstArgs, dctKwArgs)
      136         instance = cls.__new__(cls)
      137         try:
  --> 138             instance.__init__(*lstArgs, **dctKwArgs)
      139         except TypeError, e:
      140             if "".join(map(str, e.args)).find('__init__() takes') != -1:
  
  /highland/hosting_platform/hosting_platform/services/configuration_manager/db.py in __init__(self, url, options)
      153     def __init__(self, url=None, options=None):
      154         db_config = config.DATABASES.get("configuration_db")
  --> 155         options = options or db_config.get('options')
      156
      157         super(ConfigurationDB, self).__init__("configuration", url=url, options=options)
  
  AttributeError: 'NoneType' object has no attribute 'get'
  WARNING: Failure executing file: </highland/hosting_platform/hosting_platform/services/configuration_manager/tools/cmcmd.py>
  Python 2.6.5 (r265:79063, Oct  1 2012, 22:04:36)
  Type "copyright", "credits" or "license" for more information.
  
  IPython 0.10 -- An enhanced Interactive Python.
  ?         -> Introduction and overview of IPython's features.
  %quickref -> Quick reference.
  help      -> Python's own help system.
  object?   -> Details about 'object'. ?object also works, ?? prints more.
  {code}
** Comment: bxiao
   :PROPERTIES:
   :ID:       115185
   :created:  2013-10-21 21:02:39
   :END:
  I have trouble repro the issue.
  
  Using ccmcmd to register CM on my dev node using both management region and resource region works fine.
  
** Comment: bxiao
  Test update from emacs
** Comment: bxiao
   :PROPERTIES:
   :ID:       115189
   :created:  2013-10-21 21:14:05
   :END:
  Test update from emacs
* TODO cm allows manual network connection to NAT vpn -> errors happen :PL_6839:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: pholland
  :type:     Bug
  :priority: Critical
  :status:   In Progress
  :components: cm
  :created:  2013-09-03 16:06:34
  :updated:  2013-10-21 20:59:41
  :ID:       PL-6839
  :END:
** description: PL-6839
  the conflict checking works!
  
  {noformat}
  2013-08-30 21:34:23 tuk1m1cm1 [err] [wfe.c8d59640f3d9013007ac005056a30055.901/trn.66089c30f3e9013031b074ce4686f1c0.105.1/configuration_manager-tuk1m1cm1-2.20268.run.7686.service]: [ERROR] errors.py:209 - service invocation failure for network_server.tunnel_nat_create(network-729538-766408-vpn-542770-nic-729538-1900624-0-network,network-729538-766408-vpn-542770-network-tunnel,10.1.0.100,10.0.0.1)
  --stack--
    File "/usr/lib/python2.6/threading.py", line 504, in __bootstrap
      self.__bootstrap_inner()
    File "/usr/lib/python2.6/threading.py", line 532, in __bootstrap_inner
      self.run()
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/threadutils.py", line 87, in run
      self.return_value = self.operation(*self.args, **self.kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 1515, in _execute
      self.provision_vpn_network(self.vpn_attachment)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 62, in wrapper
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 383, in provision_vpn_network
      tunnel.provision_routes(network_server)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/model/vpns.py", line 720, in provision_routes
      entry.address.dotted_quad, entry.nic.address.dotted_quad)
  --traceback--
  Traceback (most recent call last):
    File "/highland/hosting_platform/hosting_platform/common/errors/errors.py", line 325, in _service_error_handler
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/rpc/CallNode.py", line 128, in __call__
      raise e
  ProtocolError: (901, 'ParameterValueInvalid("param(local_ip), value(10.0.0.1): must be a valid host within network subnet \'10.1.5.0/24\'",)')
  : {"incident_id": "595F40E9", "cause": {"incident_id": "742BF49C", "type": "UnknownNetworkError"}, "name": "run", "context": "wfe.c8d59640f3d9013007ac005056a30055.901/trn.66089c30f3e9013031b074ce4686f1c0.105.1/configuration_manager-tuk1m1cm1-2.20268.run.7686", "__severity": "error", "msg": "'ParameterValueInvalid(\"param(local_ip), value(10.0.0.1): must be a valid host within network subnet \\'10.1.5.0/24\\'\",)'", "timestamp": "2013-08-30 21:34:23", "type": "InternalNetworkingError"}
  {noformat}
** Comment: andrus
   :PROPERTIES:
   :ID:       110896
   :created:  2013-09-03 17:01:03
   :updated:  2013-09-03 21:04:09
   :END:
  Here is the run that spawned this:
  
  
  {code}
  2013-08-30 21:34:00 tuk1m1cm1 [debug] [wfe.c8d59640f3d9013007ac005056a30055.901/trn.66089c30f3e9013031b074ce4686f1c0.105.1/configuration_manager-tuk1m1cm1-2.20268.run.7686.api]: [DEBUG] ConfigurationManager.py:43 - run CALLED (['account-157988', 'account-140372'], ['vm-729538-1903524'], 'configuration-729538') caller_options: {'samba_fileserver': False, 'prefer_kvm': False}
  {code}
  
  {code}
  configuration-729538 https://cloud.skytap.com/configurations/1038896
    network-729538-766408 - 10.1.5.0/24 - manual
    network-729538-708988 - 10.1.15.0/24 - automatic 
  {code}
** Comment: andrus
   :PROPERTIES:
   :ID:       111002
   :created:  2013-09-03 23:22:04
   :END:
  Beyond the specifics of the IP address we're trying to use, a bigger issue is that we've ended up with a manual network attached to a NAT VPN, which just won't work.
  

** Comment: andrus
   :PROPERTIES:
   :ID:       111003
   :created:  2013-09-03 23:23:28
   :END:
  Note that the VM being run is different than the VM mentioned in the error watch. 
  
  Notice also there are several failures like this in the logs today.
  
  I believe the run is triggering a general provisioning phase which keeps finding 'work to be done', attempting it, and failing.
  

** Comment: andrus
   :PROPERTIES:
   :ID:       111004
   :created:  2013-09-03 23:33:36
   :END:
  Support is already tracking with customer - advised them that this will *never* work and the best we can hope for is to disallow this misconfiguration in future.
  

** Comment: bschick
   :PROPERTIES:
   :ID:       111909
   :created:  2013-09-16 06:22:09
   :END:
  Let's make sure we get manual test cases added as well.
** Comment: andrus
   :PROPERTIES:
   :ID:       111924
   :created:  2013-09-16 13:26:46
   :END:
  By "manual test cases added" do you mean "add to QAIT BVT"?
  
** Comment: bschick
   :PROPERTIES:
   :ID:       112021
   :created:  2013-09-17 05:48:05
   :END:
  Yes
* TODO update CM systems tests to new multi-services model	    :PL_7232:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: andrus
  :type:     Bug
  :priority: Major
  :status:   Open
  :components: common
  :created:  2013-10-21 18:13:54
  :updated:  2013-11-01 21:57:21
  :ID:       PL-7232
  :END:
** description: PL-7232
  It's become clear over the last weeks that our integration efforts could be improved by more targeted testing. The most likely approach will be to "embrace and extend" the old CM-centric system tests, specifically to widen the scope to include more of the key platform services.
  
  I think it makes sense to *copy* the existing tests from CM/system_tests to hosting_platform/system_tests (i.e. peered with services) and start building them out to drive operations across the platform. 
  
  I think the order will be accounting, then GB/CM, then GB/awsdriver.
** Comment: mpietrek
   :PROPERTIES:
   :ID:       115127
   :created:  2013-10-21 18:17:38
   :END:
  Here is a diff of some code I wrote that made the existing CM system tests call through MQ to Greenbox instead. Happy to answer any questions:
  
  {code}
  -bash-3.2$ cat xmlrpc 
  # HG changeset patch
  # Parent ec474c96382e4548c399a91a8ee6fb3f36af9124
  
  diff -r ec474c96382e hosting_platform/services/configuration_manager/system_tests/base.py
  --- a/hosting_platform/services/configuration_manager/system_tests/base.py	Mon Jun 24 16:25:23 2013 +0000
  +++ b/hosting_platform/services/configuration_manager/system_tests/base.py	Tue Jun 25 17:04:28 2013 +0000
  @@ -32,6 +32,10 @@
   from hosting_platform.common.config import get_configuration
   from hosting_platform.services.framework import constant as service_constants
   from hosting_platform.services.ftp_server import FTPControlMixin
  +from xmlrpc_to_mq_adapter import XmlRpcToMQAdapter
  +from hosting_platform.services.configuration_manager.operations.StorageHelper import VendedDepot
  +import uuid
  +from hosting_platform.services.configuration_manager.model import Registrar
   
   API_RETRY_LOGGER = initialize_logger("api_retry")
   
  @@ -119,6 +123,17 @@
           setattr(self, attr, _attr)  # cache for later
           return _attr
   
  +class VMKeyWithAnnotations(str):
  +    """
  +    A small hack to bind the configuration instance to a vm_key. We need the configuration
  +    to get at the region, which is needed to mount the vm's depot. Lonnie and mpietrek
  +    cooked this up. Blame them.
  +    """
  +    def __new__(cls, s, configuration):
  +        obj = str.__new__(cls, s)
  +        obj.configuration = configuration
  +        return obj
  +
   class SystemTestBase(unittest.TestCase, FTPControlMixin):
       """
       Base class for system tests.
  @@ -148,7 +163,9 @@
           host = self.config.get('configuration_manager', {}).get('host', 'localhost')
           port = self.config.get('configuration_manager', {}).get('port', 9000)
   
  -        self.api = Service(ServiceConstants.CONFIGURATION_MANAGER, config.HIGHLAND_ENV, host, None, port)()
  +        # self.api = Service(ServiceConstants.CONFIGURATION_MANAGER, config.HIGHLAND_ENV, host, None, port)()
  +        self.api = XmlRpcToMQAdapter(ServiceConstants.CONFIGURATION_MANAGER, 'whatever', host, port)
  +
           if self.options.verbose_level:
               self.api = VerboseCalls(self, self.api)
   
  @@ -374,8 +391,24 @@
       def mounted_session_depot(self, vm_key):
           """mount session depot in a context"""
   
  -        depot_guid = self.api.get_vm_provisioning_details(vm_key)[vm_key]['depot_guid']
  +        provisioning_details = self.api.get_vm_provisioning_details(vm_key)[vm_key]
  +        depot_guid = provisioning_details['depot_guid']
   
  +        vend_token = 'my_vend_token_%s' % (uuid.uuid4())
  +
  +        storage_service = Registrar.lookup_storage_service(vm_key.configuration.region)
  +        try:
  +            sdepot = storage_service.vend_world(vend_token, depot_guid)
  +        except Exception as e:
  +            print e
  +            raise e
  +
  +        vended_depot = VendedDepot(vend_token, sdepot)
  +
  +        with vended_depot.mounted_depot_context() as mount_path:
  +            yield mount_path
  +
  +        """
           runCmd("skytap-cmd strgcmd mount %s" % depot_guid)
           vend_token, _ = runCmd("skytap-cmd strgcmd get_vends %s | tail -1 | cut -d' ' -f1" % depot_guid)
           vend_token = vend_token.split('[')[-1].strip()
  @@ -385,6 +418,7 @@
           finally:
               runCmd("skytap-cmd strgcmd unmount %s" % depot_guid)
               runCmd("skytap-cmd strgcmd release %s" % vend_token)
  +        """
   
       def add_vm(self, accounts, configuration, vm_template, cpus=1, ram=1024, force_populate=False):
           """add the vm in the vm_template to configuration"""
  @@ -412,7 +446,7 @@
           if force_populate:
               # RLA HACK - force the depot to really be cloned here by doing something that will vend it - for
               # now, use strgcmd to mount it
  -            with mounted_session_depot(vm_key):
  +            with self.mounted_session_depot(vm_key):
                   pass
   
               # LH - I'd like to do the set_vm_disks in parallel and poll_for_statuses at the end, but merge operation
  @@ -421,7 +455,7 @@
               self.poll_for_statuses("vm", configuration.vms.keys(), [VM.STATUSES.POWERED_OFF])
   
   
  -        return vm_key
  +        return VMKeyWithAnnotations(vm_key, configuration)
   
   
       def set_session_depot_attrs(self, vm_key, user_name, group_name, permissions, recurse=False):
  diff -r ec474c96382e hosting_platform/services/configuration_manager/system_tests/xmlrpc_to_mq_adapter.py
  --- /dev/null	Thu Jan 01 00:00:00 1970 +0000
  +++ b/hosting_platform/services/configuration_manager/system_tests/xmlrpc_to_mq_adapter.py	Tue Jun 25 17:04:28 2013 +0000
  @@ -0,0 +1,117 @@
  +'''
  +Created on Jun 13, 2013
  +
  +@author: mpietrek
  +'''
  +from hosting_platform.rpc import CallNodeProxy
  +from hosting_platform.common.mq_framework import MQMessageProcessor
  +from hosting_platform.common.logs import initialize_logger
  +from hosting_platform.common.errors import Error
  +
  +_LOGGER = initialize_logger("system_tests/xmlrpc_to_mq_adapter")
  +
  +MQ = MQMessageProcessor('unit_test', _LOGGER, None, None, 'xmlrpc_to_mq-1')
  +
  +CM_API_DESCRIPTOR = {}
  +
  +class InvokeMQ(object):
  +    def __init__(self, api_name):
  +        self.api_name = api_name
  +
  +    def __call__(self, *args, **kwargs):
  +
  +        cm_api = CM_API_DESCRIPTOR[self.api_name]
  +        gb_payload = {}
  +
  +        if len(kwargs):
  +            raise Exception('kwargs not yet supported')
  +
  +        # Iterate over all the arguments passed to us
  +        for i in range(0, len(args)):
  +            arg_name, _ = cm_api['arguments'][i]
  +            gb_arg_name, gb_arg_value = self.translate_argument(arg_name, args[i])
  +
  +            gb_payload[gb_arg_name] = gb_arg_value
  +
  +        print "XML_RPC call: %s %s" % (cm_api['greenbox_name'], gb_payload)
  +        reply_message = MQ.wait_for_reply_message('greenbox', cm_api['greenbox_name'], gb_payload)
  +        print "XML_RPC reply: %s %s" % (cm_api['greenbox_name'], reply_message.payload)
  +
  +        result = self.translate_reply(reply_message)
  +        print "XML_RPC returns: %s %s" % (cm_api['greenbox_name'], result)
  +
  +        return result
  +
  +    def translate_argument(self, arg_name, arg_value):
  +        if arg_name == 'source_config_key':  # Misnamed in CM API
  +            arg_name = 'source_configuration_key'
  +        if '_key_list' in arg_name:  # Changing param names relative to CM
  +            arg_name = arg_name.replace('_key_list', '_keys')
  +        if arg_name.startswith('account_depot_key'):  # Changing param names relative to CM
  +            arg_name = arg_name.replace('account_depot_key', 'depot_key')
  +        if arg_name.endswith('_key_versions'):  # We got rid of key_versions in CM - Replace with list of keys
  +            arg_name = arg_name.replace('_key_versions', '_keys')
  +            arg_value = arg_value.keys()
  +
  +        return arg_name, arg_value
  +
  +    def translate_reply(self, reply):
  +        if 'error' in reply:
  +            error = Error(reply.error['_original_exception'])
  +            print "XML_RPC raising %s" % (error)
  +            raise error.as_xmlrpclib_fault
  +
  +        result = reply['result']
  +
  +        if self.api_name.endswith('_specification'):
  +            for key, value in result.iteritems():
  +                result[key] = (value, 0)
  +            return result
  +        elif self.api_name == 'merge':
  +            return (result['target_configuration_key'], result['vm_mapping'], result['network_mapping'])
  +
  +        return reply['result']
  +
  +def create_call_node_function(_type, name, host, port, function, **kwargs):
  +    return InvokeMQ(name, host, port, function, **kwargs).__call__
  +
  +class XmlRpcToMQAdapter(CallNodeProxy):
  +    """
  +    def __init__(self, _type, region, host, instance_id, port, **kwargs):
  +        super(XmlRpcToMQAdapter, self).__init__(_type, host, port, **kwargs)
  +    """
  +
  +    def __getattr__(self, function, *args, **kwargs):
  +            if function == 'create_disk_policy':
  +                return super(XmlRpcToMQAdapter, self).__getattr__(function, *args, **kwargs)
  +            else:
  +                return InvokeMQ(function).__call__
  +
  +#===================================================================
  +from hosting_platform.services.configuration_manager.operations import api_operations as cm_api_operations
  +
  +RENAMED_APIS = \
  +{
  +'run': 'run_vms',
  +'shutdown': 'shutdown_vms',
  +'poweroff': 'poweroff_vms',
  +'suspend': 'suspend_vms',
  +'resize_desktop': 'resize_vm_desktop',
  +'get_export_ephemera': 'get_export_vm_ephemera',
  +'get_import_ephemera': 'get_import_vm_ephemera',
  +'delete_import': 'delete_import_vm',
  +'delete_export': 'delete_export_vm',
  +'attach_iso': 'attach_vm_iso',
  +}
  +
  +for cm_operation in cm_api_operations:
  +    if not hasattr(cm_operation, 'api_name'):
  +        continue
  +    if not hasattr(cm_operation, '__init__'):
  +        continue
  +
  +    cm_api_name = cm_operation.api_name()
  +    greenbox_name = cm_api_name if cm_api_name not in RENAMED_APIS else RENAMED_APIS[cm_api_name]
  +
  +    CM_API_DESCRIPTOR[cm_api_name] = {'greenbox_name': greenbox_name,
  +                                      'arguments': cm_operation.__init__.__parameter_validation_decls}
  {code}
  
** Comment: bxiao
   :PROPERTIES:
   :ID:       115324
   :created:  2013-10-22 16:45:30
   :END:
  Thanks Matt. I am working on loonieh cloud now. I saw many people has their own environment there, wonder whether I should do the same or just use lonnieh
* DONE Error Watch: "list index out of range" error unprovisioning networks during VM poweroff :PL_7270:
  :PROPERTIES:
  :assignee: evanc
  :reporter: evanc
  :type:     Bug
  :priority: Blocker
  :status:   Resolved
  :created:  2013-10-22 20:16:24
  :updated:  2013-10-24 03:18:46
  :ID:       PL-7270
  :resolution: Fixed
  :END:
** description: PL-7270
  {noformat}
  Sessions and configurations affected:
  
  configuration-615590
  ---------------------------------------------------------
  
  2013-10-22 19:55:47 tuk6m1cm2 [err] [wfe.aa0978701d81013181dc005056a30032.11/trn.f22e11f01d800131f358005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-5-0.17402.request.poweroff_vms.2082/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.15830.poweroff.40.configuration_manager]: [ERROR] errors.py:209 - failure unprovisioning network(s) for configuration-615590: {"incident_id": "753EFAE3", "name": "poweroff", "timestamp": "2013-10-22 19:55:47", "_original_exception": "IndexError('list index out of range',)", "__severity": "error", "context": "wfe.aa0978701d81013181dc005056a30032.11/trn.f22e11f01d800131f358005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-5-0.17402.request.poweroff_vms.2082/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.15830.poweroff.40", "msg": "'list index out of range'", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 228, in provision
      inactive_busy_networks=[n.key for n in self.networks.iterkeys()]).execute()
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 699, in __init__
      vpn.vpn_router_unprovision = vpn.router and len(active_networks) == 0
    File "/usr/lib/python2.6/contextlib.py", line 34, in __exit__
      self.gen.throw(type, value, traceback)
    File "/highland/hosting_platform/hosting_platform/db/db.py", line 106, in _run_transaction
      yield sess
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 638, in __init__
      vpn = self.load_vpn(vpn_key, tx, must_exist=False, for_update=True)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 292, in load_vpn
      return VPNProvisioningMixin.load_vpns([vpn_key], *args, **kwargs)[0]
  ----------------------------------------------------------------------
  2013-10-22 19:55:47 tuk6m1cm2 [err] [wfe.aa0978701d81013181dc005056a30032.11/trn.f22e11f01d800131f358005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-5-0.17402.request.poweroff_vms.2082/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.15830.poweroff.40.configuration_manager]: [ERROR] threadutils.py:95 - Error executing asynchronous operation asynch-op <function execute at 0x4e18c08>(, {}) : InternalError: ("'failure unprovisioning networks: list index out of range'", 'failure unprovisioning networks: list index out of range') at Traceback (most recent call last):
    File "/highland/hosting_platform/hosting_platform/common/threadutils.py", line 87, in run
      self.return_value = self.operation(*self.args, **self.kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 62, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 80, in execute
      self.provision()  # pylint: disable=E1101
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 627, in provision
      logger=self.logger, chained=self.chained, cleanup=self.cleanup).execute()
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 80, in execute
      self.provision()  # pylint: disable=E1101
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 241, in provision
      raise InternalError("failure unprovisioning networks: " + ",".join(map(str, e.args)))
  InternalError: InternalError: ("'failure unprovisioning networks: list index out of range'", 'failure unprovisioning networks: list index out of range')
  ----------------------------------------------------------------------
  {noformat}
** Comment: andrus
   :PROPERTIES:
   :ID:       115388
   :created:  2013-10-22 20:40:22
   :END:
  Bing - can you take a look at this issue? I see "VPN" in the stack and think of you ;)
** Comment: evanc
   :PROPERTIES:
   :ID:       115391
   :created:  2013-10-22 20:43:06
   :END:
  The config spec:
  {noformat}
  In [3]: api.get_configuration_specification({'configuration-615590':None})
  Out[3]: 
  {'configuration-615590': [{'account_keys': ['account-19866'],
     'disable_internet': False,
     'key': 'configuration-615590',
     'networks': [{'cidr_block': '192.168.4.0/24',
       'configuration_key': 'configuration-615590',
       'domain_name': 'test.net',
       'gateway': '192.168.4.254',
       'key': 'network-615590-454516',
       'nat_cidr': None,
       'nat_pool': None,
       'network_type': 'automatic',
       'primary_nameserver': None,
       'secondary_nameserver': None,
       'tunnels': [],
       'vpn_attachments': []},
      {'cidr_block': '192.168.3.0/24',
       'configuration_key': 'configuration-615590',
       'domain_name': 'test.net',
       'gateway': '192.168.3.254',
       'key': 'network-615590-454518',
       'nat_cidr': None,
       'nat_pool': None,
       'network_type': 'automatic',
       'primary_nameserver': None,
       'secondary_nameserver': None,
       'tunnels': [],
       'vpn_attachments': []},
      {'cidr_block': '192.168.2.0/24',
       'configuration_key': 'configuration-615590',
       'domain_name': 'test.net',
       'gateway': '192.168.2.254',
       'key': 'network-615590-454520',
       'nat_cidr': None,
       'nat_pool': None,
       'network_type': 'automatic',
       'primary_nameserver': None,
       'secondary_nameserver': None,
       'tunnels': [],
       'vpn_attachments': []},
      {'cidr_block': '192.168.1.0/24',
       'configuration_key': 'configuration-615590',
       'domain_name': 'example.com',
       'gateway': '192.168.1.254',
       'key': 'network-615590-454522',
       'nat_cidr': None,
       'nat_pool': None,
       'network_type': 'automatic',
       'primary_nameserver': None,
       'secondary_nameserver': None,
       'tunnels': [],
       'vpn_attachments': []}],
     'region': 'test/tuk6r1',
     'routable': False,
     'runnable': True,
     'vms': [{'hardware': {'cpus': '1',
        'disk_controllers': [{'bus_type': 'ide',
          'controller': '0',
          'disks': [{'device_type': 'cdrom',
            'key': 'disk-615590-4140420-ide-0-0',
            'lun': '0',
            'size': '0'}],
          'key': 'disk_controller-615590-4140420-ide-0',
          'virtual_dev': None},
         {'bus_type': 'scsi',
          'controller': '0',
          'disks': [{'device_type': 'disk',
            'key': 'disk-615590-4140420-scsi-0-0',
            'lun': '0',
            'size': '2048'}],
          'key': 'disk_controller-615590-4140420-scsi-0',
          'virtual_dev': 'lsilogic'}],
        'disk_policy': {'disk_limit': '15',
         'disk_size_limit': '2096128',
         'iops_limit': '275',
         'key': 'disk_policy-100',
         'offset': '165',
         'size_limit': '2097152'},
        'floppy': 'fd0',
        'guestOS': 'otherlinux-64',
        'guest_cpu_type': '4',
        'hardware_policy': {'max_cpus': '8',
         'max_ram': '32768',
         'min_cpus': '1',
         'min_ram': '256'},
        'hardware_version': '7',
        'host_cpu_type': None,
        'nics': [{'address': '192.168.1.1',
          'configuration_key': 'configuration-615590',
          'hostname': 'host1',
          'key': 'nic-615590-4140420-0',
          'mac': '00:50:56:2e:10:ca',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454522',
          'nic_type': 'e1000',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': 'e1000',
          'vm_key': 'vm-615590-4140420'}],
        'ram': '1024',
        'uuid': None,
        'vnc_keymap': None},
       'key': 'vm-615590-4140420',
       'vm_extra_options_groups': []},
      {'hardware': {'cpus': '1',
        'disk_controllers': [{'bus_type': 'ide',
          'controller': '0',
          'disks': [{'device_type': 'cdrom',
            'key': 'disk-615590-4140422-ide-0-0',
            'lun': '0',
            'size': '0'}],
          'key': 'disk_controller-615590-4140422-ide-0',
          'virtual_dev': None},
         {'bus_type': 'scsi',
          'controller': '0',
          'disks': [{'device_type': 'disk',
            'key': 'disk-615590-4140422-scsi-0-0',
            'lun': '0',
            'size': '2048'}],
          'key': 'disk_controller-615590-4140422-scsi-0',
          'virtual_dev': 'lsilogic'}],
        'disk_policy': {'disk_limit': '15',
         'disk_size_limit': '2096128',
         'iops_limit': '275',
         'key': 'disk_policy-100',
         'offset': '165',
         'size_limit': '2097152'},
        'floppy': 'fd0',
        'guestOS': 'otherlinux-64',
        'guest_cpu_type': '4',
        'hardware_policy': {'max_cpus': '8',
         'max_ram': '32768',
         'min_cpus': '1',
         'min_ram': '256'},
        'hardware_version': '7',
        'host_cpu_type': None,
        'nics': [{'address': '192.168.1.2',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-1',
          'key': 'nic-615590-4140422-0',
          'mac': '00:50:56:24:6B:2F',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454522',
          'nic_type': 'e1000',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': 'e1000',
          'vm_key': 'vm-615590-4140422'},
         {'address': '192.168.2.1',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-1',
          'key': 'nic-615590-4140422-1',
          'mac': '00:50:56:05:74:4E',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454520',
          'nic_type': 'default',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': None,
          'vm_key': 'vm-615590-4140422'},
         {'address': '192.168.3.1',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-1',
          'key': 'nic-615590-4140422-2',
          'mac': '00:50:56:22:55:3B',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454518',
          'nic_type': 'default',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': None,
          'vm_key': 'vm-615590-4140422'}],
        'ram': '1024',
        'uuid': None,
        'vnc_keymap': None},
       'key': 'vm-615590-4140422',
       'vm_extra_options_groups': []},
      {'hardware': {'cpus': '1',
        'disk_controllers': [{'bus_type': 'ide',
          'controller': '0',
          'disks': [{'device_type': 'cdrom',
            'key': 'disk-615590-4140424-ide-0-0',
            'lun': '0',
            'size': '0'}],
          'key': 'disk_controller-615590-4140424-ide-0',
          'virtual_dev': None},
         {'bus_type': 'scsi',
          'controller': '0',
          'disks': [{'device_type': 'disk',
            'key': 'disk-615590-4140424-scsi-0-0',
            'lun': '0',
            'size': '2048'}],
          'key': 'disk_controller-615590-4140424-scsi-0',
          'virtual_dev': 'lsilogic'}],
        'disk_policy': {'disk_limit': '15',
         'disk_size_limit': '2096128',
         'iops_limit': '275',
         'key': 'disk_policy-100',
         'offset': '165',
         'size_limit': '2097152'},
        'floppy': 'fd0',
        'guestOS': 'otherlinux-64',
        'guest_cpu_type': '4',
        'hardware_policy': {'max_cpus': '8',
         'max_ram': '32768',
         'min_cpus': '1',
         'min_ram': '256'},
        'hardware_version': '7',
        'host_cpu_type': None,
        'nics': [{'address': '192.168.1.3',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-2',
          'key': 'nic-615590-4140424-0',
          'mac': '00:50:56:3A:D9:22',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454522',
          'nic_type': 'e1000',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': 'e1000',
          'vm_key': 'vm-615590-4140424'}],
        'ram': '1024',
        'uuid': None,
        'vnc_keymap': None},
       'key': 'vm-615590-4140424',
       'vm_extra_options_groups': []},
      {'hardware': {'cpus': '1',
        'disk_controllers': [{'bus_type': 'ide',
          'controller': '0',
          'disks': [{'device_type': 'cdrom',
            'key': 'disk-615590-4140426-ide-0-0',
            'lun': '0',
            'size': '0'}],
          'key': 'disk_controller-615590-4140426-ide-0',
          'virtual_dev': None},
         {'bus_type': 'scsi',
          'controller': '0',
          'disks': [{'device_type': 'disk',
            'key': 'disk-615590-4140426-scsi-0-0',
            'lun': '0',
            'size': '2048'}],
          'key': 'disk_controller-615590-4140426-scsi-0',
          'virtual_dev': 'lsilogic'}],
        'disk_policy': {'disk_limit': '15',
         'disk_size_limit': '2096128',
         'iops_limit': '275',
         'key': 'disk_policy-100',
         'offset': '165',
         'size_limit': '2097152'},
        'floppy': 'fd0',
        'guestOS': 'otherlinux-64',
        'guest_cpu_type': '4',
        'hardware_policy': {'max_cpus': '8',
         'max_ram': '32768',
         'min_cpus': '1',
         'min_ram': '256'},
        'hardware_version': '7',
        'host_cpu_type': None,
        'nics': [{'address': '192.168.1.4',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-3',
          'key': 'nic-615590-4140426-0',
          'mac': '00:50:56:38:C1:D0',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454522',
          'nic_type': 'e1000',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': 'e1000',
          'vm_key': 'vm-615590-4140426'},
         {'address': '192.168.2.2',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-2',
          'key': 'nic-615590-4140426-1',
          'mac': '00:50:56:0F:3B:A0',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454520',
          'nic_type': 'default',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': None,
          'vm_key': 'vm-615590-4140426'},
         {'address': '192.168.3.2',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-2',
          'key': 'nic-615590-4140426-2',
          'mac': '00:50:56:2C:6C:33',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454518',
          'nic_type': 'default',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': None,
          'vm_key': 'vm-615590-4140426'},
         {'address': '192.168.4.1',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-1',
          'key': 'nic-615590-4140426-3',
          'mac': '00:50:56:1C:FA:32',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454516',
          'nic_type': 'default',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': None,
          'vm_key': 'vm-615590-4140426'}],
        'ram': '1024',
        'uuid': None,
        'vnc_keymap': None},
       'key': 'vm-615590-4140426',
       'vm_extra_options_groups': []}]},
    0]}
  {noformat}
  
** Comment: bxiao
   :PROPERTIES:
   :ID:       115583
   :created:  2013-10-23 17:12:44
   :END:
  Investigating
** Comment: bxiao
   :PROPERTIES:
   :ID:       115650
   :created:  2013-10-23 20:33:40
   :END:
   Fix submitted to my repo, waiting for jenkens build
** Comment: bxiao
   :PROPERTIES:
   :ID:       115738
   :created:  2013-10-24 03:18:46
   :END:
  fix pushed to integ
* DONE Error watch: run failed due to AttributeError		    :PL_7307:
  :PROPERTIES:
  :assignee: nastete
  :reporter: nastete
  :type:     Bug
  :priority: Blocker
  :status:   Closed
  :components: cm
  :created:  2013-10-23 23:19:39
  :updated:  2013-10-24 00:16:01
  :ID:       PL-7307
  :resolution: Cannot Reproduce
  :END:
** description: PL-7307
  (Since test is currently down, I can't use a console to check the impact of this. Therefore, I'm filing it as a blocker.)
  
  {noformat}
  2013-10-23 23:15:07 tuk6m1trun1 [err] [/wfe.d61384201e66013181e4005056a30032.5/trn.eb151f101e630131f37a005056a30030.5]: [ERROR] logging.rb:47 - API EXCEPTION! The operation cannot be performed at this time. Please try again or contact support. (UnhandledException, 10C03116) [Platform::Client::Error]
   (in: run(["ctag-e10f53701e660131f37a005056a30030", ["vm-49804-80746"], "configuration-49804"])(caller_options: {"prefer_kvm"=>true, "samba_fileserver"=>false})
  
  ...
  
  2013-10-23 23:15:07 tuk6m1cm2 [err] [wfe.d61384201e66013181e4005056a30032.5/trn.eb151f101e630131f37a005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-0-0.25956.request.run_vms.42/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.15061.run.15.api]: [ERROR] errors.py:209 - run EXCEPT 0.44 : {"incident
  _id": "10C03116", "name": "run", "timestamp": "2013-10-23 23:15:07", "_original_exception": "AttributeError(\"type object 'Limit' has no attribute 'region'\",)", "__severity": "error", "context": "wfe.d61384201e66013181e4005056a30032.5/trn.eb151f101e630131f37a005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skyta
  p.com-0-0.25956.request.run_vms.42/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.15061.run.15", "msg": "\"type object 'Limit' has no attribute 'region'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 31, in common_error_handling
      yield
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 45, in run
      ret = func(cm_instance, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/mixin.py", line 29, in <lambda>
      method = wraps(operation)(lambda self, *args, **kwargs: executor(self, operation, *args, **kwargs))
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 77, in operation_executor
      op = operation(*args, logger=LOGGER, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/provision.py", line 172, in __init__
      self.allocate_resources()
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/provision.py", line 239, in allocate_resources
      self.apply_rate_limits([a.account_id for a in accounts], self.api_name(), self.vms, self.networks)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 377, in apply_rate_limits
      limiter.check_and_reserve(limits)
    File "/usr/lib/python2.6/contextlib.py", line 34, in __exit__
      self.gen.throw(type, value, traceback)
    File "/highland/hosting_platform/hosting_platform/db/db.py", line 106, in _run_transaction
      yield sess
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 374, in apply_rate_limits
      limits = tx.query(Limit).filter(Limit.account_id.in_(account_ids)).filter(Limit.region == region).all()
  {noformat}
** Comment: nastete
   :PROPERTIES:
   :ID:       115698
   :created:  2013-10-23 23:20:47
   :END:
  Also occurs for suspend:
  
  {noformat}
  2013-10-23 21:15:11 tuk6m1trun1 [err] [/wfe.79d9f7901e5501316d31005056a3002c.10/trn.133735e01e450131f372005056a30030.9]: [ERROR] logging.rb:47 - API EXCEPTION! The operation cannot be performed at this time. Please try again or contact support. (UnhandledException, 3AFC4C13) [Platform::Client::Error]
   (in: suspend([["vm-49804-80746"]])(caller_options: {"prefer_kvm"=>true, "samba_fileserver"=>false})
  
  ...
  
  2013-10-23 21:15:11 tuk6m1cm3 [err] [wfe.79d9f7901e5501316d31005056a3002c.10/trn.133735e01e450131f372005056a30030.9.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-2-0.23729.request.suspend_vms.52/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-1.26080.suspend.34.api]: [ERROR] errors.py:209 - suspend EXCEPT 0.21
   : {"incident_id": "3AFC4C13", "name": "suspend", "timestamp": "2013-10-23 21:15:11", "_original_exception": "AttributeError(\"type object 'Limit' has no attribute 'region'\",)", "__severity": "error", "context": "wfe.79d9f7901e5501316d31005056a3002c.10/trn.133735e01e450131f372005056a30030.9.1@/greenbox-tuk6m1
  gb1.mgt.test.skytap.com-2-0.23729.request.suspend_vms.52/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-1.26080.suspend.34", "msg": "\"type object 'Limit' has no attribute 'region'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 31, in common_error_handling
      yield
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 45, in suspend
      ret = func(cm_instance, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/mixin.py", line 29, in <lambda>
      method = wraps(operation)(lambda self, *args, **kwargs: executor(self, operation, *args, **kwargs))
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 77, in operation_executor
      op = operation(*args, logger=LOGGER, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 1239, in __init__
      self.apply_rate_limits(self.account_ids, self.api_name(), self.vms, self.networks)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 377, in apply_rate_limits
      limiter.check_and_reserve(limits)
    File "/usr/lib/python2.6/contextlib.py", line 34, in __exit__
      self.gen.throw(type, value, traceback)
    File "/highland/hosting_platform/hosting_platform/db/db.py", line 106, in _run_transaction
      yield sess
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 374, in apply_rate_limits
      limits = tx.query(Limit).filter(Limit.account_id.in_(account_ids)).filter(Limit.region == region).all()
  {noformat}
** Comment: bxiao
   :PROPERTIES:
   :ID:       115700
   :created:  2013-10-23 23:30:45
   :END:
  investigating
** Comment: andrus
   :PROPERTIES:
   :ID:       115707
   :created:  2013-10-23 23:48:36
   :END:
  Note:
  
  From ipython
  
  {code}
  In [5]: from hosting_platform.services.configuration_manager.db import Limit
  
  In [6]: Limit.region
  ---------------------------------------------------------------------------
  AttributeError                            Traceback (most recent call last)
  
  /highland/<ipython console> in <module>()
  
  AttributeError: type object 'Limit' has no attribute 'region'
  {code}
  
  Looks familiar.
  
  I wonder if this is some wacky sqlalchemy mapper issue? 
  
** Comment: andrus
   :PROPERTIES:
   :ID:       115709
   :created:  2013-10-23 23:50:33
   :updated:  2013-10-23 23:53:02
   :END:
  In INTEG, in cmcmd - this works:
  
  {code}
  In [13]: with transaction() as tx:
     ....:     lims = tx.query(Limit).filter(Limit.region==region).all()
     ....:
     ....:
  
  In [14]: lims
  
  /// prints lots of stuff
  
  In [17]: lim = lims[-1]
  
  In [18]: lim.region
  Out[18]: 'integ/tuk5r1'
  
  {code}
  
** Comment: andrus
   :PROPERTIES:
   :ID:       115710
   :created:  2013-10-23 23:52:33
   :END:
  But in test:
  
  {code}
  In [10]: with transaction() as tx:
     ....:     lims =  tx.query(Limit).all()
     ....:
  
  
  In [11]:
  
  In [11]: len(lims)
  Out[11]: 8417
  
  In [12]: lims[-1]
  Out[12]: <hosting_platform.services.configuration_manager.model.limits.Limit at 0x5f0a290>
  
  In [13]: lim = lims[-1]
  
  In [14]: lim.region
  ---------------------------------------------------------------------------
  AttributeError                            Traceback (most recent call last)
  <ipython-input-14-7c149f878f12> in <module>()
  ----> 1 lim.region
  
  AttributeError: 'Limit' object has no attribute 'region'
  
  {code}
  
** Comment: andrus
   :PROPERTIES:
   :ID:       115711
   :created:  2013-10-23 23:56:13
   :END:
  hg id
  
  23:53 test highland@tuk6m1logger1:~/hosting_platform$ hg id ece991cbe3ec tip
  
  hg pull -u
  
  now
  
  23:54 test highland@tuk6m1logger1:~/hosting_platform$ hg id d52721e16c04 tip
  
  {code}
  In [1]: with transaction() as tx:
      lims =  tx.query(Limit).all()
     ...:
  
  In [2]: lim = lims[-1]
  
  In [3]: lim.region
  Out[3]: 'test/tuk6r1
  {code}
  
** Comment: andrus
   :PROPERTIES:
   :ID:       115712
   :created:  2013-10-24 00:02:33
   :END:
  Hhm. I reverted hosting_platform (TEST) to ece991cbe3ec and the problem DOES NOT repro.
  
  I found a bunch of .pyc files and removed them. Still no repro.
  
  Very weird.
  
** Comment: bxiao
   :PROPERTIES:
   :ID:       115715
   :created:  2013-10-24 00:03:54
   :END:
  Cannot repro now. Resolving.
** Comment: lonnieh
   :PROPERTIES:
   :ID:       115717
   :created:  2013-10-24 00:15:36
   :END:
  I believe when Yelena restored the old version to test, the region field disappeared because it was added in R49. Until the master process was restarted, all the children spawned from it would have mappers that didn't have the attribute, and would fail with this.
  
  Nothing to see here.
* TODO regional rate limits can trivially be misconfigured without us knowing about it :PL_7280:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: lonnieh
  :type:     Bug
  :priority: Critical
  :status:   Open
  :components: cm
  :created:  2013-10-23 15:07:42
  :updated:  2013-10-25 16:39:19
  :ID:       PL-7280
  :END:
** description: PL-7280
  I happened to notice this warning (which doesn't go to error watch or fail service startup) while investigating another bug in test. I think this should be fatal and prevent service startup:
  
  {noformat}
  2013-10-22 10:56:20 tuk6m1cm1 [warning] [wfe.7a1f10201d3601316d13005056a3002c.2.11@/greenbox-tuk6m1gb2.mgt.test.skytap.com-5-0.5659.request.merge.3314/qm-merge-
  0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.23123.merge.16.ratelimiter]: [WARNING] limits.py:172 - Cannot load regional configuration: Configuration
  Error: ("'Configuration file not found for hosting_platform'", 'Configuration file not found for hosting_platform'). 
  {noformat}
** Comment: bxiao
   :PROPERTIES:
   :ID:       116011
   :created:  2013-10-25 16:39:19
   :END:
   The reason we do it this way is we assume that there might be regions that do not have regional specific configs, thus we fallback to non-regional config in such case. Is this assumption still right or all regions must have regional configs?
* TODO Rate limiter logging should include information on which region a particular operation is being evaluated against :PL_7370:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: evanc
  :type:     Bug
  :priority: Critical
  :status:   Open
  :components: cm
  :created:  2013-10-24 18:22:31
  :updated:  2013-10-25 15:38:54
  :ID:       PL-7370
  :END:
** description: PL-7370
  The logging for CM rate limiting looks something like this:
  {noformat}
  2013-10-24 18:18:42 tuk6m1cm2 [debug] [wfe.74abec001f06013181f2005056a30032.5/trn.7d56cc301f060131f388005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-3-0.27659.request.run_vms.10692/qm-run-5/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.24358.run.37.ratelimiter]: [DEBUG] limits.py:302 - ratelimiter costs: op run, storage 36.69, host 40.00, net 0.00
  2013-10-24 18:18:42 tuk6m1cm2 [debug] [wfe.74abec001f06013181f2005056a30032.5/trn.7d56cc301f060131f388005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-3-0.27659.request.run_vms.10692/qm-run-5/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.24358.run.37.ratelimiter]: [DEBUG] limits.py:410 - ratelimiter allow: op run, max_account_id 19054, max_outstanding -5, threshold 120
  {noformat}
  
  Since the rate limiter is now evaluating operations on a per region basis for per region limits, the region information should be included in the logging statements for the ease of following up on issues.
* TODO orphan vpn unprovisioning errors can lead to errors being incorrectly attributed to unrelated entities :PL_6441:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: lonnieh
  :type:     Bug
  :priority: Blocker
  :status:   In Progress
  :components: cm
  :created:  2013-06-25 17:43:33
  :updated:  2013-10-30 00:21:07
  :ID:       PL-6441
  :END:
** description: PL-6441
  Evan approached me to ask how a non-runnable configuration without vpns ended up with an error about unprovisioning vpns after merging the template into a non-running runnable configuration. After my initial skepticism, I have confirmed that this really did happen because the Unprovisioning failed while unprovisioning orphaned vpns. It looks like this error in a GC operation leaked to the invoking operation (there is no chaining). Needless to say, errors during GC unprovisioning operations should not impede the primary operation.
  
  {noformat}
  2013-06-17 05:14:56 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:14:56 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.api]: [DEBUG] ConfigurationManager.py:43 - merge CALLED (['account-9186', 'account-9182'], 'configuration-335354', ['vm-335354-1722024'], [], 'configuration-443680', None, False) caller_options: {'prefer_kvm': True}
  2013-06-17 05:14:56 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:14:56 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.configuration_manager]: [DEBUG] unprovision.py:507 - MergeOperation: [vm-335354-1722024]
  2013-06-17 05:14:57 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:14:57 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.ratelimiter]: [DEBUG] limits.py:256 - ratelimiter costs: op merge, storage 0.44, host 0.00, net 0.00
  2013-06-17 05:14:57 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:14:57 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.api]: [DEBUG] ConfigurationManager.py:46 - merge RETURN 0.36 ('configuration-443680', {'vm-335354-1722024': 'vm-443680-2549008'}, {})
  2013-06-17 05:14:57 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:14:57 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.api]: [DEBUG] ConfigurationManager.py:60 - merge ASYNC_BEGIN
  2013-06-17 05:14:57 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:14:57 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.configuration_manager]: [INFO] unprovision.py:198 - cleaning up orphaned vpns: vpn-418622,vpn-418624,vpn-418708
  2013-06-17 05:14:57 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:14:57 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.configuration_manager]: [DEBUG] unprovision.py:204 - UnprovisionNetworksOperation: configuration-335354 {} ['vpn-418622', 'vpn-418708', 'vpn-418624']
  2013-06-17 05:14:57 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:14:57 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.configuration_manager]: [DEBUG] vpns.py:705 - UnprovisionVPNsOperation: set([vpn-418622, vpn-418624, vpn-418708])
  2013-06-17 05:14:57 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:14:57 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.call_node_trace]: [DEBUG] CallNode.py:95 - 82854608: calling tuk6r1nsvip1:9002.tunnel_route_destroy(('network-419380-352226-418708-172.16.0.0/20',)) caller_options: {'prefer_kvm': True}
  2013-06-17 05:15:00 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:15:00 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.call_node_trace]: [DEBUG] CallNode.py:145 - 82854608: raised (822, 'ResourceOperationRecoverableError(u\'resource(gretunnelroute:2269180:2269172:(network-419380-352226-418708-172.16.0.0/20):[configuration-419380]:[DEPLOYED:(vgr:DEPLOYED)](locked)): \\\'python /var/run/resources/router-2268240/network-2268242/gretunnel-2269172/gretunnelroute-2269180/scripts/executor.py undeploy\\\':(1):\\\'recoverable remotescript error: original error [ip route del 172.16.0.0/20 dev vrt11 table 1510 && ip route flush cache : 255 : Cannot find device "vrt11"\\n]\\n\\\': SystemResourceOperationRecoverableError: [607] [WARNING] system_resource(vgr-10.69.240.226:virtual_gateway_router:1314:[STARTED](locked)), \\\'python /var/run/resources/router-2268240/network-2268242/gretunnel-2269172/gretunnelroute-2269180/scripts/executor.py undeploy\\\':(1):\\\'recoverable remotescript error: original error [ip route del 172.16.0.0/20 dev vrt11 table 1510 && ip route flush cache : 255 : Cannot find device "vrt11"\\n]\\n\\\'\',)')
  2013-06-17 05:15:00 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:15:00 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.service]: [ERROR] errors.py:210 - service invocation failure for network_server.tunnel_route_destroy(network-419380-352226-418708-172.16.0.0/20) 
  : {"incident_id": "6F32578C", "cause": {"incident_id": "4BA2267E", "type": "UnknownNetworkError"}, "name": "merge", "context": "wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46", "__severity": "error", "msg": "'ResourceOperationRecoverableError(u\\'resource(gretunnelroute:2269180:2269172:(network-419380-352226-418708-172.16.0.0/20):[configuration-419380]:[DEPLOYED:(vgr:DEPLOYED)](locked)): \\\\\\'python /var/run/resources/router-2268240/network-2268242/gretunnel-2269172/gretunnelroute-2269180/scripts/executor.py undeploy\\\\\\':(1):\\\\\\'recoverable remotescript error: original error [ip route del 172.16.0.0/20 dev vrt11 table 1510 && ip route flush cache : 255 : Cannot find device \"vrt11\"\\\\n]\\\\n\\\\\\': SystemResourceOperationRecoverableError: [607] [WARNING] system_resource(vgr-10.69.240.226:virtual_gateway_router:1314:[STARTED](locked)), \\\\\\'python /var/run/resources/router-2268240/network-2268242/gretunnel-2269172/gretunnelroute-2269180/scripts/executor.py undeploy\\\\\\':(1):\\\\\\'recoverable remotescript error: original error [ip route del 172.16.0.0/20 dev vrt11 table 1510 && ip route flush cache : 255 : Cannot find device \"vrt11\"\\\\n]\\\\n\\\\\\'\\',)'", "timestamp": "2013-06-17 05:15:00", "type": "InternalNetworkingError"}  
  2013-06-17 05:15:00 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:15:00 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.configuration_manager]: [ERROR] errors.py:210 - failure unprovisioning network(s) for configuration-335354: {"incident_id": "18E5F292", "name": "merge", "context": "wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46", "__severity": "error", "msg": "'failed to unprovision vpns: vpn-418708'", "timestamp": "2013-06-17 05:15:00", "type": "InternalError"}  
  2013-06-17 05:15:00 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:15:00 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.configuration_manager]: [DEBUG] network_provisioning.py:294 - unprovision_stale_nat_entries: []
  2013-06-17 05:15:00 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:15:00 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.configuration_manager]: [ERROR] operations.py:135 - operation completed with exclusions: configurations: [] vms: [vm-443680-2549008] networks: [] account_depots: [] vpns: [] tunnels: [] tunnel_consumers: []
  2013-06-17 05:15:00 tuk6m1cm1.mgt.test.skytap.com 2013-06-17: 05:15:00 tuk6m1cm1 [] [wfe.9c74f880b92701307e38005056a30032.579.4/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.6131.merge.46.api]: [DEBUG] ConfigurationManager.py:64 - merge ASYNC_END 3.78
  {noformat}
** Comment: ybranch
   :PROPERTIES:
   :ID:       105610
   :created:  2013-07-02 14:49:41
   :END:
  {code}
  ---------------------------------------------------------
  
  2013-07-02 11:48:28 tuk6m1cm2 [err] [trn.fd957a00c4f00130efef005056a30030.2282/trn.fd8bb220c4f00130efef005056a30030.2258.1/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.31829.run.196.configuration_manager]: [ERROR] errors.py:209 - failure unprovisioning network(s) for configuration-501100: {"incident_id": "4B6473C3", "name": "run", "timestamp": "2013-07-02 11:48:28", "_original_exception": "AttributeError(\"'NoneType' object has no attribute 'router_consumer'\",)", "__severity": "error", "context": "trn.fd957a00c4f00130efef005056a30030.2282/trn.fd8bb220c4f00130efef005056a30030.2258.1/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.31829.run.196", "msg": "\"'NoneType' object has no attribute 'router_consumer'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 222, in provision
      inactive_busy_networks=[n.key for n in self.networks.iterkeys()]).execute()
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 701, in __init__
      vpn.vpn_router_unprovision = vpn.router and len(active_networks) == 0
    File "/usr/lib/python2.6/contextlib.py", line 34, in __exit__
      self.gen.throw(type, value, traceback)
    File "/highland/hosting_platform/hosting_platform/db/db.py", line 105, in _run_transaction
      yield sess
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 639, in __init__
      vpns = self.load_vpns(vpn_keys, tx, must_exist=False, for_update=True) if vpn_keys else set([])
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 275, in load_vpns
      attachment.network.router_consumer
  ----------------------------------------------------------------------
  2013-07-02 11:48:28 tuk6m1cm2 [err] [trn.fd957a00c4f00130efef005056a30030.2282/trn.fd8bb220c4f00130efef005056a30030.2258.1/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.31829.run.196.configuration_manager]: [ERROR] threadutils.py:95 - Error executing asynchronous operation asynch-op <function execute at 0x4bce7d0>(, {}) : InternalError: ('"failure unprovisioning networks: \'NoneType\' object has no attribute \'router_consumer\'"', "failure unprovisioning networks: 'NoneType' object has no attribute 'router_consumer'") at Traceback (most recent call last):
    File "/highland/hosting_platform/hosting_platform/common/threadutils.py", line 87, in run
      self.return_value = self.operation(*self.args, **self.kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 62, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 80, in execute
      self.provision()  # pylint: disable=E1101
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/provision.py", line 406, in provision
      unprovision_operation.execute()
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 80, in execute
      self.provision()  # pylint: disable=E1101
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 235, in provision
      raise InternalError("failure unprovisioning networks: " + ",".join(map(str, e.args)))
  InternalError: InternalError: ('"failure unprovisioning networks: \'NoneType\' object has no attribute \'router_consumer\'"', "failure unprovisioning networks: 'NoneType' object has no attribute 'router_consumer'")
  ----------------------------------------------------------------------
  {code}
  
  The config in question has no networks
** Comment: lonnieh
   :PROPERTIES:
   :ID:       116538
   :created:  2013-10-29 15:00:34
   :END:
  PL-7471 was a case where this error caused exclusions to be leaked and required manual clean up. I think this is different from before r49...I don't remember having to release exclusions for this issue previously.
** Comment: lonnieh
   :PROPERTIES:
   :ID:       116540
   :created:  2013-10-29 15:05:27
   :END:
  The fix for PL-6727 appears to have introduced the possibility of leaking exclusions when there are reification errors. Increasing to blocker and pulling in to r49.
** Comment: lonnieh
   :PROPERTIES:
   :ID:       116545
   :created:  2013-10-29 16:04:01
   :END:
  Ross and I discussed handing this one back to you since it is related to the work you did on vpns in r49. Grab me when you are ready so we can discuss what you need go get started on this.  Thanks
** Comment: bxiao
   :PROPERTIES:
   :ID:       116548
   :created:  2013-10-29 16:50:16
   :END:
  Looking
** Comment: bxiao
   :PROPERTIES:
   :ID:       116670
   :created:  2013-10-29 23:17:04
   :END:
  Still working on repro with unit test
** Comment: bxiao
   :PROPERTIES:
   :ID:       116693
   :created:  2013-10-30 00:21:07
   :END:
  Seems got a repro in unit test, but still working on the exclusion and how to verify it in unit test.
* TODO Rate Limiting: Charged operation which fails to complete is still counted against the total :PL_7468:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: evanc
  :type:     Bug
  :priority: Blocker
  :status:   Open
  :components: ratelimiting
  :created:  2013-10-29 01:28:37
  :updated:  2013-11-08 19:04:30
  :ID:       PL-7468
  :END:
** description: PL-7468
  It appears as though a large operation which is approved by rate limiting, but then fails for another reason, is still changed against the customer.
  
  Steps:
  1) An 80 SVM operation was attempted (Run 10 - 8 SVM VMs), and the log shows it was approved by rate limiting:
  {noformat} 2013-10-29 01:18:31 tuk6m1cm1 [debug]
  [wfe.827c71d0226401318231005056a30032.4/trn.59c84d6022650131f3b1005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-3-0.23408.request.run_vms.1465/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.16750.run.19.ratelimiter]:
  [DEBUG] limits.py:302 - ratelimiter costs: op run, storage 152.12,
  host 68.18, net 0.00 2013-10-29 01:18:31 tuk6m1cm1 [debug]
  [wfe.827c71d0226401318231005056a30032.4/trn.59c84d6022650131f3b1005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-3-0.23408.request.run_vms.1465/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.16750.run.19.ratelimiter]:
  [DEBUG] limits.py:410 - ratelimiter allow: op run, max_account_id
  10012, max_outstanding -39, threshold 30 {noformat}
  
  But then the operation failed due to exceeding the SVM quota by 8 SVMs
  2) The attempted operation was reduced to 72 SVMs (Run 9 - 8 SVM VMs)
  
  Results: The log shows that rate limiting blocks this operation even
  though no other operations have been performed for this customer:
  {noformat} 2013-10-29 01:18:53 tuk6m1cm3 [debug]
  [wfe.b0aac1f0226101318219005056a30032.12/trn.59c84d6022650131f3b1005056a30030.6.1@/greenbox-tuk6m1gb2.mgt.test.skytap.com-0-0.31591.request.run_vms.1398/qm-run-0/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-1.11059.run.16.ratelimiter]:
  [DEBUG] limits.py:302 - ratelimiter costs: op run, storage 135.71,
  host 61.36, net 0.00 2013-10-29 01:18:53 tuk6m1cm3 [info]
  [wfe.b0aac1f0226101318219005056a30032.12/trn.59c84d6022650131f3b1005056a30030.6.1@/greenbox-tuk6m1gb2.mgt.test.skytap.com-0-0.31591.request.run_vms.1398/qm-run-0/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-1.11059.run.16.ratelimiter]:
  [INFO] limits.py:394 - ratelimiter block: op run, max_account_id
  10012, max_outstanding 129, threshold_exceeded 30, delay 118 (limit
  times: account_id 10012, region test/tuk6r2, storage_time 2013-10-29
  01:21:03, host_time 2013-10-29 01:19:39, network_time 2013-10-29
  01:18:31) 2013-10-29 01:20:58 tuk6m1cm1 [debug]
  [wfe.b0aac1f0226101318219005056a30032.12/trn.8beb4ae022650131f3b1005056a30030.6.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-9-0.23421.request.run_vms.1417/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.17147.run.9.ratelimiter]:
  [DEBUG] limits.py:302 - ratelimiter costs: op run, storage 135.71,
  host 61.36, net 0.00 2013-10-29 01:20:58 tuk6m1cm1 [debug]
  [wfe.b0aac1f0226101318219005056a30032.12/trn.8beb4ae022650131f3b1005056a30030.6.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-9-0.23421.request.run_vms.1417/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.17147.run.9.ratelimiter]:
  [DEBUG] limits.py:410 - ratelimiter allow: op run, max_account_id
  10012, max_outstanding 4, threshold 30 {noformat}
** Comment: andrus
   :PROPERTIES:
   :ID:       116498
   :created:  2013-10-29 03:18:40
   :END:
  This seems like more fallout from breaking accounting out of the reification transaction.
  
  However, I suspect it's not really new - I suspect that in R48 if a vm-run failed due to technical problems - like the VM just failed to start - the rate-limit would have been similarly impacted.
  
  That said, this should be fixed, but I'm not sure it's a blocker.
  
** Comment: bxiao
   :PROPERTIES:
   :ID:       117973
   :created:  2013-11-08 19:04:30
   :END:
  Working on unit test
* DONE Global rate limits missing				    :PL_7510:
  :PROPERTIES:
  :assignee: evanc
  :reporter: bschick
  :type:     Bug
  :priority: Blocker
  :status:   Closed
  :created:  2013-10-30 06:59:19
  :updated:  2013-10-31 22:37:41
  :ID:       PL-7510
  :resolution: Fixed
  :components: cm
  :END:
** description: PL-7510
  While updating bmetrics, and I found that we have no er regional global limits in the databse:
  
  {noformat}
  mysql> select * from limits where account_id = -1;
  Empty set (0.00 sec)
  {noformat}
  
  I see the following comment in a migration, but so far have not found any code that actually adds the regional global limits back. Seems to me we are currently running without global limits.
  
  {noformat}
  +                # Starting from R49 we will have per-region global account. The limit record for global
  +                # account in each region will be created (fresh) the first time the region is encountered
  +                # by CM's create datacenter. Due to the prolonged downtime for this migration we don't
  +                # don't care about migrating the existing limit record for global account.
  +                "DELETE from limits WHERE account_id = -1",
  {noformat}
** Comment: bxiao
   :PROPERTIES:
   :ID:       117021
   :created:  2013-10-31 20:01:15
   :END:
  Evan,
  resolve this to you as well. To verify the fix, after upgrading configuration_manager, make a create_datacenter call with a region, then check the limits table in the configuration database. You should see a record inserted for account_id -1 with the specified region. The timestamps should be current times
** Comment: bxiao
   :PROPERTIES:
   :ID:       117047
   :created:  2013-10-31 21:48:09
   :END:
  Hold on for a moment, just need to make another change. Sorry about it
** Comment: bxiao
   :PROPERTIES:
   :ID:       117053
   :created:  2013-10-31 22:09:55
   :END:
  New change submitted. Now you can start testing
** Comment: evanc
   :PROPERTIES:
   :ID:       117070
   :created:  2013-10-31 22:37:41
   :END:
  Run in Test:r49:hosting_platform:c9385ea8f7d1 -
  
  Created a new customer with all regions, and the global accounts got written to the limits table...
* TODO Rate Limiting: Disabling rate limiting is combinational between management / regional settings files :PL_7470:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: evanc
  :type:     Bug
  :priority: Critical
  :status:   Open
  :components: ratelimiting
  :created:  2013-10-29 02:49:31
  :updated:  2013-10-29 18:02:14
  :ID:       PL-7470
  :END:
** description: PL-7470
  Disabling rate limiting combines settings from the management region hosting_platform_conf.yaml and the regional settings files:
  
  {code}
           Management:      Regional:     Effective:
  disable: True             False         Enabled
  disable: True             True          Disabled
  disable: False            True          Enabled
  {code}
** Comment: andrus
   :PROPERTIES:
   :ID:       116563
   :created:  2013-10-29 18:02:14
   :END:
  Resource region setting should override mgmt region.
* TODO ImageDepotAllocation already attached to session		    :PL_7473:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: andrus
  :type:     Bug
  :priority: Critical
  :status:   Open
  :components: cm
  :created:  2013-10-29 16:20:03
  :updated:  2013-10-29 18:53:19
  :ID:       PL-7473
  :END:
** description: PL-7473
  From error watch:
  {code}
  ---------------------------------------------------------
  
  2013-10-29 15:00:42 tuk1m1cm2 [err] [trn.8f34b49022d801314d7d74ce4686f1c0.4/trn.af72e7e022d801314d7d74ce4686f1c0.5.1@/greenbox-tuk1m1gb4-6-0.30871.request.run_vms.73334/qm-run-5/configuration_manager-tuk1m1cm2-3.25927.run.3243.configuration_manager]: [ERROR] errors.py:209 - Failure in run inner logic detected by QM resource manager: {"incident_id": "16880E12", "name": "run", "timestamp": "2013-10-29 15:00:42", "_original_exception": "InvalidRequestError(\"Object '<ImageDepotAllocation at 0x52f4bd0>' is already attached to session '82544720' (this is '88344912')\",)", "__severity": "error", "context": "trn.8f34b49022d801314d7d74ce4686f1c0.4/trn.af72e7e022d801314d7d74ce4686f1c0.5.1@/greenbox-tuk1m1gb4-6-0.30871.request.run_vms.73334/qm-run-5/configuration_manager-tuk1m1cm2-3.25927.run.3243", "msg": "\"Object '<ImageDepotAllocation at 0x52f4bd0>' is already attached to session '82544720' (this is '88344912')\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/quota_manager.py", line 111, in quota_allocation_management
      yield
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/provision.py", line 165, in __init__
      self.logger.debug("RunOperation (exclusion): %s %s %s" % (self.vms, self.networks, self._operation))
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/model/operations.py", line 188, in __str__
      return "%s %s excluding %s" % (repr(self), self.operation_type, self.exclusions)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/model/operations.py", line 182, in exclusions
      tx.add(self)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/session.py", line 1161, in add
      self._save_or_update_state(state)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/session.py", line 1177, in _save_or_update_state
      self._save_or_update_impl(st_)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/session.py", line 1402, in _save_or_update_impl
      self._update_impl(state)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/session.py", line 1394, in _update_impl
      self._attach(state)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/session.py", line 1428, in _attach
      state.session_id, self.hash_key))
  ----------------------------------------------------------------------
  2013-10-29 15:00:42 tuk1m1cm2 [err] [trn.8f34b49022d801314d7d74ce4686f1c0.4/trn.af72e7e022d801314d7d74ce4686f1c0.5.1@/greenbox-tuk1m1gb4-6-0.30871.request.run_vms.73334/configuration_manager-tuk1m1cm2-3.25927.run.3243.api]: [ERROR] errors.py:209 - run EXCEPT 3.00 : {"incident_id": "3B5F12F8", "name": "run", "timestamp": "2013-10-29 15:00:42", "_original_exception": "InvalidRequestError(\"Object '<ImageDepotAllocation at 0x52f4bd0>' is already attached to session '82544720' (this is '88344912')\",)", "__severity": "error", "context": "trn.8f34b49022d801314d7d74ce4686f1c0.4/trn.af72e7e022d801314d7d74ce4686f1c0.5.1@/greenbox-tuk1m1gb4-6-0.30871.request.run_vms.73334/configuration_manager-tuk1m1cm2-3.25927.run.3243", "msg": "\"Object '<ImageDepotAllocation at 0x52f4bd0>' is already attached to session '82544720' (this is '88344912')\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 31, in common_error_handling
      yield
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 45, in run
      ret = func(cm_instance, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/mixin.py", line 29, in <lambda>
      method = wraps(operation)(lambda self, *args, **kwargs: executor(self, operation, *args, **kwargs))
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 77, in operation_executor
      op = operation(*args, logger=LOGGER, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/provision.py", line 165, in __init__
      self.logger.debug("RunOperation (exclusion): %s %s %s" % (self.vms, self.networks, self._operation))
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/model/operations.py", line 188, in __str__
      return "%s %s excluding %s" % (repr(self), self.operation_type, self.exclusions)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/model/operations.py", line 182, in exclusions
      tx.add(self)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/session.py", line 1161, in add
      self._save_or_update_state(state)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/session.py", line 1177, in _save_or_update_state
      self._save_or_update_impl(st_)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/session.py", line 1402, in _save_or_update_impl
      self._update_impl(state)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/session.py", line 1394, in _update_impl
      self._attach(state)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/session.py", line 1428, in _attach
      state.session_id, self.hash_key))
  ----------------------------------------------------------------------
  {code}
* TODO migration to add <secret> annotation to existing pre-shared-key values in CM db. :PL_7481:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: andrus
  :priority: Critical
  :status:   Open
  :components: cm
  :created:  2013-10-29 17:55:28
  :updated:  2013-10-29 17:55:28
  :ID:       PL-7481
  :END:
** description: PL-7481
* TODO error during tunnel provisioning				    :PL_2936:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: lonnieh
  :type:     Bug
  :priority: Critical
  :status:   Reopened
  :components: cm
  :created:  2011-08-05 14:03:18
  :updated:  2013-10-31 17:27:28
  :ID:       PL-2936
  :END:
** description: PL-2936
  {noformat}
  
  ----------------------------------------------------------------------
  2011-08-05 05:44:57 cm1.mgt.test.skytap.com [wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40.configuration_manager]: [ERROR] errors.py:144 - network-41020-54214: {"incident_id": "64F10C96", "name": "run", "timestamp": "2011-08-05 05:44:57", "_original_exception": "AttributeError(\"'NoneType' object has no attribute 'provision_endpoint'\",)", "__severity": "error", "context": "wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40", "msg": "\"'NoneType' object has no attribute 'provision_endpoint'\"", "type": "UnhandledException"}
  --traceback---
   File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/network_provisioning.py", line 168, in _execute
     self._provision_network()
   File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/network_provisioning.py", line 161, in _provision_network
     tunnel.consumer.provision_endpoint(network_server, self.network)
  ----------------------------------------------------------------------
  2011-08-05 05:44:57 cm1.mgt.test.skytap.com [wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40.configuration_manager]: [ERROR] threadutils.py:95 - Error executing asynchronous operation asynch-op <bound method NetworkProvisioningExecutor._execute of <NetworkProvisioningExecutor(Thread-107, started 139862682347264)>>(, {}) : 'NoneType' object has no attribute 'provision_endpoint' at Traceback (most recent call last):
   File "/highland/hosting_platform/hosting_platform/common/threadutils.py", line 87, in run
     self.return_value = self.operation(*self.args, **self.kwargs)
   File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 194, in wrap
     return func(*args, **kwargs)
   File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/network_provisioning.py", line 168, in _execute
     self._provision_network()
   File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/network_provisioning.py", line 161, in _provision_network
     tunnel.consumer.provision_endpoint(network_server, self.network)
  AttributeError: 'NoneType' object has no attribute 'provision_endpoint'
  ----------------------------------------------------------------------
  {noformat}
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60048
   :created:  2011-08-05 16:10:33
   :END:
  The tunnel wasn't registered during the run (tid=73930832 is the main run operation thread, tid=76582224 is a network executor thread which happens after the tunnel should be registered:
  {noformat}
  2011-08-05 05:44:40 cm1.mgt.test.skytap.com [wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40.call_node_trace]: [DEBUG] CallNode.py:88 - 73930832: calling lb1-ns.mgt.test.skytap.com:9002.router_create(('configuration-41020', None, None, None, None))
  2011-08-05 05:44:40 cm1.mgt.test.skytap.com [wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40.call_node_trace]: [DEBUG] CallNode.py:88 - 73930832: calling lb1-ns.mgt.test.skytap.com:9002.router_deploy((389640,))
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40.call_node_trace]: [DEBUG] CallNode.py:88 - 76582224: calling lb1-ns.mgt.test.skytap.com:9002.network_create((389640L, 557L, '13.99.2.0/24', '13.99.2.254', False, True))
  {noformat}
  
  The run operation exclusions:
  {noformat}
  
  networks: [        network-41020-54208,         network-41020-54210,         network-41020-54212,         network-41020-54214,         network-41020-54216,         network-41020-54222,         network-41020-54226,         network-41020-54230,         network-41020-54236,         network-41020-54240, 
          network-41020-54244, 
          network-41020-54248,         network-41020-54252,         network-41020-54256,         network-41020-54262, 
          network-41020-54264]tunnels: [        tunnel-54208-54308,         tunnel-54210-54312,         tunnel-54212-54316, 
          tunnel-54214-54320, 
          tunnel-54216-54324, 
          tunnel-54222-54330, 
          tunnel-54226-54336, 
          tunnel-54230-54342, 
          tunnel-54236-54346, 
          tunnel-54240-54352, 
          tunnel-54244-54360, 
          tunnel-54248-54366, 
          tunnel-54252-54370, 
          tunnel-54256-54380, 
          tunnel-54262-54382, 
          tunnel-54264-54388] 
  tunnel_consumers: [
          tunnel-54208-54308 (None, 364806), 
          tunnel-54212-54316 (None, 364898), 
          tunnel-54216-54324 (None, 365008), 
          tunnel-54226-54336 (None, 365104), 
          tunnel-54236-54346 (None, 365218), 
          tunnel-54244-54360 (None, 365332), 
          tunnel-54252-54370 (None, 365436), 
          tunnel-54264-54388 (None, 365600)]
  {noformat}
  
  So, all the networks and tunnels were excluded. The tunnel consumer exclusions indicate that some of the tunnels were provisioned on the right side (this configs networks are all left networks).
  
  The tunnel endpoints that were created:
  {noformat}
  tunnel_endpoint_create((389646, 'tunnel-54210-54312', 'left'))
  tunnel_endpoint_create((389654, 'tunnel-54264-54388', 'left'))
  tunnel_endpoint_create((389676, 'tunnel-54222-54330', 'left'))
  tunnel_endpoint_create((389688, 'tunnel-54216-54324', 'left'))
  tunnel_endpoint_create((389690, 'tunnel-54248-54366', 'left'))
  tunnel_endpoint_create((389712, 'tunnel-54236-54346', 'left'))
  tunnel_endpoint_create((389730, 'tunnel-54230-54342', 'left'))
  tunnel_endpoint_create((389788, 'tunnel-54262-54382', 'left'))
  tunnel_endpoint_create((389650, 'tunnel-54212-54316', 'left'))
  tunnel_endpoint_create((389816, 'tunnel-54208-54308', 'left'))
  tunnel_endpoint_create((389890, 'tunnel-54240-54352', 'left'))
  tunnel_endpoint_create((389900, 'tunnel-54256-54380', 'left'))
  tunnel_endpoint_create((389960, 'tunnel-54226-54336', 'left'))
  tunnel_endpoint_create((389976, 'tunnel-54252-54370', 'left'))
  tunnel_endpoint_create((389732, 'tunnel-54244-54360', 'left'))
  {noformat}
  
  so, by exclusion the tunnel that had the failure was tunnel-54214-54320.
  
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60049
   :created:  2011-08-05 16:11:59
   :END:
  {noformat}
  
  {'tunnel-54214-54320': [{'key': 'tunnel-54214-54320',
                           'left_network': {'cidr_block': '13.99.4.0/24',
                                            'configuration_key': 'configuration-41020',
                                            'domain_name': 'auto-test-4.net',
                                            'gateway': '13.99.4.254',
                                            'key': 'network-41020-54214',
                                            'network_type': 'automatic',
                                            'primary_nameserver': None,
                                            'secondary_nameserver': None,
                                            'tunnels': ['tunnel-54214-54320'],
                                            'vpn_attachments': []},
                           'right_network': {'cidr_block': '10.4.1.0/24',
                                             'configuration_key': 'configuration-41042',
                                             'domain_name': 'auto-test-1.net',
                                             'gateway': '10.4.1.254',
                                             'key': 'network-41042-54320',
                                             'network_type': 'automatic',
                                             'primary_nameserver': None,
                                             'secondary_nameserver': None,
                                             'tunnels': ['tunnel-54214-54320'],
                                             'vpn_attachments': []}},
                          0]}
  {noformat}
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60130
   :created:  2011-08-08 17:03:44
   :END:
  the right side of that tunnel was unprovisioned over an hour earlier:
  {noformat}
  2011-08-05 04:40:01 nsvc1.mgt.test.skytap.com [wfe.910347f0a184012e72fe005056a30032.20/trn.acd99650a173012edd37005056a30030.277.1/configuration_manager-cm1.mgt.test.skytap.com-0.22727.poweroff.92/ns.27262.tunnel_endpoint_destroy.39316.network_server]: [DEBUG] restrpcapi.py:111 - tunnel_endpoint_destroy [364950] [ctx:wfe.910347f0a184012e72fe005056a30032.20/trn.acd99650a173012edd37005056a30030.277.1/configuration_manager-cm1.mgt.test.skytap.com-0.22727.poweroff.92] returning None
  {noformat}
  
  This operation did not deregister the tunnel. The tunnel in question is:
  {noformat}
  
  2011-08-05 04:40:01 nsvc1.mgt.test.skytap.com [wfe.910347f0a184012e72fe005056a30032.20/trn.acd99650a173012edd37005056a30030.277.1/configuration_manager-cm1.mgt.test.skytap.com-0.22727.poweroff.92/ns.27262.tunnel_endpoint_destroy.39316.network_server]: [DEBUG] resourcemanager.py:49 - Network Resource 'gretunnel:364950:364946:[configuration-41042]' lock transition: unlocked -> locked:
  {noformat}
  
  The tunnel was deregistered many times (not sure why at this time), including right before the provisioning operation failed to use it:
  {noformat}
  2011-08-05 04:40:49 cm2.mgt.test.skytap.com [wfe.70f60450a185012e5d4b005056a3002c.8/trn.acd99650a173012edd37005056a30030.273.1/configuration_manager-cm2.mgt.test.skytap.com-1.20387.poweroff.46.call_node_trace]: [DEBUG] CallNode.py:88 - 74641168: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  2011-08-05 04:40:49 nsvc2.mgt.test.skytap.com [/11753.network_server]: [DEBUG] restrpcapi.py:81 - deregister_tunnel ('tunnel-54214-54320',) [ctx:wfe.70f60450a185012e5d4b005056a3002c.8/trn.acd99650a173012edd37005056a30030.273.1/configuration_manager-cm2.mgt.test.skytap.com-1.20387.poweroff.46]
  2011-08-05 04:40:49 nsvc2.mgt.test.skytap.com [wfe.70f60450a185012e5d4b005056a3002c.8/trn.acd99650a173012edd37005056a30030.273.1/configuration_manager-cm2.mgt.test.skytap.com-1.20387.poweroff.46/ns.11753.deregister_tunnel.39212.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.70f60450a185012e5d4b005056a3002c.8/trn.acd99650a173012edd37005056a30030.273.1/configuration_manager-cm2.mgt.test.skytap.com-1.20387.poweroff.46] returning None
  2011-08-05 04:48:04 cm2.mgt.test.skytap.com [wfe.0f5db1f0a181012e72fe005056a30032.131/trn.acd99650a173012edd37005056a30030.296.1/configuration_manager-cm2.mgt.test.skytap.com-0.23703.suspend.61.call_node_trace]: [DEBUG] CallNode.py:88 - 74014928: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  2011-08-05 04:48:04 nsvc2.mgt.test.skytap.com [/11738.network_server]: [DEBUG] restrpcapi.py:81 - deregister_tunnel ('tunnel-54214-54320',) [ctx:wfe.0f5db1f0a181012e72fe005056a30032.131/trn.acd99650a173012edd37005056a30030.296.1/configuration_manager-cm2.mgt.test.skytap.com-0.23703.suspend.61]
  2011-08-05 04:48:04 nsvc2.mgt.test.skytap.com [wfe.0f5db1f0a181012e72fe005056a30032.131/trn.acd99650a173012edd37005056a30030.296.1/configuration_manager-cm2.mgt.test.skytap.com-0.23703.suspend.61/ns.11738.deregister_tunnel.40566.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.0f5db1f0a181012e72fe005056a30032.131/trn.acd99650a173012edd37005056a30030.296.1/configuration_manager-cm2.mgt.test.skytap.com-0.23703.suspend.61] returning None
  2011-08-05 04:54:28 cm1.mgt.test.skytap.com [wfe.6d205ce0a16b012e5d4b005056a3002c.755/trn.acd99650a173012edd37005056a30030.305.1/configuration_manager-cm1.mgt.test.skytap.com-0.30018.poweroff.41.call_node_trace]: [DEBUG] CallNode.py:88 - 79227152: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  2011-08-05 04:54:28 nsvc2.mgt.test.skytap.com [/11738.network_server]: [DEBUG] restrpcapi.py:81 - deregister_tunnel ('tunnel-54214-54320',) [ctx:wfe.6d205ce0a16b012e5d4b005056a3002c.755/trn.acd99650a173012edd37005056a30030.305.1/configuration_manager-cm1.mgt.test.skytap.com-0.30018.poweroff.41]
  2011-08-05 04:54:28 nsvc2.mgt.test.skytap.com [wfe.6d205ce0a16b012e5d4b005056a3002c.755/trn.acd99650a173012edd37005056a30030.305.1/configuration_manager-cm1.mgt.test.skytap.com-0.30018.poweroff.41/ns.11738.deregister_tunnel.41110.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.6d205ce0a16b012e5d4b005056a3002c.755/trn.acd99650a173012edd37005056a30030.305.1/configuration_manager-cm1.mgt.test.skytap.com-0.30018.poweroff.41] returning None
  2011-08-05 05:00:21 cm1.mgt.test.skytap.com [wfe.0871f440a188012e72fe005056a30032.20/trn.acd99650a173012edd37005056a30030.312.1/configuration_manager-cm1.mgt.test.skytap.com-1.31665.suspend.435.call_node_trace]: [DEBUG] CallNode.py:88 - 75611024: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  2011-08-05 05:00:21 nsvc1.mgt.test.skytap.com [/27262.network_server]: [DEBUG] restrpcapi.py:81 - deregister_tunnel ('tunnel-54214-54320',) [ctx:wfe.0871f440a188012e72fe005056a30032.20/trn.acd99650a173012edd37005056a30030.312.1/configuration_manager-cm1.mgt.test.skytap.com-1.31665.suspend.435]
  2011-08-05 05:00:21 nsvc1.mgt.test.skytap.com [wfe.0871f440a188012e72fe005056a30032.20/trn.acd99650a173012edd37005056a30030.312.1/configuration_manager-cm1.mgt.test.skytap.com-1.31665.suspend.435/ns.27262.deregister_tunnel.40865.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.0871f440a188012e72fe005056a30032.20/trn.acd99650a173012edd37005056a30030.312.1/configuration_manager-cm1.mgt.test.skytap.com-1.31665.suspend.435] returning None
  2011-08-05 05:06:45 cm2.mgt.test.skytap.com [wfe.94666f30a188012e72fe005056a30032.43/trn.acd99650a173012edd37005056a30030.321.1/configuration_manager-cm2.mgt.test.skytap.com-0.1184.poweroff.70.call_node_trace]: [DEBUG] CallNode.py:88 - 77568464: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  2011-08-05 05:06:46 nsvc2.mgt.test.skytap.com [/11738.network_server]: [DEBUG] restrpcapi.py:81 - deregister_tunnel ('tunnel-54214-54320',) [ctx:wfe.94666f30a188012e72fe005056a30032.43/trn.acd99650a173012edd37005056a30030.321.1/configuration_manager-cm2.mgt.test.skytap.com-0.1184.poweroff.70]
  2011-08-05 05:06:46 nsvc2.mgt.test.skytap.com [wfe.94666f30a188012e72fe005056a30032.43/trn.acd99650a173012edd37005056a30030.321.1/configuration_manager-cm2.mgt.test.skytap.com-0.1184.poweroff.70/ns.11738.deregister_tunnel.42163.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.94666f30a188012e72fe005056a30032.43/trn.acd99650a173012edd37005056a30030.321.1/configuration_manager-cm2.mgt.test.skytap.com-0.1184.poweroff.70] returning None
  2011-08-05 05:12:38 cm1.mgt.test.skytap.com [wfe.467a1d90a185012e72fe005056a30032.132/trn.acd99650a173012edd37005056a30030.334.1/configuration_manager-cm1.mgt.test.skytap.com-1.8270.suspend.366.call_node_trace]: [DEBUG] CallNode.py:88 - 66847440: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  2011-08-05 05:12:38 nsvc2.mgt.test.skytap.com [/11753.network_server]: [DEBUG] restrpcapi.py:81 - deregister_tunnel ('tunnel-54214-54320',) [ctx:wfe.467a1d90a185012e72fe005056a30032.132/trn.acd99650a173012edd37005056a30030.334.1/configuration_manager-cm1.mgt.test.skytap.com-1.8270.suspend.366]
  2011-08-05 05:12:38 nsvc2.mgt.test.skytap.com [wfe.467a1d90a185012e72fe005056a30032.132/trn.acd99650a173012edd37005056a30030.334.1/configuration_manager-cm1.mgt.test.skytap.com-1.8270.suspend.366/ns.11753.deregister_tunnel.41630.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.467a1d90a185012e72fe005056a30032.132/trn.acd99650a173012edd37005056a30030.334.1/configuration_manager-cm1.mgt.test.skytap.com-1.8270.suspend.366] returning None
  2011-08-05 05:17:55 cm1.mgt.test.skytap.com [wfe.910347f0a184012e72fe005056a30032.110/trn.acd99650a173012edd37005056a30030.341.1/configuration_manager-cm1.mgt.test.skytap.com-1.13409.poweroff.43.call_node_trace]: [DEBUG] CallNode.py:88 - 77049360: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  2011-08-05 05:17:55 nsvc2.mgt.test.skytap.com [/11738.network_server]: [DEBUG] restrpcapi.py:81 - deregister_tunnel ('tunnel-54214-54320',) [ctx:wfe.910347f0a184012e72fe005056a30032.110/trn.acd99650a173012edd37005056a30030.341.1/configuration_manager-cm1.mgt.test.skytap.com-1.13409.poweroff.43]
  2011-08-05 05:17:55 nsvc2.mgt.test.skytap.com [wfe.910347f0a184012e72fe005056a30032.110/trn.acd99650a173012edd37005056a30030.341.1/configuration_manager-cm1.mgt.test.skytap.com-1.13409.poweroff.43/ns.11738.deregister_tunnel.43080.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.910347f0a184012e72fe005056a30032.110/trn.acd99650a173012edd37005056a30030.341.1/configuration_manager-cm1.mgt.test.skytap.com-1.13409.poweroff.43] returning None
  2011-08-05 05:25:01 cm1.mgt.test.skytap.com [wfe.8a508370a182012e5d4b005056a3002c.216/trn.acd99650a173012edd37005056a30030.354.1/configuration_manager-cm1.mgt.test.skytap.com-0.14888.suspend.675.call_node_trace]: [DEBUG] CallNode.py:88 - 85821968: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  2011-08-05 05:25:01 nsvc2.mgt.test.skytap.com [/11753.network_server]: [DEBUG] restrpcapi.py:81 - deregister_tunnel ('tunnel-54214-54320',) [ctx:wfe.8a508370a182012e5d4b005056a3002c.216/trn.acd99650a173012edd37005056a30030.354.1/configuration_manager-cm1.mgt.test.skytap.com-0.14888.suspend.675]
  2011-08-05 05:25:01 nsvc2.mgt.test.skytap.com [wfe.8a508370a182012e5d4b005056a3002c.216/trn.acd99650a173012edd37005056a30030.354.1/configuration_manager-cm1.mgt.test.skytap.com-0.14888.suspend.675/ns.11753.deregister_tunnel.42754.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.8a508370a182012e5d4b005056a3002c.216/trn.acd99650a173012edd37005056a30030.354.1/configuration_manager-cm1.mgt.test.skytap.com-0.14888.suspend.675] returning None
  2011-08-05 05:31:08 cm1.mgt.test.skytap.com [wfe.846e9dc0a166012e5d4b005056a3002c.1044/trn.acd99650a173012edd37005056a30030.363.1/configuration_manager-cm1.mgt.test.skytap.com-0.19170.poweroff.525.call_node_trace]: [DEBUG] CallNode.py:88 - 75586576: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  2011-08-05 05:31:08 nsvc1.mgt.test.skytap.com [/27262.network_server]: [DEBUG] restrpcapi.py:81 - deregister_tunnel ('tunnel-54214-54320',) [ctx:wfe.846e9dc0a166012e5d4b005056a3002c.1044/trn.acd99650a173012edd37005056a30030.363.1/configuration_manager-cm1.mgt.test.skytap.com-0.19170.poweroff.525]
  2011-08-05 05:31:08 nsvc1.mgt.test.skytap.com [wfe.846e9dc0a166012e5d4b005056a3002c.1044/trn.acd99650a173012edd37005056a30030.363.1/configuration_manager-cm1.mgt.test.skytap.com-0.19170.poweroff.525/ns.27262.deregister_tunnel.43315.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.846e9dc0a166012e5d4b005056a3002c.1044/trn.acd99650a173012edd37005056a30030.363.1/configuration_manager-cm1.mgt.test.skytap.com-0.19170.poweroff.525] returning None
  2011-08-05 05:37:00 cm1.mgt.test.skytap.com [wfe.0f5db1f0a181012e72fe005056a30032.356/trn.acd99650a173012edd37005056a30030.370.1/configuration_manager-cm1.mgt.test.skytap.com-0.25366.suspend.35.call_node_trace]: [DEBUG] CallNode.py:88 - 69104592: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  2011-08-05 05:37:00 nsvc1.mgt.test.skytap.com [/27277.network_server]: [DEBUG] restrpcapi.py:81 - deregister_tunnel ('tunnel-54214-54320',) [ctx:wfe.0f5db1f0a181012e72fe005056a30032.356/trn.acd99650a173012edd37005056a30030.370.1/configuration_manager-cm1.mgt.test.skytap.com-0.25366.suspend.35]
  2011-08-05 05:37:00 nsvc1.mgt.test.skytap.com [wfe.0f5db1f0a181012e72fe005056a30032.356/trn.acd99650a173012edd37005056a30030.370.1/configuration_manager-cm1.mgt.test.skytap.com-0.25366.suspend.35/ns.27277.deregister_tunnel.43504.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.0f5db1f0a181012e72fe005056a30032.356/trn.acd99650a173012edd37005056a30030.370.1/configuration_manager-cm1.mgt.test.skytap.com-0.25366.suspend.35] returning None
  2011-08-05 05:43:39 cm1.mgt.test.skytap.com [wfe.bb356750a173012e5d4b005056a3002c.861/trn.acd99650a173012edd37005056a30030.376.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.40.call_node_trace]: [DEBUG] CallNode.py:88 - 73611984: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  2011-08-05 05:43:39 nsvc1.mgt.test.skytap.com [/27277.network_server]: [DEBUG] restrpcapi.py:81 - deregister_tunnel ('tunnel-54214-54320',) [ctx:wfe.bb356750a173012e5d4b005056a3002c.861/trn.acd99650a173012edd37005056a30030.376.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.40]
  2011-08-05 05:43:39 nsvc1.mgt.test.skytap.com [wfe.bb356750a173012e5d4b005056a3002c.861/trn.acd99650a173012edd37005056a30030.376.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.40/ns.27277.deregister_tunnel.44096.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.bb356750a173012e5d4b005056a3002c.861/trn.acd99650a173012edd37005056a30030.376.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.40] returning None
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130.call_node_trace]: [DEBUG] CallNode.py:88 - 73391248: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  2011-08-05 05:44:41 nsvc1.mgt.test.skytap.com [/27277.network_server]: [DEBUG] restrpcapi.py:81 - deregister_tunnel ('tunnel-54214-54320',) [ctx:wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130]
  2011-08-05 05:44:41 nsvc1.mgt.test.skytap.com [wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130/ns.27277.deregister_tunnel.44152.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130] returning None
  {noformat}
  

** Comment: lonnieh
   :PROPERTIES:
   :ID:       60132
   :created:  2011-08-08 17:10:37
   :END:
  the left side of the tunnel was being provisioned up and down:
  {noformat}
  2011-08-05 05:42:00 cm2.mgt.test.skytap.com [wfe.537480e0a170012e72fe005056a30032.839/trn.acd99650a173012edd37005056a30030.377.1/configuration_manager-cm2.mgt.test.skytap.com-1.22203.poweroff.43.api]: [DEBUG] ConfigurationManager.py:50 - poweroff CALLED (['vm-41020-70110', 'vm-41020-70116', 'vm-41020-70120', 'vm-41020-70128'],)
  2011-08-05 05:44:37 cm1.mgt.test.skytap.com [wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40.api]: [DEBUG] ConfigurationManager.py:50 - run CALLED (['account-19430', 'account-10588'], ['vm-41020-70110', 'vm-41020-70116', 'vm-41020-70120', 'vm-41020-70128']){noformat}
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60133
   :created:  2011-08-08 17:14:40
   :END:
  The operation that deregistered the missing tunnel:
  {noformat}
  
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130.call_node_trace]: [DEBUG] CallNode.py:88 - 73391248: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  {noformat}
  
  Was for a completely different configuration (tunnel is configuration-41020-configuration-41042):
  {noformat} 
  
  2011-08-05 05:42:36 cm1.mgt.test.skytap.com [wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130.api]: [DEBUG] ConfigurationManager.py:50 - poweroff CALLED (['vm-41024-70114', 'vm-41024-70126', 'vm-41024-70134', 'vm-4
  {noformat}
  
  But the failing run operation had registered it moments before:
  {noformat}
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40.call_node_trace]: [DEBUG] CallNode.py:88 - 73930832: calling lb1-ns.mgt.test.skytap.com:9002.register_tunnel(('tunnel-54214-543
  20',)){noformat}
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60134
   :created:  2011-08-08 17:17:28
   :END:
  This looks suspiciously like PL-2926 since the inheritance of the exclusion is done to prevent just this scenario from happening.
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60136
   :created:  2011-08-08 17:27:46
   :END:
  PL-2926 - the fix for this bug was pushed on 8/4. There were no code updates done on test that day, so the fix was not running when this error happened the morning of 8/5:
  {noformat}
  2011-08-04 10:43:06 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - executing `invoke'
  2011-08-04 10:43:06 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - executing "uptime"
  2011-08-04 10:43:06 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - servers: ["lb1-ns", "nsvc1", "lb2-vnc", "mysql2-pl", "wfe1", "wfe2", "lb1-vnc", "trun1", "mon1", "c8b15", "metric1", "qi1", "lb2-ns", "nsvc2", "munger", "lb1-wfe", "mysql1-pl", "cm1", "lb2-wfe", "lb2-cm", "java1", "lb1-cm", "vnc1", "cm2", "vnc2", "ftp", "vsn103.mgt.test.skytap.com", "sn01.mgt.test.skytap.com", "vsn108.mgt.test.skytap.com", "vsn107.mgt.test.skytap.com", "vsn101.mgt.test.skytap.com", "vsn104.mgt.test.skytap.com"]
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - metric1 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - c8b15 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - sn01.mgt.test.skytap.com :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - vsn103.mgt.test.skytap.com :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - vsn108.mgt.test.skytap.com :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - vsn107.mgt.test.skytap.com :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - vsn101.mgt.test.skytap.com :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - vsn104.mgt.test.skytap.com :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - c8b15 ::  10:43:07 up 38 days,  2:45,  8 users,  load average: 0.51, 0.36, 0.28
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - metric1 ::  10:43:07 up 2 days,  2:03,  1 user,  load average: 2.48, 3.37, 3.70
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - sn01 ::  10:43am  up 50 day(s), 10:10,  2 users,  load average: 0.06, 0.04, 0.05
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - trun1 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - mon1 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - nsvc1 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - trun1 ::  10:43:07 up 72 days,  1:00,  1 user,  load average: 0.40, 0.32, 0.24
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - lb1-ns :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - nsvc1 ::  10:43:07 up 69 days, 52 min,  1 user,  load average: 0.32, 0.38, 0.37
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - mon1 ::  10:43:07 up 72 days,  1:16,  1 user,  load average: 0.00, 0.00, 0.00
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - nsvc2 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - munger :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - lb2-vnc :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - lb2-ns :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - mysql1-pl :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - wfe1 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - nsvc2 ::  10:43:07 up 72 days,  1:00,  1 user,  load average: 0.01, 0.07, 0.13
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - munger ::  10:43:07 up 72 days,  1:03,  1 user,  load average: 0.00, 0.00, 0.00
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - lb1 ::  10:43:07 up 72 days,  1:16,  1 user,  load average: 0.17, 0.14, 0.05
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - lb2 ::  10:43:07 up 72 days,  1:16,  1 user,  load average: 0.00, 0.08, 0.09
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - lb2 ::  10:43:07 up 72 days,  1:01,  1 user,  load average: 0.00, 0.02, 0.03
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - mysql1 ::  10:43:07 up 72 days,  1:08,  1 user,  load average: 0.36, 0.27, 0.20
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - lb2-wfe :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - mysql2-pl :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - lb1-vnc :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - wfe1 ::  10:43:07 up 36 days, 22:41,  1 user,  load average: 0.57, 0.69, 0.77
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - lb2 ::  10:43:07 up 72 days,  1:01,  1 user,  load average: 0.33, 0.15, 0.10
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - lb2-cm :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - lb1 ::  10:43:07 up 72 days,  1:16,  1 user,  load average: 0.08, 0.10, 0.08
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - vnc1 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - vnc1 ::  10:43:07 up 72 days,  1:16,  1 user,  load average: 0.12, 0.05, 0.08
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - lb2 ::  10:43:07 up 72 days,  1:02,  1 user,  load average: 0.14, 0.05, 0.04
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - mysql2 ::  10:43:07 up 49 days, 19:20,  2 users,  load average: 0.15, 0.18, 0.18
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - lb1-wfe :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - vnc2 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - ftp :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - vnc2 ::  10:43:07 up 72 days,  1:16,  1 user,  load average: 0.00, 0.00, 0.00
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - java1 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - lb1 ::  10:43:07 up 72 days,  1:16,  1 user,  load average: 0.06, 0.05, 0.05
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - java1 ::  10:43:07 up 72 days,  1:16,  1 user,  load average: 0.95, 1.02, 0.92
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - qi1 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - cm2 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - wfe2 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - ftp ::  10:43:07 up 72 days,  1:16,  1 user,  load average: 0.00, 0.00, 0.00
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - cm2 ::  10:43:07 up 72 days,  1:16,  1 user,  load average: 0.81, 0.81, 0.71
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - wfe2 ::  10:43:07 up 72 days,  1:16,  1 user,  load average: 0.77, 0.66, 0.67
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - qi1 ::  10:43:07 up 72 days,  1:16,  1 user,  load average: 0.10, 0.10, 0.08
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - lb1-cm :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - lb1 ::  10:43:07 up 72 days,  1:16,  1 user,  load average: 0.03, 0.10, 0.04
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - cm1 :: executing command
  2011-08-04 10:43:07 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - cm1 ::  10:43:07 up 72 days,  1:16,  2 users,  load average: 0.71, 0.44, 0.37
  2011-08-04 10:43:10 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - vsn108 ::  10:43am  up 75 day(s), 20:32,  1 user,  load average: 0.04, 0.01, 0.00
  2011-08-04 10:43:11 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - vsn104 ::  10:43am  up 75 day(s), 21:06,  1 user,  load average: 0.04, 0.01, 0.00
  2011-08-04 10:43:11 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - vsn107 ::  10:43am  up 75 day(s), 20:52,  1 user,  load average: 0.04, 0.02, 0.01
  2011-08-04 10:43:12 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - vsn101 ::  10:43am  up 75 day(s), 20:57,  1 user,  load average: 0.04, 0.02, 0.02
  2011-08-04 10:43:12 cm1.mgt.test.skytap.com skycap(highland): [30211]: [INFO] skycap: - vsn103 ::  10:43am  up 75 day(s), 20:32,  1 user,  load average: 0.05, 0.05, 0.07
  2011-08-04 10:43:12 cm1.mgt.test.skytap.com skycap(highland): [30211]: [DEBUG] skycap: - command finished
  {noformat}
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60140
   :created:  2011-08-08 17:36:57
   :END:
  remembering that test logging isn't always reliable, I checked for the actual revision logged by the CM on startup:
  {noformat}
  
  2011-08-04 18:06:13 cm1.mgt.test.skytap.com [/27582.configuration_manager]: [INFO] ConfigurationManagerService.py:104 - SERVICE IS ONLINE xmlrpc/json port: 30000 workers: 2 rev: 6283:79967030956f
  2011-08-04 18:06:32 cm2.mgt.test.skytap.com [/1524.configuration_manager]: [INFO] ConfigurationManagerService.py:104 - SERVICE IS ONLINE xmlrpc/json port: 30000 workers: 2 rev: 6283:79967030956f
  {noformat}
  
  This rev *is* the revision that "fixed" PL-2926, so either it wasn't fixed, or this is a different issue.
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60145
   :created:  2011-08-08 18:53:19
   :END:
  Registered by failing op:
  {noformat}
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40.call_node_trace]: [DEBUG] CallNode.py:88 - 73930832: calling lb1-ns.mgt.test.skytap.com:9002.register_tunnel(('tunnel-54214-54320',))
  2011-08-05 05:44:41 nsvc1.mgt.test.skytap.com [/27277.network_server]: [DEBUG] restrpcapi.py:81 - register_tunnel ('tunnel-54214-54320',) [ctx:wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40]
  2011-08-05 05:44:41 nsvc1.mgt.test.skytap.com [wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40/ns.27277.register_tunnel.44150.network_server]: [DEBUG] restrpcapi.py:111 - register_tunnel ['tunnel-54214-54320'] [ctx:wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40] returning 54
  {noformat}
  
  and almost immediately removed:
  {noformat}
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130.call_node_trace]: [DEBUG] CallNode.py:88 - 73391248: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-54214-54320',))
  2011-08-05 05:44:41 nsvc1.mgt.test.skytap.com [/27277.network_server]: [DEBUG] restrpcapi.py:81 - deregister_tunnel ('tunnel-54214-54320',) [ctx:wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130]
  2011-08-05 05:44:41 nsvc1.mgt.test.skytap.com [wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130/ns.27277.deregister_tunnel.44152.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130] returning None
  {noformat}
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60146
   :created:  2011-08-08 19:12:31
   :END:
  This would have remove the consumer for the tunnel, the only way the consumer existed for the UnprovisionTunnelsOperation to see it and unprovision it was after it had been created by the run.
  {noformat}
  2011-08-05 05:43:39 nsvc1.mgt.test.skytap.com [wfe.bb356750a173012e5d4b005056a3002c.861/trn.acd99650a173012edd37005056a30030.376.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.40/ns.27277.deregister_tunnel.44096.network_server]: [DEBUG] restrpcapi.py:111 - deregister_tunnel ['tunnel-54214-54320'] [ctx:wfe.bb356750a173012e5d4b005056a3002c.861/trn.acd99650a173012edd37005056a30030.376.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.40] returning None
  {noformat}
  
  This implies:
  # RunOperation should have had the tunnel excluded and the consumer should not have existed
  # after provisioning the tunnel the consumer was created an inserted into the db...it should have been excluded
  # UnprovisionTunnelsOperation unprovisioned the tunnel when the run operation should have had it excluded (need to figure out if run didn't have exclusion, UTO didn't honor exclusion, or the exclusion was trampled by some other operation)
  
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60161
   :created:  2011-08-08 21:10:02
   :END:
  UnprovisionNetworks started: 05:43:06 (it had 16 networks to unprovision, tunnel_endpoint_undeploy took 14 seconds, 20+ for fileserver_undeploy)
  
  run operation reification started: 05:44:37
                              ended: 05:44:40
       holds exclusion on the tunnel, not consumer (which presumably doesn't exist yet)
  
  05:44:41 - UnprovisionTunnelsOperation called from UnprovisionNetworkExecutor that unprovisioned it
  
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60162
   :created:  2011-08-08 21:21:22
   :END:
  It appears that right after the tunnel was registered the poweroff operation ran UnprovisionTunnelsOperation which picked up the tunnel and unprovisioned it.
  
  poweroff finishes unprovisioning it's last network:
  {noformat}
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130.call_node_trace]: [DEBUG] CallNode.py:88 - 73391248: calling lb1-ns.mgt.test.skytap.com:9002.network_destroy((389150L,))2011-08-05 05:44:41 nsvc1.mgt.test.skytap.com [/27277.network_server]: [DEBUG] restrpcapi.py:81 - network_destroy (389150,) [ctx:wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130]
  ...
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130.call_node_trace]: [DEBUG] CallNode.py:142 - 73391248: returning None
  {noformat}
  
  and the next thing it does is deregister the tunnel:
  {noformat}
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130.call_node_trace]: [DEBUG] CallNode.py:88 - 73391248: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(('tunnel-
  54214-54320',))
  {noformat}
  
  This was the only tunnel deregistered by the UnprovisionTunnelsOperation.
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60171
   :created:  2011-08-08 21:47:43
   :END:
  The run operation *thinks* it has exclusion since it didn't fail on this assertion right before provisioning the tunnel:
  {code}
  
                      assert tunnel.operation is self._operation, "tunnel is excluded by another operation during provisioning"
  {code}
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60199
   :created:  2011-08-09 15:32:38
   :END:
  it doesn't look like the right side of the tunnel was being provisioned around this time (helps rule out other operation incorrectly releasing exclusion on tunnel):
  {noformat}
  lonnieh@c8b15:/highland/logs/syslog/2011/08$ egrep "(run|poweroff) CALLED.*-41042" 2011-08-05-all.log
  2011-08-05 02:36:17 cm1.mgt.test.skytap.com [wfe.b6394dd0a16f012e5d4b005056a3002c.118/trn.acd99650a173012edd37005056a30030.12.1/configuration_manager-cm1.mgt.test.skytap.com-0.18670.run.33.api]: [DEBUG] ConfigurationManager.py:50 - run CALLED (['account-19430', 'account-10588'], ['vm-41042-70160'])
  2011-08-05 04:39:49 cm1.mgt.test.skytap.com [wfe.910347f0a184012e72fe005056a30032.20/trn.acd99650a173012edd37005056a30030.277.1/configuration_manager-cm1.mgt.test.skytap.com-0.22727.poweroff.92.api]: [DEBUG] ConfigurationManager.py:50 - poweroff CALLED (['vm-41042-70160'],)
  lonnieh@c8b15:/highland/logs/syslog/2011/08${noformat}
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60209
   :created:  2011-08-09 17:46:38
   :updated:  2011-08-09 17:48:38
   :END:
  Not much happened between the register_tunnel and deregister tunnel calls (wfe logs filtered out):
  {noformat}
  2011-08-05 05:44:41 nsvc1.mgt.test.skytap.com [wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40/ns.27277.register_tunnel.44150.network_server]: [DEBUG] restrpcapi.py:111 - register_tunnel ['tunnel-54214-54320'] [
  ctx:wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40] returning 54
  ----------------------------------------------------------------------
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40.call_node_trace]: [DEBUG] CallNode.py:142 - 73930832: returning 54
  ----------------------------------------------------------------------
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130.call_node_trace]: [DEBUG] CallNode.py:142 - 73391248: returning None
  ----------------------------------------------------------------------
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40.call_node_trace]: [DEBUG] CallNode.py:88 - 73930832: calling lb1-ns.mgt.test.skytap.com:9002.register_tunnel(('tunnel-
  54240-54352',))
  ----------------------------------------------------------------------
  2011-08-05 05:44:41 nsvc1.mgt.test.skytap.com [/27277.network_server]: [DEBUG] restrpcapi.py:81 - register_tunnel ('tunnel-54240-54352',) [ctx:wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40]
  ----------------------------------------------------------------------
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.bd2c7f20a172012e5d4b005056a3002c.620/trn.acd99650a173012edd37005056a30030.378.1/configuration_manager-cm1.mgt.test.skytap.com-0.28283.poweroff.130.call_node_trace]: [DEBUG] CallNode.py:88 - 73391248: calling lb1-ns.mgt.test.skytap.com:9002.deregister_tunnel(
  ('tunnel-54214-54320',))
  ...
  2011-08-05 05:44:41 cm1.mgt.test.skytap.com [wfe.c9b3a2e0a18a012e5d4b005056a3002c.88/trn.acd99650a173012edd37005056a30030.380.1/configuration_manager-cm1.mgt.test.skytap.com-1.29647.run.40.call_node_trace]: [DEBUG] CallNode.py:88 - 73930832: calling lb1-ns.mgt.test.skytap.com:9002.register_tunnel(('tunnel-54256-54380',))
  {noformat}
  
  So, the deregister happens while the run operation is still provisioning the networks since it registers other tunnels after that. Specifically, the run operation was in this loop when the consumer was created and destroyed:
  {code}
          for network in self.networks:
              try:
                  for tunnel in network.tunnels:
                      assert tunnel.operation is self._operation, "tunnel is excluded by another operation during provisioning"
                      tunnel.provision_tunnel(network_server)
              except:
                  Error.from_exc().log(self.logger, "error provisioning tunnel")
                  failed_networks.append(network){code}
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60246
   :created:  2011-08-09 22:48:01
   :END:
  discussed strategy to add logging and assertions in R24.X and wait for repro with Yelena, she's fine with that :( (I'd prefer an actual resolution).
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60364
   :created:  2011-08-11 15:25:10
   :END:
  This bug is not fixed, but it isn't actionable at this point. The code changes will provide more information if we ever see this again. I'm resolving since it is *so* similar to PL-2926 which was fixed/deployed the day before. Reopen when we see it again.
** Comment: ybranch
   :PROPERTIES:
   :ID:       60470
   :created:  2011-08-15 23:53:08
   :END:
  Oh goody, got the new logging:
  
  {code}
  2011-08-15 16:30:40 cm1.mgt.test.skytap.com [wfe.a97c3170a9bd012e7319005056a30032.303.2/configuration_manager-cm1.mgt.test.skytap.com-1.16255.delete_tunnel.304.assertion]: [CRITICAL] tunnels.py:199 - provisioning requires operation exclusion
  {code}
** Comment: ybranch
   :PROPERTIES:
   :ID:       60471
   :created:  2011-08-15 23:54:20
   :END:
  Since the bug is not really fixed, reopening it. The logging portion is in (see comment above)
** Comment: lonnieh
   :PROPERTIES:
   :ID:       60495
   :created:  2011-08-16 16:58:22
   :END:
  both delete_tunnel and create_tunnel have this problem:
  {noformat}
  2011-08-16 04:14:02 cm2.mgt.test.skytap.com [wfe.880fd480aa26012e731f005056a30032.31.2/configuration_manager-cm2.mgt.test.skytap.com-0.20891.delete_tunnel.448.assertion]: [CRITICAL] tunnels.py:199 - provisioning requires operation exclusion
  ----------------------------------------------------------------------
  2011-08-16 04:15:41 cm1.mgt.test.skytap.com [wfe.d8a47dc0aa04012e731f005056a30032.875.3/configuration_manager-cm1.mgt.test.skytap.com-1.16736.create_tunnel.41.assertion]: [CRITICAL] tunnels.py:199 - provisioning requires operation exclusion
  ----------------------------------------------------------------------
  2011-08-16 04:15:59 cm1.mgt.test.skytap.com [wfe.a65c72d0aa20012e5d77005056a3002c.227.2/configuration_manager-cm1.mgt.test.skytap.com-0.16477.delete_tunnel.163.assertion]: [CRITICAL] tunnels.py:199 - provisioning requires operation exclusion{noformat}
** Comment: evanc
   :PROPERTIES:
   :ID:       61587
   :created:  2011-09-02 21:00:16
   :END:
  Tunnel specific stress has not shown this issue in Test for awhile...
** Comment: evanc
   :PROPERTIES:
   :ID:       62942
   :created:  2011-09-29 22:57:58
   :END:
  Since we have not seen this in all of R26 I am closing.
** Comment: lonnieh
   :PROPERTIES:
   :ID:       63660
   :created:  2011-10-12 13:23:22
   :END:
  I had a suspicion, but couldn't prove it....
  
  {noformat}
  configuration-57006
  ---------------------------------------------------------
  
  2011-10-12 02:03:21 cm2.mgt.test.skytap.com [wfe.605b73d0d6dd012e7413005056a30032.67/trn.c074f740d6dd012edfc2005056a30030.118.1/configuration_manager-cm2.mgt.test.skytap.com-0.21990.suspend.52.configuration_manager]: [ERROR] threadutils.py:95 - Error executing asynchronous operation asynch-op <bound method UnprovisionNetworkExecutor._execute of <UnprovisionNetworkExecutor(Thread-617, started 139807426541312)>>(, {}) : this is an instance of PL-2936, the query returned a consumer with exclusion at Traceback (most recent call last):
   File "/highland/hosting_platform/hosting_platform/common/threadutils.py", line 87, in run
     self.return_value = self.operation(*self.args, **self.kwargs)
   File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 193, in wrap
     return func(*args, **kwargs)
   File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 366, in _execute
     UnprovisionTunnelsOperation(logger=self.logger).execute()
   File "/highland/hosting_platform/hosting_platform/common/validation.py", line 135, in _validate_params
     return func(self, *args, **kwargs)
   File "/highland/hosting_platform/hosting_platform/common/validation.py", line 135, in _validate_params
     return func(self, *args, **kwargs)
   File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/tunnels.py", line 171, in __init__
     assert consumer.operation is None, "this is an instance of PL-2936, the query returned a consumer with exclusion"
  AssertionError: this is an instance of PL-2936, the query returned a consumer with exclusion
  ----------------------------------------------------------------------
  2011-10-12 02:03:33 cm2.mgt.test.skytap.com [wfe.605b73d0d6dd012e7413005056a30032.67/trn.c074f740d6dd012edfc2005056a30030.118.1/configuration_manager-cm2.mgt.test.skytap.com-0.21990.suspend.52.configuration_manager]: [ERROR] errors.py:144 - failure unprovisioning network(s) for configuration-57006: {"incident_id": "159CCC93", "name": "suspend", "context": "wfe.605b73d0d6dd012e7413005056a30032.67/trn.c074f740d6dd012edfc2005056a30030.118.1/configuration_manager-cm2.mgt.test.skytap.com-0.21990.suspend.52", "__severity": "error", "msg": "'failed to unprovision networks network-57006-70978'", "timestamp": "2011-10-12 02:03:33", "type": "InternalError"}
  ----------------------------------------------------------------------
  2011-10-12 02:03:33 cm2.mgt.test.skytap.com [wfe.605b73d0d6dd012e7413005056a30032.67/trn.c074f740d6dd012edfc2005056a30030.118.1/configuration_manager-cm2.mgt.test.skytap.com-0.21990.suspend.52.configuration_manager]: [ERROR] threadutils.py:95 - Error executing asynchronous operation asynch-op <function execute at 0x3e05050>(, {}) : InternalError: ('"failure unprovisioning networks: \'failed to unprovision networks network-57006-70978\',failed to unprovision networks network-57006-70978"', "failure unprovisioning networks: 'failed to unprovision networks network-57006-70978',failed to unprovision networks network-57006-70978") at Traceback (most recent call last):
   File "/highland/hosting_platform/hosting_platform/common/threadutils.py", line 87, in run
     self.return_value = self.operation(*self.args, **self.kwargs)
   File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 193, in wrap
     return func(*args, **kwargs)
   File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 69, in wrap
     return func(*args, **kwargs)
   File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 74, in execute
     self.provision()
   File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 526, in provision
     logger=self.logger, chained=self.chained, cleanup=self.cleanup).execute()
   File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 74, in execute
     self.provision()
   File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 245, in provision
     raise InternalError("failure unprovisioning networks: " + ",".join(map(str,e.args)))
  InternalError: InternalError: ('"failure unprovisioning networks: \'failed to unprovision networks network-57006-70978\',failed to unprovision networks network-57006-70978"', "failure unprovisioning networks: 'failed to unprovision networks network-57006-70978',failed to unprovision networks network-57006-70978")
  ----------------------------------------------------------------------{noformat}
** Comment: lonnieh
   :PROPERTIES:
   :ID:       74139
   :created:  2012-04-10 14:44:27
   :updated:  2012-04-10 14:46:37
   :END:
  Locks by the query before "assert tunnel_consumer.operation is None":
  
  {noformat}
  ---TRANSACTION 0 15964964, ACTIVE 2 sec, process no 14450, OS thread id 140539276089088
  4 lock struct(s), heap size 1216, 1 row lock(s)
  MySQL thread id 4016, query id 747368 dev1.lh.internal.illumita.com 172.16.245.100 root
  TABLE LOCK table `ut_configuration_ut_configuration_pydev`.`tunnel_consumers` trx id 0 15964964 lock mode IX
  RECORD LOCKS space id 0 page no 1603 n bits 72 index `PRIMARY` of table `ut_configuration_ut_configuration_pydev`.`tunnel_consumers` trx id 0 15964964 lock_mode X locks rec but not gap
  Record lock, heap no 3 PHYSICAL RECORD: n_fields 7; compact format; info bits 0
   0: len 10; hex 74756e6e656c2d312d32; asc tunnel-1-2;; 1: len 6; hex 000000f39b20; asc       ;; 2: len 7; hex 000000029d0a92; asc        ;; 3: len 4; hex 80000001; asc     ;; 4: len 4; hex 80000002; asc     ;; 5: SQL NULL; 6: SQL NULL;
  
  TABLE LOCK table `ut_configuration_ut_configuration_pydev`.`tunnel_consumer_operation_exclusion` trx id 0 15964964 lock mode IX
  RECORD LOCKS space id 0 page no 1601 n bits 72 index `PRIMARY` of table `ut_configuration_ut_configuration_pydev`.`tunnel_consumer_operation_exclusion` trx id 0 15964964 lock_mode X locks rec but not gap
  
  {noformat}
  

** Comment: lonnieh
   :PROPERTIES:
   :ID:       74140
   :created:  2012-04-10 15:14:44
   :updated:  2012-04-10 15:15:36
   :END:
  Test was updated to R27 on 10/17/11:
  {noformat}
  | 344 | 2011-10-17 17:37:31 | R27/Baseline                                                 | expand   |
  | 346 | 2011-10-17 17:37:32 | R27/Baseline                                                 | contract |
  {noformat}
  
  This included the upgrade to sqlalchemy 0.7.2 which resolved the remaining mapper configuration issues. 
  
  I've inspected the query for correctness and also confirmed the operation is eagerloaded so there aren't multiple query consistency issues.
  
  My hunch is that this was a symptom of the incompletely compiled mappers. The context also indicates this happened to request #52 in the instance, which is low enough that it's reasonable to expect the mappers to have not been fully configured.
  
  Closing as fixed by upgrade to 0.7.2. We haven't seen this since then, so spending more time on it does not seem prudent.
** Comment: evanc
   :PROPERTIES:
   :ID:       77756
   :created:  2012-06-04 20:32:27
   :END:
  Verified in Test over last several versions.
** Comment: lonnieh
   :PROPERTIES:
   :ID:       116960
   :created:  2013-10-31 14:44:36
   :END:
  This occurred in test:
  
  {noformat}
  2013-10-31 09:19:24 tuk6m1cm2 [err] [wfe.51dcd810243b01318219005056a30032.6/trn.026956e0243b0131f3b1005056a30030.6.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-7-0.9201.request.poweroff_vms.67376/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.2653.poweroff.50.configuration_manager]: [ERROR] threadutils.py:95 - Error executing asynchronous operation asynch-op <bound method UnprovisionNetworkExecutor._execute of <UnprovisionNetworkExecutor(Thread-238, started 140677636949760)>>(, {}) : this is an instance of PL-2936, the query returned a consumer with exclusion at Traceback (most recent call last):
    File "/highland/hosting_platform/hosting_platform/common/threadutils.py", line 87, in run
      self.return_value = self.operation(*self.args, **self.kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 370, in _execute
      UnprovisionTunnelsOperation(logger=self.logger, operation=self.network.operation).execute()
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/tunnels.py", line 195, in __init__
      assert consumer.operation is operation or consumer.operation is None, "this is an instance of PL-2936, the query returned a consumer with exclusion"
  AssertionError: this is an instance of PL-2936, the query returned a consumer with exclusion
  {noformat}
* TODO Enabled VM time_sync bit to be disabled via set_vm_hardware_values operation :PL_7490:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: mpietrek
  :type:     Improvement
  :priority: Major
  :status:   Open
  :components: cm
  :created:  2013-10-29 21:27:42
  :updated:  2013-10-31 17:23:49
  :ID:       PL-7490
  :END:
** description: PL-7490
  We should make this attribute settable by a CM API so that we don't have to keep doing these changes manually in PROD.
* TODO attach_network_vpn fails					    :PL_7506:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: andrus
  :type:     Bug
  :priority: Major
  :status:   Open
  :components: cm
  :created:  2013-10-30 02:44:10
  :updated:  2013-10-31 17:16:53
  :ID:       PL-7506
  :END:
** description: PL-7506
  From error watch:
  
  {code}
  
  2013-10-29 19:16:14 tuk1m1cm2 [err] [wfe.d2cfe6f022f501314eb0005056a300c9.707.3@/greenbox-tuk1m1gb3-1-0.25410.request.attach_network_vpn.125878/configuration_manager-tuk1m1cm2-3.21851.attach_network_vpn.2773.api]: [ERROR] errors.py:209 - attach_network_vpn EXCEPT 7.28 : {"incident_id": "25E2817E", "name": "attach_network_vpn", "timestamp": "2013-10-29 19:16:14", "_original_exception": "AttributeError(\"'NoneType' object has no attribute 'networks'\",)", "__severity": "error", "context": "wfe.d2cfe6f022f501314eb0005056a300c9.707.3@/greenbox-tuk1m1gb3-1-0.25410.request.attach_network_vpn.125878/configuration_manager-tuk1m1cm2-3.21851.attach_network_vpn.2773", "msg": "\"'NoneType' object has no attribute 'networks'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 31, in common_error_handling
      yield
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 45, in attach_network_vpn
      ret = func(cm_instance, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/mixin.py", line 29, in <lambda>
      method = wraps(operation)(lambda self, *args, **kwargs: executor(self, operation, *args, **kwargs))
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 77, in operation_executor
      op = operation(*args, logger=LOGGER, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 1231, in __init__
      self.vpn.nat_pool.allocate(nic, tx)
    File "/usr/lib/python2.6/contextlib.py", line 34, in __exit__
      self.gen.throw(type, value, traceback)
    File "/highland/hosting_platform/hosting_platform/db/db.py", line 106, in _run_transaction
      yield sess
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 1223, in __init__
      self.vpn.validate_routing_domain(self.network)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/model/vpns.py", line 360, in validate_routing_domain
      routing_domain = VPNRoutingDomain(self)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/model/routing_domain.py", line 190, in __init__
      self._add_network(attachment.network, attachment.enabled)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/model/routing_domain.py", line 198, in _add_network
      return self._add_domain(NetworkRoutingDomain, network, secondary=True)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/model/routing_domain.py", line 152, in _add_domain
      domain = domain_type(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/model/routing_domain.py", line 285, in __init__
      for network in network.configuration.networks:
  ----------------------------------------------------------------------
  {code}
* TODO Prod error watch: Run failed because of InvalidSpecification error :PL_7571:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: nastete
  :type:     Bug
  :priority: Critical
  :status:   Open
  :created:  2013-11-05 17:39:25
  :updated:  2013-11-06 21:19:22
  :ID:       PL-7571
  :END:
** description: PL-7571
  {noformat}
  17:37 PROD highland@tuk1m1logger1:~/logs/syslog/2013/11$ grep 6893C82C 2013-11-05-platform.log
  2013-11-05 03:45:59 tuk1m1cm4 [debug] [wfe.160ea9f027f30131834e005056a3001c.6529.197/trn.ab77b4d027f901314d8f74ce4686f1c0.31727.6.1@/greenbox-tuk1m1gb5-5-0.11363.request.run_vms.202889/qm-run-31/configuration_manager-tuk1m1cm4-3.2530.run.7747.configuration_manager]: [DEBUG] errors.py:209 - Failure in run inner logic detected by QM resource manager: {"incident_id": "6893C82C", "name": "run", "context": "wfe.160ea9f027f30131834e005056a3001c.6529.197/trn.ab77b4d027f901314d8f74ce4686f1c0.31727.6.1@/greenbox-tuk1m1gb5-5-0.11363.request.run_vms.202889/qm-run-31/configuration_manager-tuk1m1cm4-3.2530.run.7747", "__severity": "error", "msg": "'nic-899006-2182644-2 is attached to manual network network-899006-898182 and public ip pip-199.204.220.69'", "timestamp": "2013-11-05 03:45:58", "type": "InvalidSpecification"}
  2013-11-05 03:45:59 tuk1m1cm4 [debug] [wfe.160ea9f027f30131834e005056a3001c.6529.197/trn.ab77b4d027f901314d8f74ce4686f1c0.31727.6.1@/greenbox-tuk1m1gb5-5-0.11363.request.run_vms.202889/configuration_manager-tuk1m1cm4-3.2530.run.7747.api]: [DEBUG] errors.py:209 - run EXCEPT 0.70 : {"incident_id": "6893C82C", "name": "run", "context": "wfe.160ea9f027f30131834e005056a3001c.6529.197/trn.ab77b4d027f901314d8f74ce4686f1c0.31727.6.1@/greenbox-tuk1m1gb5-5-0.11363.request.run_vms.202889/qm-run-31/configuration_manager-tuk1m1cm4-3.2530.run.7747", "__severity": "error", "msg": "'nic-899006-2182644-2 is attached to manual network network-899006-898182 and public ip pip-199.204.220.69'", "timestamp": "2013-11-05 03:45:58", "type": "InvalidSpecification"}
  2013-11-05 03:45:59 tuk1m1gb5 [debug] [wfe.160ea9f027f30131834e005056a3001c.6529.197/trn.ab77b4d027f901314d8f74ce4686f1c0.31727.6.1@/greenbox-tuk1m1gb5-5-0.11363.request.run_vms.202889.call_node_trace]: [DEBUG] CallNode.py:183 - on-error: <Fault 1: '{"incident_id": "6893C82C", "name": "run", "context": "wfe.160ea9f027f30131834e005056a3001c.6529.197/trn.ab77b4d027f901314d8f74ce4686f1c0.31727.6.1@/greenbox-tuk1m1gb5-5-0.11363.request.run_vms.202889/qm-run-31/configuration_manager-tuk1m1cm4-3.2530.run.7747", "__severity": "error", "msg": "\'nic-899006-2182644-2 is attached to manual network network-899006-898182 and public ip pip-199.204.220.69\'", "timestamp": "2013-11-05 03:45:58", "type": "InvalidSpecification"}'>
  2013-11-05 03:45:59 tuk1m1gb5 [debug] [wfe.160ea9f027f30131834e005056a3001c.6529.197/trn.ab77b4d027f901314d8f74ce4686f1c0.31727.6.1@/greenbox-tuk1m1gb5-5-0.11363.request.run_vms.202889.call_node_trace]: [DEBUG] CallNode.py:149 - 66110544: raised <Fault 1: '{"incident_id": "6893C82C", "name": "run", "context": "wfe.160ea9f027f30131834e005056a3001c.6529.197/trn.ab77b4d027f901314d8f74ce4686f1c0.31727.6.1@/greenbox-tuk1m1gb5-5-0.11363.request.run_vms.202889/qm-run-31/configuration_manager-tuk1m1cm4-3.2530.run.7747", "__severity": "error", "msg": "\'nic-899006-2182644-2 is attached to manual network network-899006-898182 and public ip pip-199.204.220.69\'", "timestamp": "2013-11-05 03:45:58", "type": "InvalidSpecification"}'>
  {noformat}
  
  I have no visibility into what happened; just saw it showed up in error watch.
** Comment: lonnieh
   :PROPERTIES:
   :ID:       117602
   :created:  2013-11-06 17:39:28
   :END:
  Instantiated from template configuration-899002. This vm has 6 disconnected nics and some public IPs.
  
  Repro:
  # create a configuration (A) with a single vm attached to an automatic network with one PIP
  # create a template of configuration A *without* the network
  # create a configuration (B) from the template
  # create a *manual* network in the configuration B
  # attach the NIC to the network
  # try to run the vm
  
  This problem does not repro when you use disconnect to change from an auto netowrk to manual network. It appears this is because the disconnect removes the PIP attachment.
  
  I think merge() should be updated to properly disconnect the PIP when the attached network is not part of the merge.
  
  There may be similar issues with published services.
** Comment: lonnieh
   :PROPERTIES:
   :ID:       117604
   :created:  2013-11-06 17:44:51
   :END:
  Bing, this error indicates that we are refusing to provision a configuration that is in an illegal state (pip attached to manual network).
  
  I wrote up the repro steps, as well as what I think the issue may be in a previous comment.
  
  Due to similarities in how public ips and published services are implemented, I'm worried we have the same issue with public services. Can you check whether we need to do work there as well?
  
** Comment: lonnieh
   :PROPERTIES:
   :ID:       117606
   :created:  2013-11-06 17:46:07
   :END:
  R50 for now, but may need to slide to r51 depending on workload
** Comment: bxiao
   :PROPERTIES:
   :ID:       117642
   :created:  2013-11-06 21:19:22
   :END:
  I'll see whether I can create a system test case for this.
* TODO Error on run: "AttributeError(\"'dict' object has no attribute 'severity'\" :PL_7591:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: ybranch
  :type:     Bug
  :priority: Critical
  :status:   Open
  :components: accounting
  :created:  2013-11-06 16:22:56
  :updated:  2013-11-07 18:45:57
  :ID:       PL-7591
  :END:
** description: PL-7591
  {code}
  ---------------------------------------------------------
  
  2013-10-22 06:52:04 tuk6m1cm1 [err] [wfe.4c5d52401d14013181dc005056a30032.12/trn.84b329a01d100131f358005056a30030.5.1@/greenbox-tuk6m1gb2.mgt.test.skytap.com-8-0.5742.request.run_vms.1598/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.10668.run.37.configuration_manager]: [ERROR] errors.py:209 - Failure in AS.process_events for run: {"incident_id": "53B25AE0", "name": "run", "timestamp": "2013-10-22 06:52:04", "_original_exception": "AttributeError(\"'dict' object has no attribute 'severity'\",)", "__severity": "error", "context": "wfe.4c5d52401d14013181dc005056a30032.12/trn.84b329a01d100131f358005056a30030.5.1@/greenbox-tuk6m1gb2.mgt.test.skytap.com-8-0.5742.request.run_vms.1598/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.10668.run.37", "msg": "\"'dict' object has no attribute 'severity'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/quota_manager.py", line 143, in quota_allocation_management
      result = AccountingService().process_events(events, charge_tag, effective_context=event_context_string)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 170, in process_events
      return self.process_event_tuples(event_tuples, charge_tag, effective_context)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 210, in process_event_tuples
      raise self._translate_remote_error(result['error'])
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 114, in _translate_remote_error
      err = Error.loads(Error.dumps(remote_error))
    File "/highland/hosting_platform/hosting_platform/common/errors/errors.py", line 150, in dumps
      obj['__severity'] = obj.severity
  ----------------------------------------------------------------------
  2013-10-22 06:52:04 tuk6m1cm1 [err] [wfe.4c5d52401d14013181dc005056a30032.12/trn.84b329a01d100131f358005056a30030.5.1@/greenbox-tuk6m1gb2.mgt.test.skytap.com-8-0.5742.request.run_vms.1598/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.10668.run.37.api]: [ERROR] errors.py:209 - run EXCEPT 0.99 : {"incident_id": "7090B8C9", "name": "run", "timestamp": "2013-10-22 06:52:04", "_original_exception": "AttributeError(\"'dict' object has no attribute 'severity'\",)", "__severity": "error", "context": "wfe.4c5d52401d14013181dc005056a30032.12/trn.84b329a01d100131f358005056a30030.5.1@/greenbox-tuk6m1gb2.mgt.test.skytap.com-8-0.5742.request.run_vms.1598/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.10668.run.37", "msg": "\"'dict' object has no attribute 'severity'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 31, in common_error_handling
      yield
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 45, in run
      ret = func(cm_instance, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/mixin.py", line 29, in <lambda>
      method = wraps(operation)(lambda self, *args, **kwargs: executor(self, operation, *args, **kwargs))
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 77, in operation_executor
      op = operation(*args, logger=LOGGER, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/provision.py", line 172, in __init__
      self.allocate_resources()
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/agent.py", line 376, in __exit__
      super(_GeneratorContextManager, self).__exit__(*args, **kwargs)
    File "/usr/lib/python2.6/contextlib.py", line 23, in __exit__
      self.gen.next()
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/quota_manager.py", line 143, in quota_allocation_management
      result = AccountingService().process_events(events, charge_tag, effective_context=event_context_string)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 170, in process_events
      return self.process_event_tuples(event_tuples, charge_tag, effective_context)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 210, in process_event_tuples
      raise self._translate_remote_error(result['error'])
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 114, in _translate_remote_error
      err = Error.loads(Error.dumps(remote_error))
    File "/highland/hosting_platform/hosting_platform/common/errors/errors.py", line 150, in dumps
      obj['__severity'] = obj.severity
  ----------------------------------------------------------------------
  2013-10-22 06:52:29 tuk6m1cm3 [err] [wfe.e904dc901d12013181dc005056a30032.13/trn.8a19d7501d0f0131f358005056a30030.8.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-4-0.30208.request.run_vms.1621/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-1.3824.run.3.configuration_manager]: [ERROR] errors.py:209 - Failure in AS.process_events for run: {"incident_id": "65C995B8", "name": "run", "timestamp": "2013-10-22 06:52:29", "_original_exception": "AttributeError(\"'dict' object has no attribute 'severity'\",)", "__severity": "error", "context": "wfe.e904dc901d12013181dc005056a30032.13/trn.8a19d7501d0f0131f358005056a30030.8.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-4-0.30208.request.run_vms.1621/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-1.3824.run.3", "msg": "\"'dict' object has no attribute 'severity'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/quota_manager.py", line 143, in quota_allocation_management
      result = AccountingService().process_events(events, charge_tag, effective_context=event_context_string)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 170, in process_events
      return self.process_event_tuples(event_tuples, charge_tag, effective_context)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 210, in process_event_tuples
      raise self._translate_remote_error(result['error'])
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 114, in _translate_remote_error
      err = Error.loads(Error.dumps(remote_error))
    File "/highland/hosting_platform/hosting_platform/common/errors/errors.py", line 150, in dumps
      obj['__severity'] = obj.severity
  ----------------------------------------------------------------------
  2013-10-22 06:52:29 tuk6m1cm3 [err] [wfe.e904dc901d12013181dc005056a30032.13/trn.8a19d7501d0f0131f358005056a30030.8.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-4-0.30208.request.run_vms.1621/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-1.3824.run.3.api]: [ERROR] errors.py:209 - run EXCEPT 1.25 : {"incident_id": "0DAD8081", "name": "run", "timestamp": "2013-10-22 06:52:29", "_original_exception": "AttributeError(\"'dict' object has no attribute 'severity'\",)", "__severity": "error", "context": "wfe.e904dc901d12013181dc005056a30032.13/trn.8a19d7501d0f0131f358005056a30030.8.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-4-0.30208.request.run_vms.1621/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-1.3824.run.3", "msg": "\"'dict' object has no attribute 'severity'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 31, in common_error_handling
      yield
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 45, in run
      ret = func(cm_instance, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/mixin.py", line 29, in <lambda>
      method = wraps(operation)(lambda self, *args, **kwargs: executor(self, operation, *args, **kwargs))
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 77, in operation_executor
      op = operation(*args, logger=LOGGER, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/provision.py", line 172, in __init__
      self.allocate_resources()
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/agent.py", line 376, in __exit__
      super(_GeneratorContextManager, self).__exit__(*args, **kwargs)
    File "/usr/lib/python2.6/contextlib.py", line 23, in __exit__
      self.gen.next()
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/quota_manager.py", line 143, in quota_allocation_management
      result = AccountingService().process_events(events, charge_tag, effective_context=event_context_string)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 170, in process_events
      return self.process_event_tuples(event_tuples, charge_tag, effective_context)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 210, in process_event_tuples
      raise self._translate_remote_error(result['error'])
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 114, in _translate_remote_error
      err = Error.loads(Error.dumps(remote_error))
    File "/highland/hosting_platform/hosting_platform/common/errors/errors.py", line 150, in dumps
      obj['__severity'] = obj.severity
  ----------------------------------------------------------------------
  2013-10-22 06:53:25 tuk6m1cm3 [err] [wfe.5358c7a01d14013181c5005056a30032.7/trn.82f3f7901d0e0131f358005056a30030.17.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-0-0.30105.request.run_vms.1570/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-0.3949.run.16.configuration_manager]: [ERROR] errors.py:209 - Failure in AS.process_events for run: {"incident_id": "365CA7E5", "name": "run", "timestamp": "2013-10-22 06:53:25", "_original_exception": "AttributeError(\"'dict' object has no attribute 'severity'\",)", "__severity": "error", "context": "wfe.5358c7a01d14013181c5005056a30032.7/trn.82f3f7901d0e0131f358005056a30030.17.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-0-0.30105.request.run_vms.1570/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-0.3949.run.16", "msg": "\"'dict' object has no attribute 'severity'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/quota_manager.py", line 143, in quota_allocation_management
      result = AccountingService().process_events(events, charge_tag, effective_context=event_context_string)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 170, in process_events
      return self.process_event_tuples(event_tuples, charge_tag, effective_context)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 210, in process_event_tuples
      raise self._translate_remote_error(result['error'])
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 114, in _translate_remote_error
      err = Error.loads(Error.dumps(remote_error))
    File "/highland/hosting_platform/hosting_platform/common/errors/errors.py", line 150, in dumps
      obj['__severity'] = obj.severity
  ----------------------------------------------------------------------
  2013-10-22 06:53:25 tuk6m1cm3 [err] [wfe.5358c7a01d14013181c5005056a30032.7/trn.82f3f7901d0e0131f358005056a30030.17.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-0-0.30105.request.run_vms.1570/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-0.3949.run.16.api]: [ERROR] errors.py:209 - run EXCEPT 1.02 : {"incident_id": "3253EDE3", "name": "run", "timestamp": "2013-10-22 06:53:25", "_original_exception": "AttributeError(\"'dict' object has no attribute 'severity'\",)", "__severity": "error", "context": "wfe.5358c7a01d14013181c5005056a30032.7/trn.82f3f7901d0e0131f358005056a30030.17.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-0-0.30105.request.run_vms.1570/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-0.3949.run.16", "msg": "\"'dict' object has no attribute 'severity'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 31, in common_error_handling
      yield
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 45, in run
      ret = func(cm_instance, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/mixin.py", line 29, in <lambda>
      method = wraps(operation)(lambda self, *args, **kwargs: executor(self, operation, *args, **kwargs))
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 77, in operation_executor
      op = operation(*args, logger=LOGGER, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/provision.py", line 172, in __init__
      self.allocate_resources()
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/agent.py", line 376, in __exit__
      super(_GeneratorContextManager, self).__exit__(*args, **kwargs)
    File "/usr/lib/python2.6/contextlib.py", line 23, in __exit__
      self.gen.next()
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/quota_manager.py", line 143, in quota_allocation_management
      result = AccountingService().process_events(events, charge_tag, effective_context=event_context_string)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 170, in process_events
      return self.process_event_tuples(event_tuples, charge_tag, effective_context)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 210, in process_event_tuples
      raise self._translate_remote_error(result['error'])
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 114, in _translate_remote_error
      err = Error.loads(Error.dumps(remote_error))
    File "/highland/hosting_platform/hosting_platform/common/errors/errors.py", line 150, in dumps
      obj['__severity'] = obj.severity
  ----------------------------------------------------------------------
  2013-10-22 06:53:41 tuk6m1cm2 [err] [wfe.869457a01d14013181c4005056a30032.8/trn.7d917cf01d0f0131f358005056a30030.8.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-0-0.30105.request.run_vms.1576/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.29992.run.19.configuration_manager]: [ERROR] errors.py:209 - Failure in AS.process_events for run: {"incident_id": "2B5A5315", "name": "run", "timestamp": "2013-10-22 06:53:41", "_original_exception": "AttributeError(\"'dict' object has no attribute 'severity'\",)", "__severity": "error", "context": "wfe.869457a01d14013181c4005056a30032.8/trn.7d917cf01d0f0131f358005056a30030.8.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-0-0.30105.request.run_vms.1576/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.29992.run.19", "msg": "\"'dict' object has no attribute 'severity'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/quota_manager.py", line 143, in quota_allocation_management
      result = AccountingService().process_events(events, charge_tag, effective_context=event_context_string)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 170, in process_events
      return self.process_event_tuples(event_tuples, charge_tag, effective_context)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 210, in process_event_tuples
      raise self._translate_remote_error(result['error'])
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 114, in _translate_remote_error
      err = Error.loads(Error.dumps(remote_error))
    File "/highland/hosting_platform/hosting_platform/common/errors/errors.py", line 150, in dumps
      obj['__severity'] = obj.severity
  ----------------------------------------------------------------------
  2013-10-22 06:53:41 tuk6m1cm2 [err] [wfe.869457a01d14013181c4005056a30032.8/trn.7d917cf01d0f0131f358005056a30030.8.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-0-0.30105.request.run_vms.1576/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.29992.run.19.api]: [ERROR] errors.py:209 - run EXCEPT 1.18 : {"incident_id": "3492714F", "name": "run", "timestamp": "2013-10-22 06:53:41", "_original_exception": "AttributeError(\"'dict' object has no attribute 'severity'\",)", "__severity": "error", "context": "wfe.869457a01d14013181c4005056a30032.8/trn.7d917cf01d0f0131f358005056a30030.8.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-0-0.30105.request.run_vms.1576/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.29992.run.19", "msg": "\"'dict' object has no attribute 'severity'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 31, in common_error_handling
      yield
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 45, in run
      ret = func(cm_instance, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/mixin.py", line 29, in <lambda>
      method = wraps(operation)(lambda self, *args, **kwargs: executor(self, operation, *args, **kwargs))
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 77, in operation_executor
      op = operation(*args, logger=LOGGER, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/provision.py", line 172, in __init__
      self.allocate_resources()
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/agent.py", line 376, in __exit__
      super(_GeneratorContextManager, self).__exit__(*args, **kwargs)
    File "/usr/lib/python2.6/contextlib.py", line 23, in __exit__
      self.gen.next()
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/quota_manager.py", line 143, in quota_allocation_management
      result = AccountingService().process_events(events, charge_tag, effective_context=event_context_string)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 170, in process_events
      return self.process_event_tuples(event_tuples, charge_tag, effective_context)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 210, in process_event_tuples
      raise self._translate_remote_error(result['error'])
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 114, in _translate_remote_error
      err = Error.loads(Error.dumps(remote_error))
    File "/highland/hosting_platform/hosting_platform/common/errors/errors.py", line 150, in dumps
      obj['__severity'] = obj.severity
  ----------------------------------------------------------------------
  2013-10-22 06:53:55 tuk6m1cm2 [err] [wfe.1a5e36e01d1401316d13005056a3002c.10/trn.7d917cf01d0f0131f358005056a30030.9.1@/greenbox-tuk6m1gb2.mgt.test.skytap.com-8-0.5742.request.run_vms.1624/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.29992.run.32.configuration_manager]: [ERROR] errors.py:209 - Failure in AS.process_events for run: {"incident_id": "5A1E5523", "name": "run", "timestamp": "2013-10-22 06:53:55", "_original_exception": "AttributeError(\"'dict' object has no attribute 'severity'\",)", "__severity": "error", "context": "wfe.1a5e36e01d1401316d13005056a3002c.10/trn.7d917cf01d0f0131f358005056a30030.9.1@/greenbox-tuk6m1gb2.mgt.test.skytap.com-8-0.5742.request.run_vms.1624/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.29992.run.32", "msg": "\"'dict' object has no attribute 'severity'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/quota_manager.py", line 143, in quota_allocation_management
      result = AccountingService().process_events(events, charge_tag, effective_context=event_context_string)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 170, in process_events
      return self.process_event_tuples(event_tuples, charge_tag, effective_context)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 210, in process_event_tuples
      raise self._translate_remote_error(result['error'])
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 114, in _translate_remote_error
      err = Error.loads(Error.dumps(remote_error))
    File "/highland/hosting_platform/hosting_platform/common/errors/errors.py", line 150, in dumps
      obj['__severity'] = obj.severity
  ----------------------------------------------------------------------
  2013-10-22 06:53:55 tuk6m1cm2 [err] [wfe.1a5e36e01d1401316d13005056a3002c.10/trn.7d917cf01d0f0131f358005056a30030.9.1@/greenbox-tuk6m1gb2.mgt.test.skytap.com-8-0.5742.request.run_vms.1624/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.29992.run.32.api]: [ERROR] errors.py:209 - run EXCEPT 1.10 : {"incident_id": "0F98675D", "name": "run", "timestamp": "2013-10-22 06:53:55", "_original_exception": "AttributeError(\"'dict' object has no attribute 'severity'\",)", "__severity": "error", "context": "wfe.1a5e36e01d1401316d13005056a3002c.10/trn.7d917cf01d0f0131f358005056a30030.9.1@/greenbox-tuk6m1gb2.mgt.test.skytap.com-8-0.5742.request.run_vms.1624/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.29992.run.32", "msg": "\"'dict' object has no attribute 'severity'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 31, in common_error_handling
      yield
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 45, in run
      ret = func(cm_instance, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/mixin.py", line 29, in <lambda>
      method = wraps(operation)(lambda self, *args, **kwargs: executor(self, operation, *args, **kwargs))
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 77, in operation_executor
      op = operation(*args, logger=LOGGER, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/provision.py", line 172, in __init__
      self.allocate_resources()
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/agent.py", line 376, in __exit__
      super(_GeneratorContextManager, self).__exit__(*args, **kwargs)
    File "/usr/lib/python2.6/contextlib.py", line 23, in __exit__
      self.gen.next()
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/quota_manager.py", line 143, in quota_allocation_management
      result = AccountingService().process_events(events, charge_tag, effective_context=event_context_string)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 170, in process_events
      return self.process_event_tuples(event_tuples, charge_tag, effective_context)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 210, in process_event_tuples
      raise self._translate_remote_error(result['error'])
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/service_apis/accounting_service.py", line 114, in _translate_remote_error
      err = Error.loads(Error.dumps(remote_error))
    File "/highland/hosting_platform/hosting_platform/common/errors/errors.py", line 150, in dumps
      obj['__severity'] = obj.severity
  {code}
** Comment: lonnieh
   :PROPERTIES:
   :ID:       117583
   :created:  2013-11-06 16:29:27
   :END:
  looks like we are trying to use a dict as if it were an error...probably need to change this to "err = Error(error)" rather than converting to and from json to get an error.
** Comment: andrus
   :PROPERTIES:
   :ID:       117688
   :created:  2013-11-07 00:05:32
   :END:
  This is the _translate_remote_exception path. 
  
  In this scenario, exceptions raised lower down in the CM stack will show up for mapping into "nice" exceptions - and it's likely they won't have nice properties. Like error attribtues.
  
  
  
