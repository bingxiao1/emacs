* DONE Creating new network throws InvalidParameterError	    :PL_7157:
  :PROPERTIES:
  :assignee: arobinson
  :reporter: arobinson
  :type:     Bug
  :priority: Blocker
  :status:   Resolved
  :components: cm
  :created:  2013-10-17 20:58:41
  :updated:  2013-10-21 19:04:09
  :ID:       PL-7157
  :resolution: Fixed
  :END:
** description: PL-7157
  Trying to add a new network to a configuration fails.
  
  REPRO:
  1. Create config from template that has existing network.
  2. Attempt to add new network, making sure subnet does not overlap existing network.
  3. Click on Create Network
  
  EXPECTED:
  New network created.
  
  ACTUAL:
  Error message:
  
  {noformat}
  The operation cannot be performed at this time. Please try again or contact support. (InvalidParameterError, 40E0D375)
  {noformat}
  
  Network is not created.
  
  Error watch:
  {noformat}---------------------------------------------------------
  
  2013-10-17 20:56:12 tuk6m1wfe4 [err] [/wfe.ab0026401974013181b1005056a30032.151]: [ERROR] logging.rb:47 - API EXCEPTION! The operation cannot be performed at this time. Please try again or contact support. (InvalidParameterError, 40E0D375) [Platform::Client::MQClient::Error]
   (in: create_network(["configuration-613246", "automatic", {"nat_cidr"=>nil, "domain_name"=>"test.net", "cidr_block"=>"10.0.0.0/24", "primary_nameserver"=>nil, "secondary_nameserver"=>nil}])(caller_options: {"samba_fileserver"=>true, "prefer_kvm"=>true})
  ----------------------------------------------------------------------{noformat}
** Comment: andrus
   :PROPERTIES:
   :ID:       114793
   :created:  2013-10-18 16:12:51
   :END:
  Giving to you as this seems like it might be rate-limiting related. You'll need to poke around in the logs a bit... see me for pointers.
** Comment: bxiao
   :PROPERTIES:
   :ID:       114918
   :created:  2013-10-18 21:00:13
   :END:
  Investigating...
** Comment: bxiao
   :PROPERTIES:
   :ID:       114919
   :created:  2013-10-18 21:01:54
   :END:
  Hi Ross, 
  where can I get the full log? 
** Comment: bxiao
   :PROPERTIES:
   :ID:       114920
   :created:  2013-10-18 21:07:38
   :END:
  Looks like tuk6m1logger1 has the right logs.
** Comment: bxiao
   :PROPERTIES:
   :ID:       114927
   :created:  2013-10-18 21:46:07
   :END:
  Looks like there is None parameter which should be a string in create_network
  
  2013-10-17 20:56:12 tuk6m1gb2 [debug] [wfe.ab0026401974013181b1005056a30032.151.1@/tuk6m1gb2.mgt.test.skytap.com-0.22078.request.create_network.586.greenbox]: [DEBUG] errors.py:209 - Error processing message: MQRequestMessage(action:create_network, unique_id:tuk6m1wfe4:req:7a8642d0199c013181b1005056a30032:wfe.ab0026401974013181b1005056a30032.151.1:316): {"incident_id": "40E0D375", "name": "request.create_network", "context": "wfe.ab0026401974013181b1005056a30032.151.1@/tuk6m1gb2.mgt.test.skytap.com-0.22078.request.create_network.586", "__severity": "error", "msg": "\"None is not of type 'string'\"", "timestamp": "2013-10-17 20:56:12", "type": "InvalidParameterError", "resources": []}  
  
** Comment: bxiao
   :PROPERTIES:
   :ID:       114947
   :created:  2013-10-18 22:41:39
   :END:
  Here is the payload that causes the error:
  
  
  2013-10-17 20:56:12 tuk6m1gb2 [debug] [/greenbox-tuk6m1gb2.mgt.test.skytap.com-6.22078.greenbox]: [DEBUG] MQFramework.py:806 - MQFramework:gr
  eenbox, read, request,  time_in_queue:35 {"_mq_protocol_version": 1, "_timestamp": "2013-10-17T20:56:12Z", "_reply_routing_key": "web-rpc.ab0
  026401974013181b1005056a30032.test.default", "caller_context": {"context_id": "wfe.ab0026401974013181b1005056a30032.151.1@", "caller_options"
  : {"samba_fileserver": true, "prefer_kvm": true}}, "destination_service": "greenbox", "_unique_id": "tuk6m1wfe4:req:7a8642d0199c013181b100505
  6a30032:wfe.ab0026401974013181b1005056a30032.151.1:316", "_message_type": "request", "action": "create_network", "payload": {"network_attribu
  tes": {"secondary_nameserver": null, "cidr_block": "10.0.0.0/24", "primary_nameserver": null, "domain_name": "test.net", "nat_cidr": null}, "
  configuration_key": "configuration-613246", "network_type": "automatic"}}
  
  And the problem is "primary_nameserver" is null:
  
  Failed validating 'type' in schema['properties']['network_attributes']['properties']['primary_nameserver']:
      {'type': 'string'}
  
  

** Comment: bxiao
   :PROPERTIES:
   :ID:       114950
   :created:  2013-10-18 23:08:17
   :END:
  The "None" primary_nameserver come from configuration specification:
  
  
  2013-10-17 20:56:11 tuk6m1cm3 [debug] [wfe.ab0026401974013181b1005056a30032.151.1@/tuk6m1gb1.mgt.test.skytap.com-0.13850.request.get_configur
  ation_specification.602/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-1.32118.get_configuration_specification.8.api]: [DEBUG] Configura
  tionManager.py:46 - get_configuration_specification RETURN 0.09 {'configuration-613246': ({'disable_internet': False, 'routable': False, 'run
  nable': True, 'region': 'test/tuk6r1', 'vms': [{'hardware': {'disk_policy': {'key': 'disk_policy-100', 'iops_limit': '275', 'size_limit': '20
  97152', 'disk_limit': '15', 'disk_size_limit': '2096128', 'offset': '165'}, 'uuid': None, 'floppy': 'fd0', 'nics': [{'nic_type': 'e1000', 'pr
  omiscuous_mode_enabled': False, 'nat_addresses': {'networks': {}, 'vpns': {}}, 'vm_key': 'vm-613246-4136714', 'virtual_dev': 'e1000', 'hostna
  me': 'auto-host-1', 'public_ips': [], 'mac': '00:50:56:19:c9:fc', 'published_services': [{'external_address': '199.204.217.3', 'internal_port
  ': '22', 'external_port': '24778'}], 'key': 'nic-613246-4136714-0', 'address': '10.0.2.1', 'configuration_key': 'configuration-613246', 'netw
  ork_key': 'network-613246-452584'}], 'hardware_policy': {'max_ram': '32768', 'min_ram': '256', 'max_cpus': '8', 'min_cpus': '1'}, 'ram': '102
  4', 'hardware_version': '7', 'cpus': '1', 'guest_cpu_type': '4', 'vnc_keymap': None, 'disk_controllers': [{'bus_type': 'ide', 'controller': '
  0', 'disks': [{'size': '0', 'device_type': 'cdrom', 'key': 'disk-613246-4136714-ide-0-0', 'lun': '0'}], 'virtual_dev': None, 'key': 'disk_con
  troller-613246-4136714-ide-0'}, {'bus_type': 'scsi', 'controller': '0', 'disks': [{'size': '20480', 'device_type': 'disk', 'key': 'disk-61324
  6-4136714-scsi-0-0', 'lun': '0'}], 'virtual_dev': 'lsilogic', 'key': 'disk_controller-613246-4136714-scsi-0'}], 'host_cpu_type': None, 'guest
  OS': 'ubuntu-64'}, 'vm_extra_options_groups': [], 'key': 'vm-613246-4136714'}], 'account_keys': ['account-2'], 'key': 'configuration-613246',
   'networks': [{'nat_cidr': None, 'domain_name': 'auto-test-1.net', 'network_type': 'automatic', 'nat_pool': None, 'primary_nameserver': None,
   'vpn_attachments': [], 'key': 'network-613246-452584', 'secondary_nameserver': None, 'cidr_block': '10.0.2.0/24', 'configuration_key': 'conf
  iguration-613246', 'gateway': '10.0.2.254', 'tunnels': []}]}, 0)}
  
  Now I'm not sure who should look at this. It seems to be reasonable to not specify nameservers when creating configuration, but it will be needed when creating network.
  
  - should WFE be responsible to always pass a valid primary nameserver when sending create network request?
  - should GB wrap this in client error so 
** Comment: bxiao
   :PROPERTIES:
   :ID:       114951
   :created:  2013-10-18 23:10:55
   :END:
  
  I talked to Ross, it seems that schema should allow None primary_nameserver. Since I created unit test for this I'll go ahead and fix the schema.
** Comment: bxiao
   :PROPERTIES:
   :ID:       115148
   :created:  2013-10-21 19:03:33
   :END:
  This is fixed by MattP @ changeset 12621. Resolving.
* TODO cm deadlock in delete configuration			    :PL_6968:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: lonnieh
  :type:     Bug
  :priority: Critical
  :status:   Open
  :components: cm
  :created:  2013-09-20 13:42:05
  :updated:  2013-09-25 19:07:31
  :ID:       PL-6968
  :END:
** description: PL-6968
  {noformat}
  2013-09-20 13:33:42 tuk6m1cm2 [err] [wfe.77f43650039001316c8e005056a3002c.756.11@/tuk6m1gb2.mgt.test.skytap.com-0.26087.request.delete_configuration.1070513/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.29382.delete_configuration.49.configuration_manager]: [ERROR] operations.py:135 - operation completed with exclusions: configurations: [configuration-593492] vms: [] networks: [network-593492-437262] account_depots: [] vpns: [] tunnels: [] tunnel_consumers: []
  ----------------------------------------------------------------------
  2013-09-20 13:33:42 tuk6m1cm2 [err] [wfe.77f43650039001316c8e005056a3002c.756.11@/tuk6m1gb2.mgt.test.skytap.com-0.26087.request.delete_configuration.1070513/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-0.29382.delete_configuration.49.configuration_manager]: [ERROR] threadutils.py:95 - Error executing asynchronous operation asynch-op <function execute at 0x5a5c230>(, {}) : (OperationalError) (1213, 'Deadlock found when trying to get lock; try restarting transaction') 'DELETE FROM configurations WHERE configurations.configuration_id = %s' (593492L,) at Traceback (most recent call last):
    File "/highland/hosting_platform/hosting_platform/common/threadutils.py", line 87, in run
      self.return_value = self.operation(*self.args, **self.kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 62, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 80, in execute
      self.provision()  # pylint: disable=E1101
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 596, in provision
      self.unprovisioned_hook()
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/delete.py", line 209, in unprovisioned_hook
      tx.query(Configuration).filter(Configuration.configuration_id == self.configuration.configuration_id).delete()
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/query.py", line 2283, in delete
      result = session.execute(delete_stmt, params=self._params)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/orm/session.py", line 763, in execute
      clause, params or {})
    File "/usr/lib/pymodules/python2.6/sqlalchemy/engine/base.py", line 1399, in execute
      params)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/engine/base.py", line 1532, in _execute_clauseelement
      compiled_sql, distilled_params
    File "/usr/lib/pymodules/python2.6/sqlalchemy/engine/base.py", line 1640, in _execute_context
      context)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/engine/base.py", line 1633, in _execute_context
      context)
    File "/usr/lib/pymodules/python2.6/sqlalchemy/engine/default.py", line 325, in do_execute
      cursor.execute(statement, parameters)
    File "/usr/lib/pymodules/python2.6/MySQLdb/cursors.py", line 166, in execute
      self.errorhandler(self, exc, value)
    File "/usr/lib/pymodules/python2.6/MySQLdb/connections.py", line 35, in defaulterrorhandler
      raise errorclass, errorvalue
  OperationalError: (OperationalError) (1213, 'Deadlock found when trying to get lock; try restarting transaction') 'DELETE FROM configurations WHERE configurations.configuration_id = %s' (593492L,)
  ----------------------------------------------------------------------{noformat}
  
  deadlock from mysql:
  {noformat}
  ------------------------
  LATEST DETECTED DEADLOCK
  ------------------------
  130913 13:27:06
  *** (1) TRANSACTION:
  TRANSACTION 0 1329308440, ACTIVE 0 sec, process no 1726, OS thread id 140041075902208 fetching rows
  mysql tables in use 6, locked 4
  LOCK WAIT 4 lock struct(s), heap size 1216, 9 row lock(s)
  MySQL thread id 4158825, query id 2579798706 tuk1m1cm5.mgt.prod.skytap.com 10.8.16.39 root Sending data
  SELECT tunnel_consumers.tunnel_key AS tunnel_consumers_tunnel_key, tunnel_consumers.left_network_id AS tunnel_consumers_left_network_id, tunnel_consumers.right_network_id AS tunnel_consumers_right_network_id, tunnel_consumers.region AS tunnel_consumers_region, tunnel_consumers.left_endpoint_active AS tunnel_consumers_left_endpoint_active, tunnel_consumers.right_endpoint_active AS tunnel_consumers_right_endpoint_active, operations_1.operation_id AS operations_1_operation_id, operations_1.timestamp AS operations_1_timestamp, operations_1.service_instance_id AS operations_1_service_instance_id, operations_1.operation_type AS operations_1_operation_type, operations_1.context_id AS operations_1_context_id, operations_1.fleeting AS operations_1_fleeting, operations_1._type AS operations_1__type 
  FROM tunnel_consumer_operation_exclusion AS tunnel_consumer_operation_exclusion_1, tunnel_consumers LEFT OUTER JOIN tu
  *** (1) WAITING FOR THIS LOCK TO BE GRANTED:
  RECORD LOCKS space id 0 page no 91754 n bits 208 index `fk_tunnel_consumer_operation_exclusion_operations_operation_id` of table `configuration`.`tunnel_consumer_operation_exclusion` trx id 0 1329308440 lock_mode X locks rec but not gap waiting
  Record lock, heap no 137 PHYSICAL RECORD: n_fields 2; compact format; info bits 32
   0: len 4; hex 8114b8b2; asc     ;; 1: len 19; hex 74756e6e656c2d39303432302d313131363734; asc tunnel-90420-111674;;
  
  *** (2) TRANSACTION:
  TRANSACTION 0 1329308430, ACTIVE 0 sec, process no 1726, OS thread id 140041115305728 starting index read, thread declared inside InnoDB 500
  mysql tables in use 1, locked 1
  8 lock struct(s), heap size 1216, 6 row lock(s), undo log entries 4
  MySQL thread id 4158790, query id 2579798759 tuk1m1cm1.mgt.prod.skytap.com 10.8.16.35 root updating
  DELETE FROM tunnel_consumer_operation_exclusion WHERE tunnel_consumer_operation_exclusion.tunnel_key = 'tunnel-111656-111674' AND tunnel_consumer_operation_exclusion.operation_id = 18135218
  *** (2) HOLDS THE LOCK(S):
  RECORD LOCKS space id 0 page no 91754 n bits 208 index `fk_tunnel_consumer_operation_exclusion_operations_operation_id` of table `configuration`.`tunnel_consumer_operation_exclusion` trx id 0 1329308430 lock_mode X locks rec but not gap
  Record lock, heap no 137 PHYSICAL RECORD: n_fields 2; compact format; info bits 32
   0: len 4; hex 8114b8b2; asc     ;; 1: len 19; hex 74756e6e656c2d39303432302d313131363734; asc tunnel-90420-111674;;
  
  *** (2) WAITING FOR THIS LOCK TO BE GRANTED:
  RECORD LOCKS space id 0 page no 91753 n bits 104 index `PRIMARY` of table `configuration`.`tunnel_consumer_operation_exclusion` trx id 0 1329308430 lock_mode X locks rec but not gap waiting
  Record lock, heap no 32 PHYSICAL RECORD: n_fields 4; compact format; info bits 0
   0: len 20; hex 74756e6e656c2d3131313635362d313131363734; asc tunnel-111656-111674;; 1: len 6; hex 00004f3b851a; asc   O;  ;; 2: len 7; hex 8000018012012d; asc       -;; 3: len 4; hex 8114b8b2; asc     ;;
  
  *** WE ROLL BACK TRANSACTION (1)
  {noformat}
* DONE CM: Rate Limiting handles region incorrectly		    :PL_7162:
  :PROPERTIES:
  :assignee: evanc
  :reporter: ybranch
  :type:     Bug
  :priority: Critical
  :status:   Resolved
  :created:  2013-10-17 22:10:25
  :updated:  2013-10-24 17:34:17
  :ID:       PL-7162
  :resolution: Fixed
  :END:
** description: PL-7162
  I have no clue what to make of these errors....
  
  {code}
  2013-10-17 21:54:11 tuk6m1gb2 [err] [wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.test.skytap.com-0.22173.request.run_vms.673.greenbox]: [ERROR] errors.py:209 - message handler failed: {"incident_id": "503A01AA", "cause": {"incident_id": "503A01AA", "name": "run", "context": "wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.test.skytap.com-0.22173.request.run_vms.673/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.31540.run.8", "msg": "675.56525735294076", "timestamp": "2013-10-17 21:54:11", "type": "GlobalIsScaredError"}, "name": "request.run_vms", "context": "wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.test.skytap.com-0.22173.request.run_vms.673", "__severity": "error", "msg": "'{\"incident_id\": \"503A01AA\", \"name\": \"run\", \"context\": \"wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.test.skytap.com-0.22173.request.run_vms.673/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.31540.run.8\", \"__severity\": \"error\", \"msg\": \"675.56525735294076\", \"timestamp\": \"2013-10-17 21:54:11\", \"type\": \"GlobalIsScaredError\"}'", "timestamp": "2013-10-17 21:54:11", "type": "ConfigurationManagerInternalError"}
  {code}
** Comment: ybranch
   :PROPERTIES:
   :ID:       114708
   :created:  2013-10-17 22:13:22
   :END:
  Here's something with the same context
  {code}
  2013-10-17 21:54:11 tuk6m1cm1 [warning] [wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.tes
  t.skytap.com-0.22173.request.run_vms.673/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.31540.run.8.ratelimiter]: [WARNING] l
  imits.py:172 - Cannot load regional configuration: ConfigurationError: ("'No such environment configuration directory: /highland/configs/test
  /test/tuk6r1'", 'No such environment configuration directory: /highland/configs/test/test/tuk6r1'). 
  {code}
  
  
  The path it's trying is incorrect.
** Comment: ybranch
   :PROPERTIES:
   :ID:       114715
   :created:  2013-10-17 22:22:10
   :END:
  Bing, starting with you as the config path it tries is not correct.
** Comment: lonnieh
   :PROPERTIES:
   :ID:       114776
   :created:  2013-10-18 14:46:35
   :END:
  Bing, if it helps to play around in a multi-region environment, you can use lonnieh-cloud for that...
** Comment: bxiao
   :PROPERTIES:
   :ID:       114805
   :created:  2013-10-18 16:26:26
   :END:
  Investigating...
** Comment: bxiao
   :PROPERTIES:
   :ID:       114870
   :created:  2013-10-18 18:17:24
   :END:
  Is it possible for me to get full logs?
  
  Looks like the "region" of the vas are "test/tuk6r1". I think it's supposed to be "tuk6r1". Need to look at the full logs to get more context.
  
** Comment: bxiao
   :PROPERTIES:
   :ID:       114875
   :created:  2013-10-18 18:35:51
   :updated:  2013-10-18 18:36:14
   :END:
  Got full log. Looks like the "region" being passed around include the env name ('test/tuk6r1' vs 'tuk6r1'):
  
  
  2013-10-17 00:00:06 tuk6m1cm2 [debug] [/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.30764.configuration_manager]: [DEBUG] MQClientPika.py:251 - write_message accounting.test.default {"_mq_protocol_version": 1, "_timestamp": "2013-10-17T00:00:06.290144Z", "_reply_routing_key": "ConfigurationManager_synchronous_configuration_manager-tuk6m1cm2_mgt_test_skytap_com-1.test.default", "caller_context": null, "_unique_id": "req:tuk6m1cm2:ConfigurationManagerService:30764:2013-10-17_00-00-06:0", "_message_type": "request", "action": "manage_resources", "payload": {"reservations": {"resources": {"network-49804-66040": {"region": "test/tuk6r1", "resource_types": {"networks": 1}, "details": {}}, "vm-49804-80746": {"region": "test/tuk6r1", "resource_types": {"svms": 1}, "details": {"details": "{\"hosting_node\": \"c8b12.mgt.test.skytap.com\", \"ram\": 1024, \"cpus\": 1, \"cpu_type\": \"Type C\"}"}}}}, "charge_tag": "ctag-00bf99b018ed0131f323005056a30030"}}
  ----------------------------------------------------------------------
  

** Comment: andrus
   :PROPERTIES:
   :ID:       114876
   :created:  2013-10-18 18:38:31
   :updated:  2013-10-18 18:38:53
   :END:
  Sorry, never explained this to you.
  
  management regions are "plain" - like "test" or "prod"
  
  resource regions look like sub-directories - e.g. "test/tuk6r1" or "prod/slg1r1"
  
  So, your code here:
  {code}
  region = vms[0].region
  assert all(vm.region == region for vm in vms), "not all vms in the same region"
  env = os.path.join(config.HIGHLAND_ENV, region)
  {code}
  
  is working too hard...
  

** Comment: bxiao
   :PROPERTIES:
   :ID:       114892
   :created:  2013-10-18 19:21:24
   :END:
  Right.
  
  Talk to Lonnie, this is not Greenbox/messaging issue. So renamed the title.
  
  In the case where the file is not found (due to the wrong path issue), we fall back to default - /highland/configs/test/hosting_platform_conf.yaml", which has the following:
  
  
  rate_limiting:
     big_operation_cost:   30
     big_operation_mem:  4.0
     big_operation_disk: 6.8
     big_operation_host: 4.4
     big_operation_net:   9.6
     system_big_operation_count: 11
     max_operation_multiplier: 7.6
  
  
  
  2013-10-17 21:54:11 tuk6m1cm1 [debug] [wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.test.
  skytap.com-0.22173.request.run_vms.673/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.31540.run.8.ratelimiter]: [DEBUG] limit
  s.py:302 - ratelimiter costs: op run, storage 675.57, host 409.09, net 3.12
  
  
  2013-10-17 21:54:11 tuk6m1cm1 [info] [wfe.ae7572f0197401316ce8005056a3002c.415/trn.bd4343f0199d0131f328005056a30030.5.1@/tuk6m1gb2.mgt.test.skytap.com-0.22173.request.run_vms.673/qm-run-0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-0.31540.run.8.ratelimiter]: [INFO] limits.py:370 - ratelimiter GlobalIsScaredError: op run, max_cost 675.565257353, single_op_threshold 228.0, accounts [2L]
  
  
  
  mysql> select * from limits where account_id=2;
  +------------+---------------------+---------------------+---------------------+-------------+
  | account_id | storage_time        | host_time           | network_time        | region      |
  +------------+---------------------+---------------------+---------------------+-------------+
  |          2 | 2013-05-14 20:15:15 | 2013-05-14 20:15:11 | 2013-05-14 20:15:09 |             |
  |          2 | 2013-10-18 19:00:47 | 2013-10-18 19:00:44 | 2013-10-18 19:00:39 | test/tuk6r1 |
  |          2 | 2013-10-18 17:22:08 | 2013-10-18 17:22:08 | 2013-10-18 17:22:08 | test/tuk6r2 |
  +------------+---------------------+---------------------+---------------------+-------------+
  
  
  Because the max_cost(675.56) > single_op_threshold(228) we raise GlobalScaredError. So even if we fix the path this is still expected behavior.
  
** Comment: bxiao
   :PROPERTIES:
   :ID:       115030
   :created:  2013-10-21 00:15:09
   :END:
  Fix is in integ
** Comment: evanc
   :PROPERTIES:
   :ID:       115703
   :created:  2013-10-23 23:40:07
   :END:
  In Test:r49:hosting_platform:0f7c5fbfee20, we are seeing the following recorded in the log file:
  {noformat}
  2013-10-23 19:24:40 tuk6m1cm2 [warning] [wfe.01d3da201e45013181f4005056a30032.11/trn.7b00d0201e460131f372005056a30030.5.1@/greenbox-tuk6m1gb2.mgt.test.skytap.com-9-0.23872.request.run_vms.550/qm-run-0/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.18682.run.26.ratelimiter]: [WARNING] limits.py:172 - Cannot load regional configuration: ConfigurationError: ("'Configuration file not found for hosting_platform'", 'Configuration file not found for hosting_platform'). 
  {noformat}
  
  However, we have 'hosting_platform_conf.yaml' in each of our regional configuration directories, and these config files contain rate limiting entries.
** Comment: bxiao
   :PROPERTIES:
   :ID:       115740
   :created:  2013-10-24 04:15:46
   :END:
  Investigating
** Comment: bxiao
   :PROPERTIES:
   :ID:       115745
   :created:  2013-10-24 05:06:57
   :END:
  Pushed fix to integ
** Comment: bxiao
   :PROPERTIES:
   :ID:       115802
   :created:  2013-10-24 16:38:52
   :END:
  Looks like I pushed the fix to R50 instead of R49. Reactive until I move the change over
** Comment: bxiao
   :PROPERTIES:
   :ID:       115811
   :created:  2013-10-24 17:34:17
   :END:
  Now the fix is pushed to R49
* DONE Parameter validation errors in mq reply does not reveal useful information :PL_7198:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: bxiao
  :type:     Bug
  :priority: Critical
  :resolution: Unfortunate
  :status:   Resolved
  :components: common
  :created:  2013-10-18 22:50:02
  :updated:  2013-10-20 20:17:58
  :ID:       PL-7198
  :END:
** description: PL-7198
  Here is an example error:
  
  2013-10-17 20:56:12 tuk6m1gb2 [debug] [wfe.ab0026401974013181b1005056a30032.151.1@/tuk6m1gb2.mgt.test.skytap.com-0.22078.request.create_network.586.greenbox]: [DEBUG] errors.py:209 - Error processing message: MQRequestMessage(action:create_network, unique_id:tuk6m1wfe4:req:7a8642d0199c013181b1005056a30032:wfe.ab0026401974013181b1005056a30032.151.1:316): {"incident_id": "40E0D375", "name": "request.create_network", "context": "wfe.ab0026401974013181b1005056a30032.151.1@/tuk6m1gb2.mgt.test.skytap.com-0.22078.request.create_network.586", "__severity": "error", "msg": "\"None is not of type 'string'\"", "timestamp": "2013-10-17 20:56:12", "type": "InvalidParameterError", "resources": []}
  
  This error gives a message "None is not of type 'string'" but does not tell which part in the request that causes this.
  
  The message should include details from json schema validator, as is below:
  
  
  ValidationError: None is not of type 'string'
  
  Failed validating 'type' in schema['properties']['network_attributes']['properties']['primary_nameserver']:
      {'type': 'string'}
  
  On instance['network_attributes']['primary_nameserver']:
      None
** Comment: mpietrek
   :PROPERTIES:
   :ID:       115016
   :created:  2013-10-20 20:17:46
   :END:
  The problem here is the use of "oneOf". Because replies can be either "result" or "error", we have to have schema for both of the possible results. If there was only one schema, jsonschema validation could point to the exact problem.
  
  I have a tool checked in (.../api/tools/jsonschema_errorhelper.py) which is trivially modified to include the payload data and the target schema. It then makes a best effort to give you more useful information.
* TODO add ppa for emacs-24 to appdev nodetype			    :PL_6701:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: andrus
  :type:     Improvement
  :priority: Major
  :status:   In Progress
  :components: skytap-support
  :created:  2013-08-12 16:46:47
  :updated:  2013-09-03 19:44:57
  :ID:       PL-6701
  :END:
** description: PL-6701
** Comment: bxiao
   :PROPERTIES:
   :ID:       108906
   :created:  2013-08-12 17:09:01
   :END:
  The emacs 24 for Ubuntu I found is not the official release, but a stable snapshot. It can be installed side by side with emacs 23.
  
  With emacs 24 you can install Jedi extension (http://tkf.github.io/emacs-jedi/) for Python. It has much improved support over emacs basic Python mode.
  
  To install emacs 24 on Ubuntu 10.04:
  
  
  sudo add-apt-repository ppa:cassou/emacs
  sudo apt-get update
  sudo apt-get install emacs-snapshot-el emacs-snapshot-gtk emacs-snapshot
  
** Comment: andrus
   :PROPERTIES:
   :ID:       108923
   :created:  2013-08-12 20:22:03
   :END:
  Jeremy - can you show bing how to do this in puppet, then assign it to him to actually do? 
** Comment: jlingmann
   :PROPERTIES:
   :ID:       108929
   :created:  2013-08-12 20:36:32
   :END:
  Sure, no problem.  Bing, do you have some time this afternoon to chat?
* TODO rate limiting should differentiate customer from user	    :PL_6744:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: andrus
  :type:     Improvement
  :priority: Major
  :status:   Open
  :components: cm
  :created:  2013-08-19 17:50:48
  :updated:  2013-08-21 20:01:32
  :ID:       PL-6744
  :END:
** description: PL-6744
  So one user can't slam the entire customer.
** Comment: andrus
   :PROPERTIES:
   :ID:       109814
   :created:  2013-08-21 20:01:13
   :END:
  Presenting case:
  
  * A single customer with multiple users, including one primarily using the API. The goal is to be able to have the API usage *not* clobber the "real" users. The suggested approach is to have customer-scope rate limiting at some multiple of user-scope rate limiting (like 3 or 5).
  
  

* TODO Finish two-phase CM migration (first part in R49)	    :PL_6901:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: bxiao
  :type:     Task
  :priority: Major
  :status:   Open
  :components: cm
  :created:  2013-09-10 16:17:35
  :updated:  2013-10-18 19:28:55
  :ID:       PL-6901
  :END:
** description: PL-6901
  In Configuration database
  Make the new 'name' column of accounts table non-nullable
  
  The new region column of limits table has default empty string. Remove this default.
** Comment: bxiao
   :PROPERTIES:
   :ID:       114898
   :created:  2013-10-18 19:28:55
   :END:
  Non-nullable and default to empty is done in R49.
  
  In R50 we will cleanup old record with empty region.
* TODO nhn failure leaves nic disconnected, requires manual reconnect :PL_6931:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: andrus
  :type:     Improvement
  :priority: Major
  :status:   Open
  :components: cm, hosting
  :created:  2013-09-12 19:54:25
  :updated:  2013-09-25 19:06:53
  :ID:       PL-6931
  :END:
** description: PL-6931
  Encountered during outage-115.
  
  Here's a fragment of the error log (this is vm-746838-1893656):
  
  {code}
  2013-09-11 15:24:25 tuk1m1cm4 [err] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.service]: [ERROR] errors.py:209 - service invocation failure for network_server.metadatahost_create(nic-746838-1893656-0-metadatahost,network-746838-728466-metadataserver,192.168.0.5,vm-746838-1893656)
  : {"incident_id": "4276904F", "cause": {"incident_id": "162C2DC8", "type": "UnknownNetworkError"}, "name": "run", "context": "wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294", "__severity": "error", "msg": "\"ResourceParentStateInvalid(u'resource=(metadatahost:6473560:6441556:(nic-746838-1893656-0-metadatahost:network-746838-728466-metadataserver):[configuration-746838]:[UNDEPLOYED:(vgr:UNDEPLOYED)](locked)), parent_resource=(metadataserver:6441556:6441546:(network-746838-728466-metadataserver:network-746838-728466):[configuration-746838]:[DEPLOYED:(vgr:UNDEPLOYED)](unlocked)): ',)\"", "timestamp": "2013-09-11 15:24:25", "type": "InternalNetworkingError"}
  2013-09-11 15:24:25 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [DEBUG] unprovision.py:741 - unprovisioning nic-746838-1893656-0-metadatahost for nic-746838-1893656-0
  2013-09-11 15:24:25 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.call_node_trace]: [DEBUG] CallNode.py:96 - 110174672: calling tuk1r1nsvip1:9002.metadatahost_destroy(('nic-746838-1893656-0-metadatahost',)) caller_options: {'samba_fileserver': False, 'prefer_kvm': False}
  2013-09-11 15:24:25 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.call_node_trace]: [DEBUG] CallNode.py:152 - 110174672: returning None
  2013-09-11 15:24:25 tuk1m1cm4 [err] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [ERROR] errors.py:209 - failure attaching nic-746838-1893656-0, it will not have network services or connectivity: {"incident_id": "4276904F", "cause": {"incident_id": "162C2DC8", "type": "UnknownNetworkError"}, "name": "run", "context": "wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294", "__severity": "error", "msg": "\"ResourceParentStateInvalid(u'resource=(metadatahost:6473560:6441556:(nic-746838-1893656-0-metadatahost:network-746838-728466-metadataserver):[configuration-746838]:[UNDEPLOYED:(vgr:UNDEPLOYED)](locked)), parent_resource=(metadataserver:6441556:6441546:(network-746838-728466-metadataserver:network-746838-728466):[configuration-746838]:[DEPLOYED:(vgr:UNDEPLOYED)](unlocked)): ',)\"", "timestamp": "2013-09-11 15:24:25", "type": "InternalNetworkingError"}
  2013-09-11 15:24:25 tuk1m1cm4 [info] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [INFO] unprovision.py:198 - cleaning up orphaned vpns: vpn-129390,vpn-195962,vpn-542770,vpn-661130,vpn-661182,vpn-670476,vpn-711360
  2013-09-11 15:24:25 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [DEBUG] unprovision.py:204 - UnprovisionNetworksOperation: configuration-746838 {} ['vpn-542770', 'vpn-129390', 'vpn-661182', 'vpn-670476', 'vpn-195962', 'vpn-661130', 'vpn-711360']
  2013-09-11 15:24:29 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [DEBUG] vpns.py:697 - UnprovisionVPNsOperation: set([vpn-195962, vpn-670476, vpn-661182, vpn-542770, vpn-129390, vpn-661130])
  2013-09-11 15:24:29 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [DEBUG] network_provisioning.py:303 - unprovision_stale_nat_entries: []
  2013-09-11 15:24:29 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.configuration_manager]: [DEBUG] network_provisioning.py:303 - unprovision_stale_nat_entries: []
  2013-09-11 15:24:29 tuk1m1cm4 [debug] [wfe.bb52ebb0fd1e0130e680005056a30025.522/trn.1fe478b0fd23013031c474ce4686f1c0.7.1/configuration_manager-tuk1m1cm4-1.1412.run.5294.api]: [DEBUG] ConfigurationManager.py:64 - run ASYNC_END 155.67
  {code}
  
  That is, cm couldn't correctly provision the nic, so it was left disconnected, and it entailed a manual stop/start of the VM to resolve it.
  
  NOTE: a disconnect/reconnect of the NIC would have done as well, but I didn't think of it at the time.
  
  I also didn't act quickly enough to catch it then, but I *presume* the NIC would have had an error attached to it, which could be automatically detected and, possibly, resolved.
** Comment: andrus
   :PROPERTIES:
   :ID:       111935
   :created:  2013-09-16 17:04:05
   :END:
  I expect this will entail changes to vmrunstateservice to scan for "busted" NICs and Do Something Responsible. Might also require a change to CM APIs.
* TODO Remove vend plumbing from CM for shared drives that use samba service :PL_6939:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: ybranch
  :type:     Task
  :priority: Major
  :status:   Open
  :components: cm
  :created:  2013-09-13 17:14:41
  :updated:  2013-09-16 15:27:07
  :ID:       PL-6939
  :END:
** description: PL-6939
  *If* we go full samba service in R49, these are not needed. 
** Comment: andrus
   :PROPERTIES:
   :ID:       111825
   :created:  2013-09-13 17:25:04
   :END:
  Apologies, but IDK the plans for phased deployment of samba. Are we running samba at all in production? 
  
  If not, I'd prefer to leave the vends in CM until we've got some experience with samba, and have run with it full-throttle-up for at least a release.
  
  So... r50? 
  
  
  
  

* TODO cmcmd fails ugly in resource region			    :PL_7158:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: andrus
  :type:     Bug
  :priority: Major
  :status:   In Progress
  :components: cm
  :created:  2013-10-17 21:07:27
  :updated:  2013-10-21 21:14:05
  :ID:       PL-7158
  :END:
** description: PL-7158
  {code}
  ---------------------------------------------------------------------------
  AttributeError                            Traceback (most recent call last)
  
  /highland/hosting_platform/hosting_platform/services/configuration_manager/tools/cmcmd.py in <module>()
      188     cmcmd = InterpreterProxy(ConfigurationManagerCommand())
      189     ConfigurationManagerCommand().execute_cmdline()  # in interpreter, this will print command help
  --> 190     debug_context("cmcmd by %s" % getuser())
      191
      192     def register_service(region, service_type, host, port, service_instance_id=''):
  
  /highland/hosting_platform/hosting_platform/services/configuration_manager/Context.py in debug_context(context_name)
       74         current_context.leave()
       75     else:
  ---> 76         db = ConfigurationDB.getInstance()
       77     return Context(context_name, "debug_instance", db=db,
       78                    mq=MQMessageProcessor(ConfigurationManagerAPI.SERVICE_NAME,
  
  /highland/hosting_platform/hosting_platform/common/singletonmixin.py in getInstance(cls, *lstArgs, **dctKwArgs)
      204                 raise SingletonException, 'Singleton already instantiated, but getInstance() called with args.'
      205         else:
  --> 206             _createSingletonInstance(cls, lstArgs, dctKwArgs)
      207
      208         return cls.cInstance
  
  /highland/hosting_platform/hosting_platform/common/singletonmixin.py in _createSingletonInstance(cls, lstArgs, dctKwArgs)
      136         instance = cls.__new__(cls)
      137         try:
  --> 138             instance.__init__(*lstArgs, **dctKwArgs)
      139         except TypeError, e:
      140             if "".join(map(str, e.args)).find('__init__() takes') != -1:
  
  /highland/hosting_platform/hosting_platform/services/configuration_manager/db.py in __init__(self, url, options)
      153     def __init__(self, url=None, options=None):
      154         db_config = config.DATABASES.get("configuration_db")
  --> 155         options = options or db_config.get('options')
      156
      157         super(ConfigurationDB, self).__init__("configuration", url=url, options=options)
  
  AttributeError: 'NoneType' object has no attribute 'get'
  WARNING: Failure executing file: </highland/hosting_platform/hosting_platform/services/configuration_manager/tools/cmcmd.py>
  Python 2.6.5 (r265:79063, Oct  1 2012, 22:04:36)
  Type "copyright", "credits" or "license" for more information.
  
  IPython 0.10 -- An enhanced Interactive Python.
  ?         -> Introduction and overview of IPython's features.
  %quickref -> Quick reference.
  help      -> Python's own help system.
  object?   -> Details about 'object'. ?object also works, ?? prints more.
  {code}
** Comment: bxiao
   :PROPERTIES:
   :ID:       115185
   :created:  2013-10-21 21:02:39
   :END:
  I have trouble repro the issue.
  
  Using ccmcmd to register CM on my dev node using both management region and resource region works fine.
  
** Comment: bxiao
  Test update from emacs
** Comment: bxiao
   :PROPERTIES:
   :ID:       115189
   :created:  2013-10-21 21:14:05
   :END:
  Test update from emacs
* TODO cm allows manual network connection to NAT vpn -> errors happen :PL_6839:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: pholland
  :type:     Bug
  :priority: Critical
  :status:   In Progress
  :components: cm
  :created:  2013-09-03 16:06:34
  :updated:  2013-10-21 20:59:41
  :ID:       PL-6839
  :END:
** description: PL-6839
  the conflict checking works!
  
  {noformat}
  2013-08-30 21:34:23 tuk1m1cm1 [err] [wfe.c8d59640f3d9013007ac005056a30055.901/trn.66089c30f3e9013031b074ce4686f1c0.105.1/configuration_manager-tuk1m1cm1-2.20268.run.7686.service]: [ERROR] errors.py:209 - service invocation failure for network_server.tunnel_nat_create(network-729538-766408-vpn-542770-nic-729538-1900624-0-network,network-729538-766408-vpn-542770-network-tunnel,10.1.0.100,10.0.0.1)
  --stack--
    File "/usr/lib/python2.6/threading.py", line 504, in __bootstrap
      self.__bootstrap_inner()
    File "/usr/lib/python2.6/threading.py", line 532, in __bootstrap_inner
      self.run()
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/threadutils.py", line 87, in run
      self.return_value = self.operation(*self.args, **self.kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 1515, in _execute
      self.provision_vpn_network(self.vpn_attachment)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 62, in wrapper
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 383, in provision_vpn_network
      tunnel.provision_routes(network_server)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/model/vpns.py", line 720, in provision_routes
      entry.address.dotted_quad, entry.nic.address.dotted_quad)
  --traceback--
  Traceback (most recent call last):
    File "/highland/hosting_platform/hosting_platform/common/errors/errors.py", line 325, in _service_error_handler
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/rpc/CallNode.py", line 128, in __call__
      raise e
  ProtocolError: (901, 'ParameterValueInvalid("param(local_ip), value(10.0.0.1): must be a valid host within network subnet \'10.1.5.0/24\'",)')
  : {"incident_id": "595F40E9", "cause": {"incident_id": "742BF49C", "type": "UnknownNetworkError"}, "name": "run", "context": "wfe.c8d59640f3d9013007ac005056a30055.901/trn.66089c30f3e9013031b074ce4686f1c0.105.1/configuration_manager-tuk1m1cm1-2.20268.run.7686", "__severity": "error", "msg": "'ParameterValueInvalid(\"param(local_ip), value(10.0.0.1): must be a valid host within network subnet \\'10.1.5.0/24\\'\",)'", "timestamp": "2013-08-30 21:34:23", "type": "InternalNetworkingError"}
  {noformat}
** Comment: andrus
   :PROPERTIES:
   :ID:       110896
   :created:  2013-09-03 17:01:03
   :updated:  2013-09-03 21:04:09
   :END:
  Here is the run that spawned this:
  
  
  {code}
  2013-08-30 21:34:00 tuk1m1cm1 [debug] [wfe.c8d59640f3d9013007ac005056a30055.901/trn.66089c30f3e9013031b074ce4686f1c0.105.1/configuration_manager-tuk1m1cm1-2.20268.run.7686.api]: [DEBUG] ConfigurationManager.py:43 - run CALLED (['account-157988', 'account-140372'], ['vm-729538-1903524'], 'configuration-729538') caller_options: {'samba_fileserver': False, 'prefer_kvm': False}
  {code}
  
  {code}
  configuration-729538 https://cloud.skytap.com/configurations/1038896
    network-729538-766408 - 10.1.5.0/24 - manual
    network-729538-708988 - 10.1.15.0/24 - automatic 
  {code}
** Comment: andrus
   :PROPERTIES:
   :ID:       111002
   :created:  2013-09-03 23:22:04
   :END:
  Beyond the specifics of the IP address we're trying to use, a bigger issue is that we've ended up with a manual network attached to a NAT VPN, which just won't work.
  

** Comment: andrus
   :PROPERTIES:
   :ID:       111003
   :created:  2013-09-03 23:23:28
   :END:
  Note that the VM being run is different than the VM mentioned in the error watch. 
  
  Notice also there are several failures like this in the logs today.
  
  I believe the run is triggering a general provisioning phase which keeps finding 'work to be done', attempting it, and failing.
  

** Comment: andrus
   :PROPERTIES:
   :ID:       111004
   :created:  2013-09-03 23:33:36
   :END:
  Support is already tracking with customer - advised them that this will *never* work and the best we can hope for is to disallow this misconfiguration in future.
  

** Comment: bschick
   :PROPERTIES:
   :ID:       111909
   :created:  2013-09-16 06:22:09
   :END:
  Let's make sure we get manual test cases added as well.
** Comment: andrus
   :PROPERTIES:
   :ID:       111924
   :created:  2013-09-16 13:26:46
   :END:
  By "manual test cases added" do you mean "add to QAIT BVT"?
  
** Comment: bschick
   :PROPERTIES:
   :ID:       112021
   :created:  2013-09-17 05:48:05
   :END:
  Yes
* TODO update CM systems tests to new multi-services model	    :PL_7232:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: andrus
  :type:     Bug
  :priority: Major
  :status:   Open
  :components: common
  :created:  2013-10-21 18:13:54
  :updated:  2013-10-22 16:45:30
  :ID:       PL-7232
  :END:
** description: PL-7232
  It's become clear over the last weeks that our integration efforts could be improved by more targeted testing. The most likely approach will be to "embrace and extend" the old CM-centric system tests, specifically to widen the scope to include more of the key platform services.
  
  I think it makes sense to *copy* the existing tests from CM/system_tests to hosting_platform/system_tests (i.e. peered with services) and start building them out to drive operations across the platform. 
  
  I think the order will be accounting, then GB/CM, then GB/awsdriver.
** Comment: mpietrek
   :PROPERTIES:
   :ID:       115127
   :created:  2013-10-21 18:17:38
   :END:
  Here is a diff of some code I wrote that made the existing CM system tests call through MQ to Greenbox instead. Happy to answer any questions:
  
  {code}
  -bash-3.2$ cat xmlrpc 
  # HG changeset patch
  # Parent ec474c96382e4548c399a91a8ee6fb3f36af9124
  
  diff -r ec474c96382e hosting_platform/services/configuration_manager/system_tests/base.py
  --- a/hosting_platform/services/configuration_manager/system_tests/base.py	Mon Jun 24 16:25:23 2013 +0000
  +++ b/hosting_platform/services/configuration_manager/system_tests/base.py	Tue Jun 25 17:04:28 2013 +0000
  @@ -32,6 +32,10 @@
   from hosting_platform.common.config import get_configuration
   from hosting_platform.services.framework import constant as service_constants
   from hosting_platform.services.ftp_server import FTPControlMixin
  +from xmlrpc_to_mq_adapter import XmlRpcToMQAdapter
  +from hosting_platform.services.configuration_manager.operations.StorageHelper import VendedDepot
  +import uuid
  +from hosting_platform.services.configuration_manager.model import Registrar
   
   API_RETRY_LOGGER = initialize_logger("api_retry")
   
  @@ -119,6 +123,17 @@
           setattr(self, attr, _attr)  # cache for later
           return _attr
   
  +class VMKeyWithAnnotations(str):
  +    """
  +    A small hack to bind the configuration instance to a vm_key. We need the configuration
  +    to get at the region, which is needed to mount the vm's depot. Lonnie and mpietrek
  +    cooked this up. Blame them.
  +    """
  +    def __new__(cls, s, configuration):
  +        obj = str.__new__(cls, s)
  +        obj.configuration = configuration
  +        return obj
  +
   class SystemTestBase(unittest.TestCase, FTPControlMixin):
       """
       Base class for system tests.
  @@ -148,7 +163,9 @@
           host = self.config.get('configuration_manager', {}).get('host', 'localhost')
           port = self.config.get('configuration_manager', {}).get('port', 9000)
   
  -        self.api = Service(ServiceConstants.CONFIGURATION_MANAGER, config.HIGHLAND_ENV, host, None, port)()
  +        # self.api = Service(ServiceConstants.CONFIGURATION_MANAGER, config.HIGHLAND_ENV, host, None, port)()
  +        self.api = XmlRpcToMQAdapter(ServiceConstants.CONFIGURATION_MANAGER, 'whatever', host, port)
  +
           if self.options.verbose_level:
               self.api = VerboseCalls(self, self.api)
   
  @@ -374,8 +391,24 @@
       def mounted_session_depot(self, vm_key):
           """mount session depot in a context"""
   
  -        depot_guid = self.api.get_vm_provisioning_details(vm_key)[vm_key]['depot_guid']
  +        provisioning_details = self.api.get_vm_provisioning_details(vm_key)[vm_key]
  +        depot_guid = provisioning_details['depot_guid']
   
  +        vend_token = 'my_vend_token_%s' % (uuid.uuid4())
  +
  +        storage_service = Registrar.lookup_storage_service(vm_key.configuration.region)
  +        try:
  +            sdepot = storage_service.vend_world(vend_token, depot_guid)
  +        except Exception as e:
  +            print e
  +            raise e
  +
  +        vended_depot = VendedDepot(vend_token, sdepot)
  +
  +        with vended_depot.mounted_depot_context() as mount_path:
  +            yield mount_path
  +
  +        """
           runCmd("skytap-cmd strgcmd mount %s" % depot_guid)
           vend_token, _ = runCmd("skytap-cmd strgcmd get_vends %s | tail -1 | cut -d' ' -f1" % depot_guid)
           vend_token = vend_token.split('[')[-1].strip()
  @@ -385,6 +418,7 @@
           finally:
               runCmd("skytap-cmd strgcmd unmount %s" % depot_guid)
               runCmd("skytap-cmd strgcmd release %s" % vend_token)
  +        """
   
       def add_vm(self, accounts, configuration, vm_template, cpus=1, ram=1024, force_populate=False):
           """add the vm in the vm_template to configuration"""
  @@ -412,7 +446,7 @@
           if force_populate:
               # RLA HACK - force the depot to really be cloned here by doing something that will vend it - for
               # now, use strgcmd to mount it
  -            with mounted_session_depot(vm_key):
  +            with self.mounted_session_depot(vm_key):
                   pass
   
               # LH - I'd like to do the set_vm_disks in parallel and poll_for_statuses at the end, but merge operation
  @@ -421,7 +455,7 @@
               self.poll_for_statuses("vm", configuration.vms.keys(), [VM.STATUSES.POWERED_OFF])
   
   
  -        return vm_key
  +        return VMKeyWithAnnotations(vm_key, configuration)
   
   
       def set_session_depot_attrs(self, vm_key, user_name, group_name, permissions, recurse=False):
  diff -r ec474c96382e hosting_platform/services/configuration_manager/system_tests/xmlrpc_to_mq_adapter.py
  --- /dev/null	Thu Jan 01 00:00:00 1970 +0000
  +++ b/hosting_platform/services/configuration_manager/system_tests/xmlrpc_to_mq_adapter.py	Tue Jun 25 17:04:28 2013 +0000
  @@ -0,0 +1,117 @@
  +'''
  +Created on Jun 13, 2013
  +
  +@author: mpietrek
  +'''
  +from hosting_platform.rpc import CallNodeProxy
  +from hosting_platform.common.mq_framework import MQMessageProcessor
  +from hosting_platform.common.logs import initialize_logger
  +from hosting_platform.common.errors import Error
  +
  +_LOGGER = initialize_logger("system_tests/xmlrpc_to_mq_adapter")
  +
  +MQ = MQMessageProcessor('unit_test', _LOGGER, None, None, 'xmlrpc_to_mq-1')
  +
  +CM_API_DESCRIPTOR = {}
  +
  +class InvokeMQ(object):
  +    def __init__(self, api_name):
  +        self.api_name = api_name
  +
  +    def __call__(self, *args, **kwargs):
  +
  +        cm_api = CM_API_DESCRIPTOR[self.api_name]
  +        gb_payload = {}
  +
  +        if len(kwargs):
  +            raise Exception('kwargs not yet supported')
  +
  +        # Iterate over all the arguments passed to us
  +        for i in range(0, len(args)):
  +            arg_name, _ = cm_api['arguments'][i]
  +            gb_arg_name, gb_arg_value = self.translate_argument(arg_name, args[i])
  +
  +            gb_payload[gb_arg_name] = gb_arg_value
  +
  +        print "XML_RPC call: %s %s" % (cm_api['greenbox_name'], gb_payload)
  +        reply_message = MQ.wait_for_reply_message('greenbox', cm_api['greenbox_name'], gb_payload)
  +        print "XML_RPC reply: %s %s" % (cm_api['greenbox_name'], reply_message.payload)
  +
  +        result = self.translate_reply(reply_message)
  +        print "XML_RPC returns: %s %s" % (cm_api['greenbox_name'], result)
  +
  +        return result
  +
  +    def translate_argument(self, arg_name, arg_value):
  +        if arg_name == 'source_config_key':  # Misnamed in CM API
  +            arg_name = 'source_configuration_key'
  +        if '_key_list' in arg_name:  # Changing param names relative to CM
  +            arg_name = arg_name.replace('_key_list', '_keys')
  +        if arg_name.startswith('account_depot_key'):  # Changing param names relative to CM
  +            arg_name = arg_name.replace('account_depot_key', 'depot_key')
  +        if arg_name.endswith('_key_versions'):  # We got rid of key_versions in CM - Replace with list of keys
  +            arg_name = arg_name.replace('_key_versions', '_keys')
  +            arg_value = arg_value.keys()
  +
  +        return arg_name, arg_value
  +
  +    def translate_reply(self, reply):
  +        if 'error' in reply:
  +            error = Error(reply.error['_original_exception'])
  +            print "XML_RPC raising %s" % (error)
  +            raise error.as_xmlrpclib_fault
  +
  +        result = reply['result']
  +
  +        if self.api_name.endswith('_specification'):
  +            for key, value in result.iteritems():
  +                result[key] = (value, 0)
  +            return result
  +        elif self.api_name == 'merge':
  +            return (result['target_configuration_key'], result['vm_mapping'], result['network_mapping'])
  +
  +        return reply['result']
  +
  +def create_call_node_function(_type, name, host, port, function, **kwargs):
  +    return InvokeMQ(name, host, port, function, **kwargs).__call__
  +
  +class XmlRpcToMQAdapter(CallNodeProxy):
  +    """
  +    def __init__(self, _type, region, host, instance_id, port, **kwargs):
  +        super(XmlRpcToMQAdapter, self).__init__(_type, host, port, **kwargs)
  +    """
  +
  +    def __getattr__(self, function, *args, **kwargs):
  +            if function == 'create_disk_policy':
  +                return super(XmlRpcToMQAdapter, self).__getattr__(function, *args, **kwargs)
  +            else:
  +                return InvokeMQ(function).__call__
  +
  +#===================================================================
  +from hosting_platform.services.configuration_manager.operations import api_operations as cm_api_operations
  +
  +RENAMED_APIS = \
  +{
  +'run': 'run_vms',
  +'shutdown': 'shutdown_vms',
  +'poweroff': 'poweroff_vms',
  +'suspend': 'suspend_vms',
  +'resize_desktop': 'resize_vm_desktop',
  +'get_export_ephemera': 'get_export_vm_ephemera',
  +'get_import_ephemera': 'get_import_vm_ephemera',
  +'delete_import': 'delete_import_vm',
  +'delete_export': 'delete_export_vm',
  +'attach_iso': 'attach_vm_iso',
  +}
  +
  +for cm_operation in cm_api_operations:
  +    if not hasattr(cm_operation, 'api_name'):
  +        continue
  +    if not hasattr(cm_operation, '__init__'):
  +        continue
  +
  +    cm_api_name = cm_operation.api_name()
  +    greenbox_name = cm_api_name if cm_api_name not in RENAMED_APIS else RENAMED_APIS[cm_api_name]
  +
  +    CM_API_DESCRIPTOR[cm_api_name] = {'greenbox_name': greenbox_name,
  +                                      'arguments': cm_operation.__init__.__parameter_validation_decls}
  {code}
  
** Comment: bxiao
   :PROPERTIES:
   :ID:       115324
   :created:  2013-10-22 16:45:30
   :END:
  Thanks Matt. I am working on loonieh cloud now. I saw many people has their own environment there, wonder whether I should do the same or just use lonnieh
* DONE Error Watch: "list index out of range" error unprovisioning networks during VM poweroff :PL_7270:
  :PROPERTIES:
  :assignee: evanc
  :reporter: evanc
  :type:     Bug
  :priority: Blocker
  :status:   Resolved
  :created:  2013-10-22 20:16:24
  :updated:  2013-10-24 03:18:46
  :ID:       PL-7270
  :resolution: Fixed
  :END:
** description: PL-7270
  {noformat}
  Sessions and configurations affected:
  
  configuration-615590
  ---------------------------------------------------------
  
  2013-10-22 19:55:47 tuk6m1cm2 [err] [wfe.aa0978701d81013181dc005056a30032.11/trn.f22e11f01d800131f358005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-5-0.17402.request.poweroff_vms.2082/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.15830.poweroff.40.configuration_manager]: [ERROR] errors.py:209 - failure unprovisioning network(s) for configuration-615590: {"incident_id": "753EFAE3", "name": "poweroff", "timestamp": "2013-10-22 19:55:47", "_original_exception": "IndexError('list index out of range',)", "__severity": "error", "context": "wfe.aa0978701d81013181dc005056a30032.11/trn.f22e11f01d800131f358005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-5-0.17402.request.poweroff_vms.2082/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.15830.poweroff.40", "msg": "'list index out of range'", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 228, in provision
      inactive_busy_networks=[n.key for n in self.networks.iterkeys()]).execute()
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 699, in __init__
      vpn.vpn_router_unprovision = vpn.router and len(active_networks) == 0
    File "/usr/lib/python2.6/contextlib.py", line 34, in __exit__
      self.gen.throw(type, value, traceback)
    File "/highland/hosting_platform/hosting_platform/db/db.py", line 106, in _run_transaction
      yield sess
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 638, in __init__
      vpn = self.load_vpn(vpn_key, tx, must_exist=False, for_update=True)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/vpns.py", line 292, in load_vpn
      return VPNProvisioningMixin.load_vpns([vpn_key], *args, **kwargs)[0]
  ----------------------------------------------------------------------
  2013-10-22 19:55:47 tuk6m1cm2 [err] [wfe.aa0978701d81013181dc005056a30032.11/trn.f22e11f01d800131f358005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-5-0.17402.request.poweroff_vms.2082/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.15830.poweroff.40.configuration_manager]: [ERROR] threadutils.py:95 - Error executing asynchronous operation asynch-op <function execute at 0x4e18c08>(, {}) : InternalError: ("'failure unprovisioning networks: list index out of range'", 'failure unprovisioning networks: list index out of range') at Traceback (most recent call last):
    File "/highland/hosting_platform/hosting_platform/common/threadutils.py", line 87, in run
      self.return_value = self.operation(*self.args, **self.kwargs)
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 62, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 80, in execute
      self.provision()  # pylint: disable=E1101
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 627, in provision
      logger=self.logger, chained=self.chained, cleanup=self.cleanup).execute()
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 80, in execute
      self.provision()  # pylint: disable=E1101
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 241, in provision
      raise InternalError("failure unprovisioning networks: " + ",".join(map(str, e.args)))
  InternalError: InternalError: ("'failure unprovisioning networks: list index out of range'", 'failure unprovisioning networks: list index out of range')
  ----------------------------------------------------------------------
  {noformat}
** Comment: andrus
   :PROPERTIES:
   :ID:       115388
   :created:  2013-10-22 20:40:22
   :END:
  Bing - can you take a look at this issue? I see "VPN" in the stack and think of you ;)
** Comment: evanc
   :PROPERTIES:
   :ID:       115391
   :created:  2013-10-22 20:43:06
   :END:
  The config spec:
  {noformat}
  In [3]: api.get_configuration_specification({'configuration-615590':None})
  Out[3]: 
  {'configuration-615590': [{'account_keys': ['account-19866'],
     'disable_internet': False,
     'key': 'configuration-615590',
     'networks': [{'cidr_block': '192.168.4.0/24',
       'configuration_key': 'configuration-615590',
       'domain_name': 'test.net',
       'gateway': '192.168.4.254',
       'key': 'network-615590-454516',
       'nat_cidr': None,
       'nat_pool': None,
       'network_type': 'automatic',
       'primary_nameserver': None,
       'secondary_nameserver': None,
       'tunnels': [],
       'vpn_attachments': []},
      {'cidr_block': '192.168.3.0/24',
       'configuration_key': 'configuration-615590',
       'domain_name': 'test.net',
       'gateway': '192.168.3.254',
       'key': 'network-615590-454518',
       'nat_cidr': None,
       'nat_pool': None,
       'network_type': 'automatic',
       'primary_nameserver': None,
       'secondary_nameserver': None,
       'tunnels': [],
       'vpn_attachments': []},
      {'cidr_block': '192.168.2.0/24',
       'configuration_key': 'configuration-615590',
       'domain_name': 'test.net',
       'gateway': '192.168.2.254',
       'key': 'network-615590-454520',
       'nat_cidr': None,
       'nat_pool': None,
       'network_type': 'automatic',
       'primary_nameserver': None,
       'secondary_nameserver': None,
       'tunnels': [],
       'vpn_attachments': []},
      {'cidr_block': '192.168.1.0/24',
       'configuration_key': 'configuration-615590',
       'domain_name': 'example.com',
       'gateway': '192.168.1.254',
       'key': 'network-615590-454522',
       'nat_cidr': None,
       'nat_pool': None,
       'network_type': 'automatic',
       'primary_nameserver': None,
       'secondary_nameserver': None,
       'tunnels': [],
       'vpn_attachments': []}],
     'region': 'test/tuk6r1',
     'routable': False,
     'runnable': True,
     'vms': [{'hardware': {'cpus': '1',
        'disk_controllers': [{'bus_type': 'ide',
          'controller': '0',
          'disks': [{'device_type': 'cdrom',
            'key': 'disk-615590-4140420-ide-0-0',
            'lun': '0',
            'size': '0'}],
          'key': 'disk_controller-615590-4140420-ide-0',
          'virtual_dev': None},
         {'bus_type': 'scsi',
          'controller': '0',
          'disks': [{'device_type': 'disk',
            'key': 'disk-615590-4140420-scsi-0-0',
            'lun': '0',
            'size': '2048'}],
          'key': 'disk_controller-615590-4140420-scsi-0',
          'virtual_dev': 'lsilogic'}],
        'disk_policy': {'disk_limit': '15',
         'disk_size_limit': '2096128',
         'iops_limit': '275',
         'key': 'disk_policy-100',
         'offset': '165',
         'size_limit': '2097152'},
        'floppy': 'fd0',
        'guestOS': 'otherlinux-64',
        'guest_cpu_type': '4',
        'hardware_policy': {'max_cpus': '8',
         'max_ram': '32768',
         'min_cpus': '1',
         'min_ram': '256'},
        'hardware_version': '7',
        'host_cpu_type': None,
        'nics': [{'address': '192.168.1.1',
          'configuration_key': 'configuration-615590',
          'hostname': 'host1',
          'key': 'nic-615590-4140420-0',
          'mac': '00:50:56:2e:10:ca',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454522',
          'nic_type': 'e1000',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': 'e1000',
          'vm_key': 'vm-615590-4140420'}],
        'ram': '1024',
        'uuid': None,
        'vnc_keymap': None},
       'key': 'vm-615590-4140420',
       'vm_extra_options_groups': []},
      {'hardware': {'cpus': '1',
        'disk_controllers': [{'bus_type': 'ide',
          'controller': '0',
          'disks': [{'device_type': 'cdrom',
            'key': 'disk-615590-4140422-ide-0-0',
            'lun': '0',
            'size': '0'}],
          'key': 'disk_controller-615590-4140422-ide-0',
          'virtual_dev': None},
         {'bus_type': 'scsi',
          'controller': '0',
          'disks': [{'device_type': 'disk',
            'key': 'disk-615590-4140422-scsi-0-0',
            'lun': '0',
            'size': '2048'}],
          'key': 'disk_controller-615590-4140422-scsi-0',
          'virtual_dev': 'lsilogic'}],
        'disk_policy': {'disk_limit': '15',
         'disk_size_limit': '2096128',
         'iops_limit': '275',
         'key': 'disk_policy-100',
         'offset': '165',
         'size_limit': '2097152'},
        'floppy': 'fd0',
        'guestOS': 'otherlinux-64',
        'guest_cpu_type': '4',
        'hardware_policy': {'max_cpus': '8',
         'max_ram': '32768',
         'min_cpus': '1',
         'min_ram': '256'},
        'hardware_version': '7',
        'host_cpu_type': None,
        'nics': [{'address': '192.168.1.2',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-1',
          'key': 'nic-615590-4140422-0',
          'mac': '00:50:56:24:6B:2F',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454522',
          'nic_type': 'e1000',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': 'e1000',
          'vm_key': 'vm-615590-4140422'},
         {'address': '192.168.2.1',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-1',
          'key': 'nic-615590-4140422-1',
          'mac': '00:50:56:05:74:4E',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454520',
          'nic_type': 'default',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': None,
          'vm_key': 'vm-615590-4140422'},
         {'address': '192.168.3.1',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-1',
          'key': 'nic-615590-4140422-2',
          'mac': '00:50:56:22:55:3B',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454518',
          'nic_type': 'default',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': None,
          'vm_key': 'vm-615590-4140422'}],
        'ram': '1024',
        'uuid': None,
        'vnc_keymap': None},
       'key': 'vm-615590-4140422',
       'vm_extra_options_groups': []},
      {'hardware': {'cpus': '1',
        'disk_controllers': [{'bus_type': 'ide',
          'controller': '0',
          'disks': [{'device_type': 'cdrom',
            'key': 'disk-615590-4140424-ide-0-0',
            'lun': '0',
            'size': '0'}],
          'key': 'disk_controller-615590-4140424-ide-0',
          'virtual_dev': None},
         {'bus_type': 'scsi',
          'controller': '0',
          'disks': [{'device_type': 'disk',
            'key': 'disk-615590-4140424-scsi-0-0',
            'lun': '0',
            'size': '2048'}],
          'key': 'disk_controller-615590-4140424-scsi-0',
          'virtual_dev': 'lsilogic'}],
        'disk_policy': {'disk_limit': '15',
         'disk_size_limit': '2096128',
         'iops_limit': '275',
         'key': 'disk_policy-100',
         'offset': '165',
         'size_limit': '2097152'},
        'floppy': 'fd0',
        'guestOS': 'otherlinux-64',
        'guest_cpu_type': '4',
        'hardware_policy': {'max_cpus': '8',
         'max_ram': '32768',
         'min_cpus': '1',
         'min_ram': '256'},
        'hardware_version': '7',
        'host_cpu_type': None,
        'nics': [{'address': '192.168.1.3',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-2',
          'key': 'nic-615590-4140424-0',
          'mac': '00:50:56:3A:D9:22',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454522',
          'nic_type': 'e1000',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': 'e1000',
          'vm_key': 'vm-615590-4140424'}],
        'ram': '1024',
        'uuid': None,
        'vnc_keymap': None},
       'key': 'vm-615590-4140424',
       'vm_extra_options_groups': []},
      {'hardware': {'cpus': '1',
        'disk_controllers': [{'bus_type': 'ide',
          'controller': '0',
          'disks': [{'device_type': 'cdrom',
            'key': 'disk-615590-4140426-ide-0-0',
            'lun': '0',
            'size': '0'}],
          'key': 'disk_controller-615590-4140426-ide-0',
          'virtual_dev': None},
         {'bus_type': 'scsi',
          'controller': '0',
          'disks': [{'device_type': 'disk',
            'key': 'disk-615590-4140426-scsi-0-0',
            'lun': '0',
            'size': '2048'}],
          'key': 'disk_controller-615590-4140426-scsi-0',
          'virtual_dev': 'lsilogic'}],
        'disk_policy': {'disk_limit': '15',
         'disk_size_limit': '2096128',
         'iops_limit': '275',
         'key': 'disk_policy-100',
         'offset': '165',
         'size_limit': '2097152'},
        'floppy': 'fd0',
        'guestOS': 'otherlinux-64',
        'guest_cpu_type': '4',
        'hardware_policy': {'max_cpus': '8',
         'max_ram': '32768',
         'min_cpus': '1',
         'min_ram': '256'},
        'hardware_version': '7',
        'host_cpu_type': None,
        'nics': [{'address': '192.168.1.4',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-3',
          'key': 'nic-615590-4140426-0',
          'mac': '00:50:56:38:C1:D0',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454522',
          'nic_type': 'e1000',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': 'e1000',
          'vm_key': 'vm-615590-4140426'},
         {'address': '192.168.2.2',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-2',
          'key': 'nic-615590-4140426-1',
          'mac': '00:50:56:0F:3B:A0',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454520',
          'nic_type': 'default',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': None,
          'vm_key': 'vm-615590-4140426'},
         {'address': '192.168.3.2',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-2',
          'key': 'nic-615590-4140426-2',
          'mac': '00:50:56:2C:6C:33',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454518',
          'nic_type': 'default',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': None,
          'vm_key': 'vm-615590-4140426'},
         {'address': '192.168.4.1',
          'configuration_key': 'configuration-615590',
          'hostname': 'host-1',
          'key': 'nic-615590-4140426-3',
          'mac': '00:50:56:1C:FA:32',
          'nat_addresses': {'networks': {}, 'vpns': {}},
          'network_key': 'network-615590-454516',
          'nic_type': 'default',
          'promiscuous_mode_enabled': False,
          'public_ips': [],
          'published_services': [],
          'virtual_dev': None,
          'vm_key': 'vm-615590-4140426'}],
        'ram': '1024',
        'uuid': None,
        'vnc_keymap': None},
       'key': 'vm-615590-4140426',
       'vm_extra_options_groups': []}]},
    0]}
  {noformat}
  
** Comment: bxiao
   :PROPERTIES:
   :ID:       115583
   :created:  2013-10-23 17:12:44
   :END:
  Investigating
** Comment: bxiao
   :PROPERTIES:
   :ID:       115650
   :created:  2013-10-23 20:33:40
   :END:
   Fix submitted to my repo, waiting for jenkens build
** Comment: bxiao
   :PROPERTIES:
   :ID:       115738
   :created:  2013-10-24 03:18:46
   :END:
  fix pushed to integ
* DONE Error watch: run failed due to AttributeError		    :PL_7307:
  :PROPERTIES:
  :assignee: nastete
  :reporter: nastete
  :type:     Bug
  :priority: Blocker
  :status:   Closed
  :components: cm
  :created:  2013-10-23 23:19:39
  :updated:  2013-10-24 00:16:01
  :ID:       PL-7307
  :resolution: Cannot Reproduce
  :END:
** description: PL-7307
  (Since test is currently down, I can't use a console to check the impact of this. Therefore, I'm filing it as a blocker.)
  
  {noformat}
  2013-10-23 23:15:07 tuk6m1trun1 [err] [/wfe.d61384201e66013181e4005056a30032.5/trn.eb151f101e630131f37a005056a30030.5]: [ERROR] logging.rb:47 - API EXCEPTION! The operation cannot be performed at this time. Please try again or contact support. (UnhandledException, 10C03116) [Platform::Client::Error]
   (in: run(["ctag-e10f53701e660131f37a005056a30030", ["vm-49804-80746"], "configuration-49804"])(caller_options: {"prefer_kvm"=>true, "samba_fileserver"=>false})
  
  ...
  
  2013-10-23 23:15:07 tuk6m1cm2 [err] [wfe.d61384201e66013181e4005056a30032.5/trn.eb151f101e630131f37a005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-0-0.25956.request.run_vms.42/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.15061.run.15.api]: [ERROR] errors.py:209 - run EXCEPT 0.44 : {"incident
  _id": "10C03116", "name": "run", "timestamp": "2013-10-23 23:15:07", "_original_exception": "AttributeError(\"type object 'Limit' has no attribute 'region'\",)", "__severity": "error", "context": "wfe.d61384201e66013181e4005056a30032.5/trn.eb151f101e630131f37a005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skyta
  p.com-0-0.25956.request.run_vms.42/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.15061.run.15", "msg": "\"type object 'Limit' has no attribute 'region'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 31, in common_error_handling
      yield
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 45, in run
      ret = func(cm_instance, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/mixin.py", line 29, in <lambda>
      method = wraps(operation)(lambda self, *args, **kwargs: executor(self, operation, *args, **kwargs))
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 77, in operation_executor
      op = operation(*args, logger=LOGGER, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/provision.py", line 172, in __init__
      self.allocate_resources()
    File "/highland/hosting_platform/hosting_platform/common/monitoring/metric.py", line 211, in wrap
      return func(*args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/provision.py", line 239, in allocate_resources
      self.apply_rate_limits([a.account_id for a in accounts], self.api_name(), self.vms, self.networks)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 377, in apply_rate_limits
      limiter.check_and_reserve(limits)
    File "/usr/lib/python2.6/contextlib.py", line 34, in __exit__
      self.gen.throw(type, value, traceback)
    File "/highland/hosting_platform/hosting_platform/db/db.py", line 106, in _run_transaction
      yield sess
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 374, in apply_rate_limits
      limits = tx.query(Limit).filter(Limit.account_id.in_(account_ids)).filter(Limit.region == region).all()
  {noformat}
** Comment: nastete
   :PROPERTIES:
   :ID:       115698
   :created:  2013-10-23 23:20:47
   :END:
  Also occurs for suspend:
  
  {noformat}
  2013-10-23 21:15:11 tuk6m1trun1 [err] [/wfe.79d9f7901e5501316d31005056a3002c.10/trn.133735e01e450131f372005056a30030.9]: [ERROR] logging.rb:47 - API EXCEPTION! The operation cannot be performed at this time. Please try again or contact support. (UnhandledException, 3AFC4C13) [Platform::Client::Error]
   (in: suspend([["vm-49804-80746"]])(caller_options: {"prefer_kvm"=>true, "samba_fileserver"=>false})
  
  ...
  
  2013-10-23 21:15:11 tuk6m1cm3 [err] [wfe.79d9f7901e5501316d31005056a3002c.10/trn.133735e01e450131f372005056a30030.9.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-2-0.23729.request.suspend_vms.52/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-1.26080.suspend.34.api]: [ERROR] errors.py:209 - suspend EXCEPT 0.21
   : {"incident_id": "3AFC4C13", "name": "suspend", "timestamp": "2013-10-23 21:15:11", "_original_exception": "AttributeError(\"type object 'Limit' has no attribute 'region'\",)", "__severity": "error", "context": "wfe.79d9f7901e5501316d31005056a3002c.10/trn.133735e01e450131f372005056a30030.9.1@/greenbox-tuk6m1
  gb1.mgt.test.skytap.com-2-0.23729.request.suspend_vms.52/configuration_manager-tuk6m1cm3.mgt.test.skytap.com-1.26080.suspend.34", "msg": "\"type object 'Limit' has no attribute 'region'\"", "type": "UnhandledException"}
  --traceback---
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 31, in common_error_handling
      yield
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 45, in suspend
      ret = func(cm_instance, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/mixin.py", line 29, in <lambda>
      method = wraps(operation)(lambda self, *args, **kwargs: executor(self, operation, *args, **kwargs))
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/ConfigurationManager.py", line 77, in operation_executor
      op = operation(*args, logger=LOGGER, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/common/validation.py", line 138, in _validate_params
      return func(self, *args, **kwargs)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/unprovision.py", line 1239, in __init__
      self.apply_rate_limits(self.account_ids, self.api_name(), self.vms, self.networks)
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 377, in apply_rate_limits
      limiter.check_and_reserve(limits)
    File "/usr/lib/python2.6/contextlib.py", line 34, in __exit__
      self.gen.throw(type, value, traceback)
    File "/highland/hosting_platform/hosting_platform/db/db.py", line 106, in _run_transaction
      yield sess
    File "/highland/hosting_platform/hosting_platform/services/configuration_manager/operations/base.py", line 374, in apply_rate_limits
      limits = tx.query(Limit).filter(Limit.account_id.in_(account_ids)).filter(Limit.region == region).all()
  {noformat}
** Comment: bxiao
   :PROPERTIES:
   :ID:       115700
   :created:  2013-10-23 23:30:45
   :END:
  investigating
** Comment: andrus
   :PROPERTIES:
   :ID:       115707
   :created:  2013-10-23 23:48:36
   :END:
  Note:
  
  From ipython
  
  {code}
  In [5]: from hosting_platform.services.configuration_manager.db import Limit
  
  In [6]: Limit.region
  ---------------------------------------------------------------------------
  AttributeError                            Traceback (most recent call last)
  
  /highland/<ipython console> in <module>()
  
  AttributeError: type object 'Limit' has no attribute 'region'
  {code}
  
  Looks familiar.
  
  I wonder if this is some wacky sqlalchemy mapper issue? 
  
** Comment: andrus
   :PROPERTIES:
   :ID:       115709
   :created:  2013-10-23 23:50:33
   :updated:  2013-10-23 23:53:02
   :END:
  In INTEG, in cmcmd - this works:
  
  {code}
  In [13]: with transaction() as tx:
     ....:     lims = tx.query(Limit).filter(Limit.region==region).all()
     ....:
     ....:
  
  In [14]: lims
  
  /// prints lots of stuff
  
  In [17]: lim = lims[-1]
  
  In [18]: lim.region
  Out[18]: 'integ/tuk5r1'
  
  {code}
  
** Comment: andrus
   :PROPERTIES:
   :ID:       115710
   :created:  2013-10-23 23:52:33
   :END:
  But in test:
  
  {code}
  In [10]: with transaction() as tx:
     ....:     lims =  tx.query(Limit).all()
     ....:
  
  
  In [11]:
  
  In [11]: len(lims)
  Out[11]: 8417
  
  In [12]: lims[-1]
  Out[12]: <hosting_platform.services.configuration_manager.model.limits.Limit at 0x5f0a290>
  
  In [13]: lim = lims[-1]
  
  In [14]: lim.region
  ---------------------------------------------------------------------------
  AttributeError                            Traceback (most recent call last)
  <ipython-input-14-7c149f878f12> in <module>()
  ----> 1 lim.region
  
  AttributeError: 'Limit' object has no attribute 'region'
  
  {code}
  
** Comment: andrus
   :PROPERTIES:
   :ID:       115711
   :created:  2013-10-23 23:56:13
   :END:
  hg id
  
  23:53 test highland@tuk6m1logger1:~/hosting_platform$ hg id ece991cbe3ec tip
  
  hg pull -u
  
  now
  
  23:54 test highland@tuk6m1logger1:~/hosting_platform$ hg id d52721e16c04 tip
  
  {code}
  In [1]: with transaction() as tx:
      lims =  tx.query(Limit).all()
     ...:
  
  In [2]: lim = lims[-1]
  
  In [3]: lim.region
  Out[3]: 'test/tuk6r1
  {code}
  
** Comment: andrus
   :PROPERTIES:
   :ID:       115712
   :created:  2013-10-24 00:02:33
   :END:
  Hhm. I reverted hosting_platform (TEST) to ece991cbe3ec and the problem DOES NOT repro.
  
  I found a bunch of .pyc files and removed them. Still no repro.
  
  Very weird.
  
** Comment: bxiao
   :PROPERTIES:
   :ID:       115715
   :created:  2013-10-24 00:03:54
   :END:
  Cannot repro now. Resolving.
** Comment: lonnieh
   :PROPERTIES:
   :ID:       115717
   :created:  2013-10-24 00:15:36
   :END:
  I believe when Yelena restored the old version to test, the region field disappeared because it was added in R49. Until the master process was restarted, all the children spawned from it would have mappers that didn't have the attribute, and would fail with this.
  
  Nothing to see here.
* TODO regional rate limits can trivially be misconfigured without us knowing about it :PL_7280:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: lonnieh
  :type:     Bug
  :priority: Critical
  :status:   Open
  :components: cm
  :created:  2013-10-23 15:07:42
  :updated:  2013-10-25 15:56:58
  :ID:       PL-7280
  :END:
** description: PL-7280
  I happened to notice this warning (which doesn't go to error watch
  or fail service startup) while investigating another bug in test. I
  think this should be fatal and prevent service startup:
  
  {noformat}
  2013-10-22 10:56:20 tuk6m1cm1 [warning] [wfe.7a1f10201d3601316d13005056a3002c.2.11@/greenbox-tuk6m1gb2.mgt.test.skytap.com-5-0.5659.request.merge.3314/qm-merge-
  0/configuration_manager-tuk6m1cm1.mgt.test.skytap.com-1.23123.merge.16.ratelimiter]: [WARNING] limits.py:172 - Cannot load regional configuration: Configuration
  Error: ("'Configuration file not found for hosting_platform'", 'Configuration file not found for hosting_platform'). 
  {noformat}
** Comment: bxiao
   :PROPERTIES:
   :ID:       116011
   :created:  2013-10-25 16:39:19
   :END:
   The reason we do it this way is we assume that there might be
   regions that do not have regional specific configs, thus we
   fallback to non-regional config in such case. Is this assumption
   still right or all regions must have regional configs?
* TODO Rate limiter logging should include information on which region a particular operation is being evaluated against :PL_7370:
  :PROPERTIES:
  :assignee: bxiao
  :reporter: evanc
  :type:     Bug
  :priority: Critical
  :status:   Open
  :components: cm
  :created:  2013-10-24 18:22:31
  :updated:  2013-10-25 15:38:54
  :ID:       PL-7370
  :END:
** description: PL-7370
  The logging for CM rate limiting looks something like this:
  {noformat}
  2013-10-24 18:18:42 tuk6m1cm2 [debug] [wfe.74abec001f06013181f2005056a30032.5/trn.7d56cc301f060131f388005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-3-0.27659.request.run_vms.10692/qm-run-5/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.24358.run.37.ratelimiter]: [DEBUG] limits.py:302 - ratelimiter costs: op run, storage 36.69, host 40.00, net 0.00
  2013-10-24 18:18:42 tuk6m1cm2 [debug] [wfe.74abec001f06013181f2005056a30032.5/trn.7d56cc301f060131f388005056a30030.5.1@/greenbox-tuk6m1gb1.mgt.test.skytap.com-3-0.27659.request.run_vms.10692/qm-run-5/configuration_manager-tuk6m1cm2.mgt.test.skytap.com-1.24358.run.37.ratelimiter]: [DEBUG] limits.py:410 - ratelimiter allow: op run, max_account_id 19054, max_outstanding -5, threshold 120
  {noformat}
  
  Since the rate limiter is now evaluating operations on a per region basis for per region limits, the region information should be included in the logging statements for the ease of following up on issues.
